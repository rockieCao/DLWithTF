{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF v1 implementation of linear regression for book DLWithTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(456)\n",
    "tf.set_random_seed(456)\n",
    "rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_r2_score(y, y_pred):\n",
    "    \"\"\"Computes Pearson R^2 (square of Pearson correlation).\"\"\"\n",
    "    return pearsonr(y, y_pred)[0]**2\n",
    "\n",
    "def rms_score(y_true, y_pred):\n",
    "    \"\"\"Computes RMS error.\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general setting\n",
    "N = 100 # train instances\n",
    "w_true = 5 # ground truth w\n",
    "b_true = 2 # ground truth b\n",
    "noise_scale = .1\n",
    "lr = 0.001\n",
    "x_np = np.random.rand(N, 1)\n",
    "noise = np.random.normal(scale=noise_scale, size=(N, 1))\n",
    "y_np = np.reshape(w_true * x_np + b_true + noise, (-1)) # convert to (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw training data distribution\n",
    "plt.scatter(x_np, y_np)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(0, 1)\n",
    "plt.title(\"Toy Linear Regression Data, \")\n",
    "          #r\"$y = 5x + 2 + N(0, 1)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tensorflow graph\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32, (N,1))\n",
    "    y = tf.placeholder(tf.float32, (N,))\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = tf.Variable(tf.random_normal((1, 1)))\n",
    "    b = tf.Variable(tf.random_normal((1,)))\n",
    "with tf.name_scope(\"prediction\"):\n",
    "    y_pred = tf.matmul(x, W) + b\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean((y - tf.squeeze(y_pred))**2)\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"/tmp/lr-train\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 18.024029\n",
      "step 1, loss: 18.011175\n",
      "step 2, loss: 17.998329\n",
      "step 3, loss: 17.985485\n",
      "step 4, loss: 17.972647\n",
      "step 5, loss: 17.959814\n",
      "step 6, loss: 17.946985\n",
      "step 7, loss: 17.934164\n",
      "step 8, loss: 17.921345\n",
      "step 9, loss: 17.908531\n",
      "step 10, loss: 17.895723\n",
      "step 11, loss: 17.882923\n",
      "step 12, loss: 17.870125\n",
      "step 13, loss: 17.857334\n",
      "step 14, loss: 17.844549\n",
      "step 15, loss: 17.831772\n",
      "step 16, loss: 17.818995\n",
      "step 17, loss: 17.806229\n",
      "step 18, loss: 17.793465\n",
      "step 19, loss: 17.780708\n",
      "step 20, loss: 17.767960\n",
      "step 21, loss: 17.755215\n",
      "step 22, loss: 17.742476\n",
      "step 23, loss: 17.729744\n",
      "step 24, loss: 17.717018\n",
      "step 25, loss: 17.704298\n",
      "step 26, loss: 17.691584\n",
      "step 27, loss: 17.678879\n",
      "step 28, loss: 17.666176\n",
      "step 29, loss: 17.653481\n",
      "step 30, loss: 17.640793\n",
      "step 31, loss: 17.628111\n",
      "step 32, loss: 17.615435\n",
      "step 33, loss: 17.602766\n",
      "step 34, loss: 17.590105\n",
      "step 35, loss: 17.577446\n",
      "step 36, loss: 17.564796\n",
      "step 37, loss: 17.552156\n",
      "step 38, loss: 17.539516\n",
      "step 39, loss: 17.526888\n",
      "step 40, loss: 17.514267\n",
      "step 41, loss: 17.501650\n",
      "step 42, loss: 17.489038\n",
      "step 43, loss: 17.476435\n",
      "step 44, loss: 17.463839\n",
      "step 45, loss: 17.451248\n",
      "step 46, loss: 17.438665\n",
      "step 47, loss: 17.426088\n",
      "step 48, loss: 17.413519\n",
      "step 49, loss: 17.400953\n",
      "step 50, loss: 17.388397\n",
      "step 51, loss: 17.375847\n",
      "step 52, loss: 17.363304\n",
      "step 53, loss: 17.350767\n",
      "step 54, loss: 17.338238\n",
      "step 55, loss: 17.325716\n",
      "step 56, loss: 17.313198\n",
      "step 57, loss: 17.300688\n",
      "step 58, loss: 17.288185\n",
      "step 59, loss: 17.275688\n",
      "step 60, loss: 17.263199\n",
      "step 61, loss: 17.250717\n",
      "step 62, loss: 17.238239\n",
      "step 63, loss: 17.225773\n",
      "step 64, loss: 17.213306\n",
      "step 65, loss: 17.200853\n",
      "step 66, loss: 17.188404\n",
      "step 67, loss: 17.175962\n",
      "step 68, loss: 17.163525\n",
      "step 69, loss: 17.151096\n",
      "step 70, loss: 17.138674\n",
      "step 71, loss: 17.126259\n",
      "step 72, loss: 17.113852\n",
      "step 73, loss: 17.101448\n",
      "step 74, loss: 17.089052\n",
      "step 75, loss: 17.076664\n",
      "step 76, loss: 17.064281\n",
      "step 77, loss: 17.051908\n",
      "step 78, loss: 17.039539\n",
      "step 79, loss: 17.027178\n",
      "step 80, loss: 17.014822\n",
      "step 81, loss: 17.002474\n",
      "step 82, loss: 16.990133\n",
      "step 83, loss: 16.977798\n",
      "step 84, loss: 16.965469\n",
      "step 85, loss: 16.953150\n",
      "step 86, loss: 16.940832\n",
      "step 87, loss: 16.928526\n",
      "step 88, loss: 16.916224\n",
      "step 89, loss: 16.903929\n",
      "step 90, loss: 16.891642\n",
      "step 91, loss: 16.879360\n",
      "step 92, loss: 16.867086\n",
      "step 93, loss: 16.854820\n",
      "step 94, loss: 16.842558\n",
      "step 95, loss: 16.830301\n",
      "step 96, loss: 16.818054\n",
      "step 97, loss: 16.805813\n",
      "step 98, loss: 16.793577\n",
      "step 99, loss: 16.781353\n",
      "step 100, loss: 16.769129\n",
      "step 101, loss: 16.756914\n",
      "step 102, loss: 16.744707\n",
      "step 103, loss: 16.732506\n",
      "step 104, loss: 16.720310\n",
      "step 105, loss: 16.708124\n",
      "step 106, loss: 16.695940\n",
      "step 107, loss: 16.683767\n",
      "step 108, loss: 16.671597\n",
      "step 109, loss: 16.659435\n",
      "step 110, loss: 16.647282\n",
      "step 111, loss: 16.635134\n",
      "step 112, loss: 16.622992\n",
      "step 113, loss: 16.610859\n",
      "step 114, loss: 16.598730\n",
      "step 115, loss: 16.586609\n",
      "step 116, loss: 16.574493\n",
      "step 117, loss: 16.562386\n",
      "step 118, loss: 16.550283\n",
      "step 119, loss: 16.538187\n",
      "step 120, loss: 16.526100\n",
      "step 121, loss: 16.514019\n",
      "step 122, loss: 16.501942\n",
      "step 123, loss: 16.489874\n",
      "step 124, loss: 16.477812\n",
      "step 125, loss: 16.465757\n",
      "step 126, loss: 16.453707\n",
      "step 127, loss: 16.441666\n",
      "step 128, loss: 16.429630\n",
      "step 129, loss: 16.417601\n",
      "step 130, loss: 16.405577\n",
      "step 131, loss: 16.393562\n",
      "step 132, loss: 16.381552\n",
      "step 133, loss: 16.369549\n",
      "step 134, loss: 16.357553\n",
      "step 135, loss: 16.345562\n",
      "step 136, loss: 16.333580\n",
      "step 137, loss: 16.321602\n",
      "step 138, loss: 16.309633\n",
      "step 139, loss: 16.297670\n",
      "step 140, loss: 16.285713\n",
      "step 141, loss: 16.273766\n",
      "step 142, loss: 16.261818\n",
      "step 143, loss: 16.249884\n",
      "step 144, loss: 16.237951\n",
      "step 145, loss: 16.226028\n",
      "step 146, loss: 16.214109\n",
      "step 147, loss: 16.202198\n",
      "step 148, loss: 16.190294\n",
      "step 149, loss: 16.178396\n",
      "step 150, loss: 16.166504\n",
      "step 151, loss: 16.154619\n",
      "step 152, loss: 16.142740\n",
      "step 153, loss: 16.130871\n",
      "step 154, loss: 16.119005\n",
      "step 155, loss: 16.107145\n",
      "step 156, loss: 16.095293\n",
      "step 157, loss: 16.083447\n",
      "step 158, loss: 16.071609\n",
      "step 159, loss: 16.059774\n",
      "step 160, loss: 16.047949\n",
      "step 161, loss: 16.036129\n",
      "step 162, loss: 16.024317\n",
      "step 163, loss: 16.012510\n",
      "step 164, loss: 16.000710\n",
      "step 165, loss: 15.988916\n",
      "step 166, loss: 15.977128\n",
      "step 167, loss: 15.965349\n",
      "step 168, loss: 15.953574\n",
      "step 169, loss: 15.941806\n",
      "step 170, loss: 15.930044\n",
      "step 171, loss: 15.918290\n",
      "step 172, loss: 15.906542\n",
      "step 173, loss: 15.894797\n",
      "step 174, loss: 15.883064\n",
      "step 175, loss: 15.871333\n",
      "step 176, loss: 15.859612\n",
      "step 177, loss: 15.847896\n",
      "step 178, loss: 15.836186\n",
      "step 179, loss: 15.824482\n",
      "step 180, loss: 15.812786\n",
      "step 181, loss: 15.801096\n",
      "step 182, loss: 15.789412\n",
      "step 183, loss: 15.777736\n",
      "step 184, loss: 15.766065\n",
      "step 185, loss: 15.754400\n",
      "step 186, loss: 15.742743\n",
      "step 187, loss: 15.731091\n",
      "step 188, loss: 15.719444\n",
      "step 189, loss: 15.707806\n",
      "step 190, loss: 15.696173\n",
      "step 191, loss: 15.684548\n",
      "step 192, loss: 15.672927\n",
      "step 193, loss: 15.661315\n",
      "step 194, loss: 15.649708\n",
      "step 195, loss: 15.638109\n",
      "step 196, loss: 15.626513\n",
      "step 197, loss: 15.614926\n",
      "step 198, loss: 15.603346\n",
      "step 199, loss: 15.591772\n",
      "step 200, loss: 15.580203\n",
      "step 201, loss: 15.568642\n",
      "step 202, loss: 15.557085\n",
      "step 203, loss: 15.545536\n",
      "step 204, loss: 15.533993\n",
      "step 205, loss: 15.522458\n",
      "step 206, loss: 15.510929\n",
      "step 207, loss: 15.499403\n",
      "step 208, loss: 15.487885\n",
      "step 209, loss: 15.476376\n",
      "step 210, loss: 15.464871\n",
      "step 211, loss: 15.453374\n",
      "step 212, loss: 15.441882\n",
      "step 213, loss: 15.430397\n",
      "step 214, loss: 15.418919\n",
      "step 215, loss: 15.407444\n",
      "step 216, loss: 15.395979\n",
      "step 217, loss: 15.384521\n",
      "step 218, loss: 15.373065\n",
      "step 219, loss: 15.361619\n",
      "step 220, loss: 15.350178\n",
      "step 221, loss: 15.338744\n",
      "step 222, loss: 15.327315\n",
      "step 223, loss: 15.315893\n",
      "step 224, loss: 15.304478\n",
      "step 225, loss: 15.293069\n",
      "step 226, loss: 15.281665\n",
      "step 227, loss: 15.270269\n",
      "step 228, loss: 15.258880\n",
      "step 229, loss: 15.247497\n",
      "step 230, loss: 15.236117\n",
      "step 231, loss: 15.224748\n",
      "step 232, loss: 15.213382\n",
      "step 233, loss: 15.202024\n",
      "step 234, loss: 15.190671\n",
      "step 235, loss: 15.179325\n",
      "step 236, loss: 15.167986\n",
      "step 237, loss: 15.156652\n",
      "step 238, loss: 15.145325\n",
      "step 239, loss: 15.134006\n",
      "step 240, loss: 15.122692\n",
      "step 241, loss: 15.111382\n",
      "step 242, loss: 15.100081\n",
      "step 243, loss: 15.088785\n",
      "step 244, loss: 15.077497\n",
      "step 245, loss: 15.066212\n",
      "step 246, loss: 15.054937\n",
      "step 247, loss: 15.043668\n",
      "step 248, loss: 15.032404\n",
      "step 249, loss: 15.021144\n",
      "step 250, loss: 15.009892\n",
      "step 251, loss: 14.998648\n",
      "step 252, loss: 14.987408\n",
      "step 253, loss: 14.976175\n",
      "step 254, loss: 14.964948\n",
      "step 255, loss: 14.953728\n",
      "step 256, loss: 14.942516\n",
      "step 257, loss: 14.931306\n",
      "step 258, loss: 14.920106\n",
      "step 259, loss: 14.908911\n",
      "step 260, loss: 14.897722\n",
      "step 261, loss: 14.886539\n",
      "step 262, loss: 14.875361\n",
      "step 263, loss: 14.864192\n",
      "step 264, loss: 14.853027\n",
      "step 265, loss: 14.841870\n",
      "step 266, loss: 14.830718\n",
      "step 267, loss: 14.819574\n",
      "step 268, loss: 14.808435\n",
      "step 269, loss: 14.797301\n",
      "step 270, loss: 14.786175\n",
      "step 271, loss: 14.775054\n",
      "step 272, loss: 14.763941\n",
      "step 273, loss: 14.752831\n",
      "step 274, loss: 14.741731\n",
      "step 275, loss: 14.730635\n",
      "step 276, loss: 14.719544\n",
      "step 277, loss: 14.708462\n",
      "step 278, loss: 14.697385\n",
      "step 279, loss: 14.686315\n",
      "step 280, loss: 14.675248\n",
      "step 281, loss: 14.664190\n",
      "step 282, loss: 14.653138\n",
      "step 283, loss: 14.642094\n",
      "step 284, loss: 14.631052\n",
      "step 285, loss: 14.620018\n",
      "step 286, loss: 14.608990\n",
      "step 287, loss: 14.597970\n",
      "step 288, loss: 14.586954\n",
      "step 289, loss: 14.575945\n",
      "step 290, loss: 14.564941\n",
      "step 291, loss: 14.553946\n",
      "step 292, loss: 14.542956\n",
      "step 293, loss: 14.531971\n",
      "step 294, loss: 14.520992\n",
      "step 295, loss: 14.510021\n",
      "step 296, loss: 14.499055\n",
      "step 297, loss: 14.488094\n",
      "step 298, loss: 14.477139\n",
      "step 299, loss: 14.466194\n",
      "step 300, loss: 14.455253\n",
      "step 301, loss: 14.444318\n",
      "step 302, loss: 14.433388\n",
      "step 303, loss: 14.422467\n",
      "step 304, loss: 14.411551\n",
      "step 305, loss: 14.400640\n",
      "step 306, loss: 14.389734\n",
      "step 307, loss: 14.378836\n",
      "step 308, loss: 14.367943\n",
      "step 309, loss: 14.357057\n",
      "step 310, loss: 14.346177\n",
      "step 311, loss: 14.335302\n",
      "step 312, loss: 14.324436\n",
      "step 313, loss: 14.313574\n",
      "step 314, loss: 14.302718\n",
      "step 315, loss: 14.291870\n",
      "step 316, loss: 14.281025\n",
      "step 317, loss: 14.270189\n",
      "step 318, loss: 14.259356\n",
      "step 319, loss: 14.248531\n",
      "step 320, loss: 14.237714\n",
      "step 321, loss: 14.226901\n",
      "step 322, loss: 14.216094\n",
      "step 323, loss: 14.205293\n",
      "step 324, loss: 14.194500\n",
      "step 325, loss: 14.183711\n",
      "step 326, loss: 14.172929\n",
      "step 327, loss: 14.162153\n",
      "step 328, loss: 14.151381\n",
      "step 329, loss: 14.140617\n",
      "step 330, loss: 14.129860\n",
      "step 331, loss: 14.119107\n",
      "step 332, loss: 14.108360\n",
      "step 333, loss: 14.097621\n",
      "step 334, loss: 14.086887\n",
      "step 335, loss: 14.076159\n",
      "step 336, loss: 14.065438\n",
      "step 337, loss: 14.054723\n",
      "step 338, loss: 14.044012\n",
      "step 339, loss: 14.033308\n",
      "step 340, loss: 14.022611\n",
      "step 341, loss: 14.011920\n",
      "step 342, loss: 14.001234\n",
      "step 343, loss: 13.990554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 344, loss: 13.979880\n",
      "step 345, loss: 13.969215\n",
      "step 346, loss: 13.958552\n",
      "step 347, loss: 13.947898\n",
      "step 348, loss: 13.937247\n",
      "step 349, loss: 13.926604\n",
      "step 350, loss: 13.915967\n",
      "step 351, loss: 13.905337\n",
      "step 352, loss: 13.894711\n",
      "step 353, loss: 13.884091\n",
      "step 354, loss: 13.873479\n",
      "step 355, loss: 13.862872\n",
      "step 356, loss: 13.852270\n",
      "step 357, loss: 13.841675\n",
      "step 358, loss: 13.831086\n",
      "step 359, loss: 13.820503\n",
      "step 360, loss: 13.809925\n",
      "step 361, loss: 13.799356\n",
      "step 362, loss: 13.788789\n",
      "step 363, loss: 13.778230\n",
      "step 364, loss: 13.767677\n",
      "step 365, loss: 13.757130\n",
      "step 366, loss: 13.746590\n",
      "step 367, loss: 13.736053\n",
      "step 368, loss: 13.725525\n",
      "step 369, loss: 13.715001\n",
      "step 370, loss: 13.704484\n",
      "step 371, loss: 13.693973\n",
      "step 372, loss: 13.683467\n",
      "step 373, loss: 13.672968\n",
      "step 374, loss: 13.662473\n",
      "step 375, loss: 13.651985\n",
      "step 376, loss: 13.641505\n",
      "step 377, loss: 13.631029\n",
      "step 378, loss: 13.620559\n",
      "step 379, loss: 13.610096\n",
      "step 380, loss: 13.599639\n",
      "step 381, loss: 13.589186\n",
      "step 382, loss: 13.578740\n",
      "step 383, loss: 13.568299\n",
      "step 384, loss: 13.557866\n",
      "step 385, loss: 13.547439\n",
      "step 386, loss: 13.537015\n",
      "step 387, loss: 13.526600\n",
      "step 388, loss: 13.516191\n",
      "step 389, loss: 13.505786\n",
      "step 390, loss: 13.495386\n",
      "step 391, loss: 13.484994\n",
      "step 392, loss: 13.474608\n",
      "step 393, loss: 13.464228\n",
      "step 394, loss: 13.453854\n",
      "step 395, loss: 13.443485\n",
      "step 396, loss: 13.433122\n",
      "step 397, loss: 13.422764\n",
      "step 398, loss: 13.412414\n",
      "step 399, loss: 13.402068\n",
      "step 400, loss: 13.391728\n",
      "step 401, loss: 13.381395\n",
      "step 402, loss: 13.371068\n",
      "step 403, loss: 13.360747\n",
      "step 404, loss: 13.350430\n",
      "step 405, loss: 13.340121\n",
      "step 406, loss: 13.329816\n",
      "step 407, loss: 13.319519\n",
      "step 408, loss: 13.309226\n",
      "step 409, loss: 13.298941\n",
      "step 410, loss: 13.288661\n",
      "step 411, loss: 13.278385\n",
      "step 412, loss: 13.268116\n",
      "step 413, loss: 13.257854\n",
      "step 414, loss: 13.247597\n",
      "step 415, loss: 13.237347\n",
      "step 416, loss: 13.227100\n",
      "step 417, loss: 13.216861\n",
      "step 418, loss: 13.206627\n",
      "step 419, loss: 13.196400\n",
      "step 420, loss: 13.186178\n",
      "step 421, loss: 13.175961\n",
      "step 422, loss: 13.165752\n",
      "step 423, loss: 13.155548\n",
      "step 424, loss: 13.145350\n",
      "step 425, loss: 13.135155\n",
      "step 426, loss: 13.124969\n",
      "step 427, loss: 13.114789\n",
      "step 428, loss: 13.104614\n",
      "step 429, loss: 13.094443\n",
      "step 430, loss: 13.084281\n",
      "step 431, loss: 13.074124\n",
      "step 432, loss: 13.063972\n",
      "step 433, loss: 13.053824\n",
      "step 434, loss: 13.043685\n",
      "step 435, loss: 13.033551\n",
      "step 436, loss: 13.023423\n",
      "step 437, loss: 13.013301\n",
      "step 438, loss: 13.003182\n",
      "step 439, loss: 12.993072\n",
      "step 440, loss: 12.982966\n",
      "step 441, loss: 12.972866\n",
      "step 442, loss: 12.962772\n",
      "step 443, loss: 12.952685\n",
      "step 444, loss: 12.942604\n",
      "step 445, loss: 12.932527\n",
      "step 446, loss: 12.922458\n",
      "step 447, loss: 12.912392\n",
      "step 448, loss: 12.902331\n",
      "step 449, loss: 12.892281\n",
      "step 450, loss: 12.882233\n",
      "step 451, loss: 12.872191\n",
      "step 452, loss: 12.862155\n",
      "step 453, loss: 12.852125\n",
      "step 454, loss: 12.842102\n",
      "step 455, loss: 12.832081\n",
      "step 456, loss: 12.822069\n",
      "step 457, loss: 12.812060\n",
      "step 458, loss: 12.802060\n",
      "step 459, loss: 12.792066\n",
      "step 460, loss: 12.782077\n",
      "step 461, loss: 12.772092\n",
      "step 462, loss: 12.762115\n",
      "step 463, loss: 12.752141\n",
      "step 464, loss: 12.742175\n",
      "step 465, loss: 12.732213\n",
      "step 466, loss: 12.722259\n",
      "step 467, loss: 12.712310\n",
      "step 468, loss: 12.702366\n",
      "step 469, loss: 12.692430\n",
      "step 470, loss: 12.682495\n",
      "step 471, loss: 12.672569\n",
      "step 472, loss: 12.662649\n",
      "step 473, loss: 12.652733\n",
      "step 474, loss: 12.642823\n",
      "step 475, loss: 12.632920\n",
      "step 476, loss: 12.623022\n",
      "step 477, loss: 12.613130\n",
      "step 478, loss: 12.603245\n",
      "step 479, loss: 12.593362\n",
      "step 480, loss: 12.583488\n",
      "step 481, loss: 12.573617\n",
      "step 482, loss: 12.563754\n",
      "step 483, loss: 12.553897\n",
      "step 484, loss: 12.544045\n",
      "step 485, loss: 12.534198\n",
      "step 486, loss: 12.524357\n",
      "step 487, loss: 12.514522\n",
      "step 488, loss: 12.504692\n",
      "step 489, loss: 12.494869\n",
      "step 490, loss: 12.485051\n",
      "step 491, loss: 12.475237\n",
      "step 492, loss: 12.465431\n",
      "step 493, loss: 12.455631\n",
      "step 494, loss: 12.445835\n",
      "step 495, loss: 12.436048\n",
      "step 496, loss: 12.426262\n",
      "step 497, loss: 12.416483\n",
      "step 498, loss: 12.406711\n",
      "step 499, loss: 12.396943\n",
      "step 500, loss: 12.387181\n",
      "step 501, loss: 12.377427\n",
      "step 502, loss: 12.367677\n",
      "step 503, loss: 12.357933\n",
      "step 504, loss: 12.348193\n",
      "step 505, loss: 12.338459\n",
      "step 506, loss: 12.328732\n",
      "step 507, loss: 12.319013\n",
      "step 508, loss: 12.309295\n",
      "step 509, loss: 12.299585\n",
      "step 510, loss: 12.289881\n",
      "step 511, loss: 12.280181\n",
      "step 512, loss: 12.270489\n",
      "step 513, loss: 12.260799\n",
      "step 514, loss: 12.251118\n",
      "step 515, loss: 12.241441\n",
      "step 516, loss: 12.231770\n",
      "step 517, loss: 12.222104\n",
      "step 518, loss: 12.212445\n",
      "step 519, loss: 12.202790\n",
      "step 520, loss: 12.193142\n",
      "step 521, loss: 12.183499\n",
      "step 522, loss: 12.173862\n",
      "step 523, loss: 12.164231\n",
      "step 524, loss: 12.154605\n",
      "step 525, loss: 12.144984\n",
      "step 526, loss: 12.135368\n",
      "step 527, loss: 12.125759\n",
      "step 528, loss: 12.116158\n",
      "step 529, loss: 12.106559\n",
      "step 530, loss: 12.096967\n",
      "step 531, loss: 12.087379\n",
      "step 532, loss: 12.077798\n",
      "step 533, loss: 12.068221\n",
      "step 534, loss: 12.058652\n",
      "step 535, loss: 12.049088\n",
      "step 536, loss: 12.039529\n",
      "step 537, loss: 12.029976\n",
      "step 538, loss: 12.020428\n",
      "step 539, loss: 12.010886\n",
      "step 540, loss: 12.001348\n",
      "step 541, loss: 11.991817\n",
      "step 542, loss: 11.982292\n",
      "step 543, loss: 11.972773\n",
      "step 544, loss: 11.963258\n",
      "step 545, loss: 11.953750\n",
      "step 546, loss: 11.944245\n",
      "step 547, loss: 11.934749\n",
      "step 548, loss: 11.925257\n",
      "step 549, loss: 11.915771\n",
      "step 550, loss: 11.906289\n",
      "step 551, loss: 11.896812\n",
      "step 552, loss: 11.887343\n",
      "step 553, loss: 11.877880\n",
      "step 554, loss: 11.868421\n",
      "step 555, loss: 11.858967\n",
      "step 556, loss: 11.849519\n",
      "step 557, loss: 11.840078\n",
      "step 558, loss: 11.830640\n",
      "step 559, loss: 11.821210\n",
      "step 560, loss: 11.811784\n",
      "step 561, loss: 11.802363\n",
      "step 562, loss: 11.792950\n",
      "step 563, loss: 11.783540\n",
      "step 564, loss: 11.774137\n",
      "step 565, loss: 11.764739\n",
      "step 566, loss: 11.755345\n",
      "step 567, loss: 11.745958\n",
      "step 568, loss: 11.736577\n",
      "step 569, loss: 11.727201\n",
      "step 570, loss: 11.717832\n",
      "step 571, loss: 11.708466\n",
      "step 572, loss: 11.699106\n",
      "step 573, loss: 11.689754\n",
      "step 574, loss: 11.680404\n",
      "step 575, loss: 11.671061\n",
      "step 576, loss: 11.661725\n",
      "step 577, loss: 11.652390\n",
      "step 578, loss: 11.643064\n",
      "step 579, loss: 11.633744\n",
      "step 580, loss: 11.624430\n",
      "step 581, loss: 11.615120\n",
      "step 582, loss: 11.605813\n",
      "step 583, loss: 11.596516\n",
      "step 584, loss: 11.587222\n",
      "step 585, loss: 11.577932\n",
      "step 586, loss: 11.568651\n",
      "step 587, loss: 11.559373\n",
      "step 588, loss: 11.550102\n",
      "step 589, loss: 11.540836\n",
      "step 590, loss: 11.531574\n",
      "step 591, loss: 11.522321\n",
      "step 592, loss: 11.513070\n",
      "step 593, loss: 11.503824\n",
      "step 594, loss: 11.494586\n",
      "step 595, loss: 11.485353\n",
      "step 596, loss: 11.476124\n",
      "step 597, loss: 11.466902\n",
      "step 598, loss: 11.457684\n",
      "step 599, loss: 11.448472\n",
      "step 600, loss: 11.439266\n",
      "step 601, loss: 11.430066\n",
      "step 602, loss: 11.420871\n",
      "step 603, loss: 11.411679\n",
      "step 604, loss: 11.402495\n",
      "step 605, loss: 11.393316\n",
      "step 606, loss: 11.384142\n",
      "step 607, loss: 11.374974\n",
      "step 608, loss: 11.365811\n",
      "step 609, loss: 11.356654\n",
      "step 610, loss: 11.347503\n",
      "step 611, loss: 11.338356\n",
      "step 612, loss: 11.329214\n",
      "step 613, loss: 11.320077\n",
      "step 614, loss: 11.310947\n",
      "step 615, loss: 11.301823\n",
      "step 616, loss: 11.292704\n",
      "step 617, loss: 11.283588\n",
      "step 618, loss: 11.274481\n",
      "step 619, loss: 11.265376\n",
      "step 620, loss: 11.256279\n",
      "step 621, loss: 11.247187\n",
      "step 622, loss: 11.238098\n",
      "step 623, loss: 11.229016\n",
      "step 624, loss: 11.219940\n",
      "step 625, loss: 11.210871\n",
      "step 626, loss: 11.201803\n",
      "step 627, loss: 11.192743\n",
      "step 628, loss: 11.183689\n",
      "step 629, loss: 11.174639\n",
      "step 630, loss: 11.165596\n",
      "step 631, loss: 11.156555\n",
      "step 632, loss: 11.147522\n",
      "step 633, loss: 11.138494\n",
      "step 634, loss: 11.129473\n",
      "step 635, loss: 11.120454\n",
      "step 636, loss: 11.111443\n",
      "step 637, loss: 11.102435\n",
      "step 638, loss: 11.093433\n",
      "step 639, loss: 11.084439\n",
      "step 640, loss: 11.075448\n",
      "step 641, loss: 11.066463\n",
      "step 642, loss: 11.057483\n",
      "step 643, loss: 11.048509\n",
      "step 644, loss: 11.039539\n",
      "step 645, loss: 11.030575\n",
      "step 646, loss: 11.021616\n",
      "step 647, loss: 11.012664\n",
      "step 648, loss: 11.003716\n",
      "step 649, loss: 10.994773\n",
      "step 650, loss: 10.985836\n",
      "step 651, loss: 10.976906\n",
      "step 652, loss: 10.967978\n",
      "step 653, loss: 10.959058\n",
      "step 654, loss: 10.950140\n",
      "step 655, loss: 10.941232\n",
      "step 656, loss: 10.932324\n",
      "step 657, loss: 10.923424\n",
      "step 658, loss: 10.914530\n",
      "step 659, loss: 10.905641\n",
      "step 660, loss: 10.896758\n",
      "step 661, loss: 10.887878\n",
      "step 662, loss: 10.879004\n",
      "step 663, loss: 10.870136\n",
      "step 664, loss: 10.861273\n",
      "step 665, loss: 10.852417\n",
      "step 666, loss: 10.843566\n",
      "step 667, loss: 10.834718\n",
      "step 668, loss: 10.825876\n",
      "step 669, loss: 10.817039\n",
      "step 670, loss: 10.808208\n",
      "step 671, loss: 10.799382\n",
      "step 672, loss: 10.790563\n",
      "step 673, loss: 10.781747\n",
      "step 674, loss: 10.772937\n",
      "step 675, loss: 10.764132\n",
      "step 676, loss: 10.755332\n",
      "step 677, loss: 10.746539\n",
      "step 678, loss: 10.737750\n",
      "step 679, loss: 10.728968\n",
      "step 680, loss: 10.720189\n",
      "step 681, loss: 10.711416\n",
      "step 682, loss: 10.702648\n",
      "step 683, loss: 10.693886\n",
      "step 684, loss: 10.685128\n",
      "step 685, loss: 10.676375\n",
      "step 686, loss: 10.667629\n",
      "step 687, loss: 10.658888\n",
      "step 688, loss: 10.650151\n",
      "step 689, loss: 10.641421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 690, loss: 10.632695\n",
      "step 691, loss: 10.623975\n",
      "step 692, loss: 10.615259\n",
      "step 693, loss: 10.606549\n",
      "step 694, loss: 10.597844\n",
      "step 695, loss: 10.589145\n",
      "step 696, loss: 10.580450\n",
      "step 697, loss: 10.571761\n",
      "step 698, loss: 10.563077\n",
      "step 699, loss: 10.554399\n",
      "step 700, loss: 10.545725\n",
      "step 701, loss: 10.537057\n",
      "step 702, loss: 10.528394\n",
      "step 703, loss: 10.519736\n",
      "step 704, loss: 10.511084\n",
      "step 705, loss: 10.502437\n",
      "step 706, loss: 10.493793\n",
      "step 707, loss: 10.485157\n",
      "step 708, loss: 10.476524\n",
      "step 709, loss: 10.467899\n",
      "step 710, loss: 10.459277\n",
      "step 711, loss: 10.450661\n",
      "step 712, loss: 10.442051\n",
      "step 713, loss: 10.433446\n",
      "step 714, loss: 10.424844\n",
      "step 715, loss: 10.416248\n",
      "step 716, loss: 10.407659\n",
      "step 717, loss: 10.399075\n",
      "step 718, loss: 10.390494\n",
      "step 719, loss: 10.381919\n",
      "step 720, loss: 10.373349\n",
      "step 721, loss: 10.364786\n",
      "step 722, loss: 10.356226\n",
      "step 723, loss: 10.347672\n",
      "step 724, loss: 10.339123\n",
      "step 725, loss: 10.330580\n",
      "step 726, loss: 10.322042\n",
      "step 727, loss: 10.313508\n",
      "step 728, loss: 10.304980\n",
      "step 729, loss: 10.296457\n",
      "step 730, loss: 10.287941\n",
      "step 731, loss: 10.279428\n",
      "step 732, loss: 10.270921\n",
      "step 733, loss: 10.262418\n",
      "step 734, loss: 10.253921\n",
      "step 735, loss: 10.245428\n",
      "step 736, loss: 10.236942\n",
      "step 737, loss: 10.228460\n",
      "step 738, loss: 10.219984\n",
      "step 739, loss: 10.211514\n",
      "step 740, loss: 10.203046\n",
      "step 741, loss: 10.194585\n",
      "step 742, loss: 10.186130\n",
      "step 743, loss: 10.177679\n",
      "step 744, loss: 10.169232\n",
      "step 745, loss: 10.160791\n",
      "step 746, loss: 10.152356\n",
      "step 747, loss: 10.143926\n",
      "step 748, loss: 10.135500\n",
      "step 749, loss: 10.127080\n",
      "step 750, loss: 10.118665\n",
      "step 751, loss: 10.110255\n",
      "step 752, loss: 10.101850\n",
      "step 753, loss: 10.093450\n",
      "step 754, loss: 10.085055\n",
      "step 755, loss: 10.076666\n",
      "step 756, loss: 10.068282\n",
      "step 757, loss: 10.059903\n",
      "step 758, loss: 10.051528\n",
      "step 759, loss: 10.043159\n",
      "step 760, loss: 10.034794\n",
      "step 761, loss: 10.026436\n",
      "step 762, loss: 10.018083\n",
      "step 763, loss: 10.009733\n",
      "step 764, loss: 10.001390\n",
      "step 765, loss: 9.993052\n",
      "step 766, loss: 9.984718\n",
      "step 767, loss: 9.976390\n",
      "step 768, loss: 9.968067\n",
      "step 769, loss: 9.959748\n",
      "step 770, loss: 9.951434\n",
      "step 771, loss: 9.943128\n",
      "step 772, loss: 9.934825\n",
      "step 773, loss: 9.926527\n",
      "step 774, loss: 9.918234\n",
      "step 775, loss: 9.909945\n",
      "step 776, loss: 9.901663\n",
      "step 777, loss: 9.893386\n",
      "step 778, loss: 9.885113\n",
      "step 779, loss: 9.876845\n",
      "step 780, loss: 9.868584\n",
      "step 781, loss: 9.860325\n",
      "step 782, loss: 9.852074\n",
      "step 783, loss: 9.843826\n",
      "step 784, loss: 9.835584\n",
      "step 785, loss: 9.827346\n",
      "step 786, loss: 9.819115\n",
      "step 787, loss: 9.810886\n",
      "step 788, loss: 9.802664\n",
      "step 789, loss: 9.794448\n",
      "step 790, loss: 9.786236\n",
      "step 791, loss: 9.778028\n",
      "step 792, loss: 9.769826\n",
      "step 793, loss: 9.761629\n",
      "step 794, loss: 9.753437\n",
      "step 795, loss: 9.745251\n",
      "step 796, loss: 9.737068\n",
      "step 797, loss: 9.728891\n",
      "step 798, loss: 9.720719\n",
      "step 799, loss: 9.712552\n",
      "step 800, loss: 9.704391\n",
      "step 801, loss: 9.696234\n",
      "step 802, loss: 9.688082\n",
      "step 803, loss: 9.679935\n",
      "step 804, loss: 9.671793\n",
      "step 805, loss: 9.663656\n",
      "step 806, loss: 9.655524\n",
      "step 807, loss: 9.647397\n",
      "step 808, loss: 9.639277\n",
      "step 809, loss: 9.631159\n",
      "step 810, loss: 9.623048\n",
      "step 811, loss: 9.614941\n",
      "step 812, loss: 9.606839\n",
      "step 813, loss: 9.598742\n",
      "step 814, loss: 9.590651\n",
      "step 815, loss: 9.582563\n",
      "step 816, loss: 9.574482\n",
      "step 817, loss: 9.566405\n",
      "step 818, loss: 9.558333\n",
      "step 819, loss: 9.550265\n",
      "step 820, loss: 9.542205\n",
      "step 821, loss: 9.534147\n",
      "step 822, loss: 9.526095\n",
      "step 823, loss: 9.518048\n",
      "step 824, loss: 9.510006\n",
      "step 825, loss: 9.501969\n",
      "step 826, loss: 9.493937\n",
      "step 827, loss: 9.485910\n",
      "step 828, loss: 9.477887\n",
      "step 829, loss: 9.469870\n",
      "step 830, loss: 9.461857\n",
      "step 831, loss: 9.453850\n",
      "step 832, loss: 9.445848\n",
      "step 833, loss: 9.437849\n",
      "step 834, loss: 9.429857\n",
      "step 835, loss: 9.421869\n",
      "step 836, loss: 9.413887\n",
      "step 837, loss: 9.405910\n",
      "step 838, loss: 9.397937\n",
      "step 839, loss: 9.389968\n",
      "step 840, loss: 9.382005\n",
      "step 841, loss: 9.374047\n",
      "step 842, loss: 9.366094\n",
      "step 843, loss: 9.358146\n",
      "step 844, loss: 9.350202\n",
      "step 845, loss: 9.342264\n",
      "step 846, loss: 9.334332\n",
      "step 847, loss: 9.326403\n",
      "step 848, loss: 9.318478\n",
      "step 849, loss: 9.310559\n",
      "step 850, loss: 9.302647\n",
      "step 851, loss: 9.294738\n",
      "step 852, loss: 9.286832\n",
      "step 853, loss: 9.278934\n",
      "step 854, loss: 9.271039\n",
      "step 855, loss: 9.263150\n",
      "step 856, loss: 9.255265\n",
      "step 857, loss: 9.247385\n",
      "step 858, loss: 9.239511\n",
      "step 859, loss: 9.231642\n",
      "step 860, loss: 9.223776\n",
      "step 861, loss: 9.215917\n",
      "step 862, loss: 9.208061\n",
      "step 863, loss: 9.200212\n",
      "step 864, loss: 9.192366\n",
      "step 865, loss: 9.184525\n",
      "step 866, loss: 9.176691\n",
      "step 867, loss: 9.168859\n",
      "step 868, loss: 9.161035\n",
      "step 869, loss: 9.153214\n",
      "step 870, loss: 9.145398\n",
      "step 871, loss: 9.137587\n",
      "step 872, loss: 9.129780\n",
      "step 873, loss: 9.121980\n",
      "step 874, loss: 9.114183\n",
      "step 875, loss: 9.106392\n",
      "step 876, loss: 9.098605\n",
      "step 877, loss: 9.090823\n",
      "step 878, loss: 9.083046\n",
      "step 879, loss: 9.075274\n",
      "step 880, loss: 9.067506\n",
      "step 881, loss: 9.059744\n",
      "step 882, loss: 9.051987\n",
      "step 883, loss: 9.044234\n",
      "step 884, loss: 9.036487\n",
      "step 885, loss: 9.028744\n",
      "step 886, loss: 9.021005\n",
      "step 887, loss: 9.013271\n",
      "step 888, loss: 9.005543\n",
      "step 889, loss: 8.997820\n",
      "step 890, loss: 8.990101\n",
      "step 891, loss: 8.982386\n",
      "step 892, loss: 8.974676\n",
      "step 893, loss: 8.966972\n",
      "step 894, loss: 8.959273\n",
      "step 895, loss: 8.951578\n",
      "step 896, loss: 8.943888\n",
      "step 897, loss: 8.936203\n",
      "step 898, loss: 8.928523\n",
      "step 899, loss: 8.920847\n",
      "step 900, loss: 8.913177\n",
      "step 901, loss: 8.905510\n",
      "step 902, loss: 8.897850\n",
      "step 903, loss: 8.890193\n",
      "step 904, loss: 8.882542\n",
      "step 905, loss: 8.874894\n",
      "step 906, loss: 8.867253\n",
      "step 907, loss: 8.859615\n",
      "step 908, loss: 8.851984\n",
      "step 909, loss: 8.844356\n",
      "step 910, loss: 8.836734\n",
      "step 911, loss: 8.829116\n",
      "step 912, loss: 8.821502\n",
      "step 913, loss: 8.813894\n",
      "step 914, loss: 8.806291\n",
      "step 915, loss: 8.798692\n",
      "step 916, loss: 8.791097\n",
      "step 917, loss: 8.783508\n",
      "step 918, loss: 8.775924\n",
      "step 919, loss: 8.768344\n",
      "step 920, loss: 8.760769\n",
      "step 921, loss: 8.753199\n",
      "step 922, loss: 8.745633\n",
      "step 923, loss: 8.738073\n",
      "step 924, loss: 8.730516\n",
      "step 925, loss: 8.722965\n",
      "step 926, loss: 8.715420\n",
      "step 927, loss: 8.707878\n",
      "step 928, loss: 8.700340\n",
      "step 929, loss: 8.692809\n",
      "step 930, loss: 8.685282\n",
      "step 931, loss: 8.677759\n",
      "step 932, loss: 8.670241\n",
      "step 933, loss: 8.662728\n",
      "step 934, loss: 8.655220\n",
      "step 935, loss: 8.647717\n",
      "step 936, loss: 8.640218\n",
      "step 937, loss: 8.632724\n",
      "step 938, loss: 8.625234\n",
      "step 939, loss: 8.617749\n",
      "step 940, loss: 8.610270\n",
      "step 941, loss: 8.602795\n",
      "step 942, loss: 8.595324\n",
      "step 943, loss: 8.587857\n",
      "step 944, loss: 8.580397\n",
      "step 945, loss: 8.572940\n",
      "step 946, loss: 8.565489\n",
      "step 947, loss: 8.558042\n",
      "step 948, loss: 8.550599\n",
      "step 949, loss: 8.543162\n",
      "step 950, loss: 8.535729\n",
      "step 951, loss: 8.528302\n",
      "step 952, loss: 8.520878\n",
      "step 953, loss: 8.513459\n",
      "step 954, loss: 8.506045\n",
      "step 955, loss: 8.498636\n",
      "step 956, loss: 8.491232\n",
      "step 957, loss: 8.483832\n",
      "step 958, loss: 8.476438\n",
      "step 959, loss: 8.469048\n",
      "step 960, loss: 8.461660\n",
      "step 961, loss: 8.454281\n",
      "step 962, loss: 8.446904\n",
      "step 963, loss: 8.439532\n",
      "step 964, loss: 8.432165\n",
      "step 965, loss: 8.424803\n",
      "step 966, loss: 8.417446\n",
      "step 967, loss: 8.410092\n",
      "step 968, loss: 8.402744\n",
      "step 969, loss: 8.395401\n",
      "step 970, loss: 8.388062\n",
      "step 971, loss: 8.380728\n",
      "step 972, loss: 8.373398\n",
      "step 973, loss: 8.366073\n",
      "step 974, loss: 8.358753\n",
      "step 975, loss: 8.351438\n",
      "step 976, loss: 8.344127\n",
      "step 977, loss: 8.336820\n",
      "step 978, loss: 8.329518\n",
      "step 979, loss: 8.322223\n",
      "step 980, loss: 8.314929\n",
      "step 981, loss: 8.307642\n",
      "step 982, loss: 8.300359\n",
      "step 983, loss: 8.293081\n",
      "step 984, loss: 8.285807\n",
      "step 985, loss: 8.278538\n",
      "step 986, loss: 8.271274\n",
      "step 987, loss: 8.264013\n",
      "step 988, loss: 8.256759\n",
      "step 989, loss: 8.249509\n",
      "step 990, loss: 8.242263\n",
      "step 991, loss: 8.235022\n",
      "step 992, loss: 8.227785\n",
      "step 993, loss: 8.220553\n",
      "step 994, loss: 8.213326\n",
      "step 995, loss: 8.206103\n",
      "step 996, loss: 8.198886\n",
      "step 997, loss: 8.191671\n",
      "step 998, loss: 8.184464\n",
      "step 999, loss: 8.177258\n",
      "step 1000, loss: 8.170059\n",
      "step 1001, loss: 8.162864\n",
      "step 1002, loss: 8.155674\n",
      "step 1003, loss: 8.148488\n",
      "step 1004, loss: 8.141308\n",
      "step 1005, loss: 8.134130\n",
      "step 1006, loss: 8.126959\n",
      "step 1007, loss: 8.119792\n",
      "step 1008, loss: 8.112629\n",
      "step 1009, loss: 8.105471\n",
      "step 1010, loss: 8.098316\n",
      "step 1011, loss: 8.091167\n",
      "step 1012, loss: 8.084024\n",
      "step 1013, loss: 8.076884\n",
      "step 1014, loss: 8.069749\n",
      "step 1015, loss: 8.062618\n",
      "step 1016, loss: 8.055492\n",
      "step 1017, loss: 8.048370\n",
      "step 1018, loss: 8.041254\n",
      "step 1019, loss: 8.034142\n",
      "step 1020, loss: 8.027035\n",
      "step 1021, loss: 8.019932\n",
      "step 1022, loss: 8.012834\n",
      "step 1023, loss: 8.005740\n",
      "step 1024, loss: 7.998651\n",
      "step 1025, loss: 7.991566\n",
      "step 1026, loss: 7.984486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1027, loss: 7.977409\n",
      "step 1028, loss: 7.970339\n",
      "step 1029, loss: 7.963273\n",
      "step 1030, loss: 7.956212\n",
      "step 1031, loss: 7.949154\n",
      "step 1032, loss: 7.942102\n",
      "step 1033, loss: 7.935054\n",
      "step 1034, loss: 7.928010\n",
      "step 1035, loss: 7.920971\n",
      "step 1036, loss: 7.913937\n",
      "step 1037, loss: 7.906907\n",
      "step 1038, loss: 7.899882\n",
      "step 1039, loss: 7.892861\n",
      "step 1040, loss: 7.885845\n",
      "step 1041, loss: 7.878833\n",
      "step 1042, loss: 7.871826\n",
      "step 1043, loss: 7.864824\n",
      "step 1044, loss: 7.857826\n",
      "step 1045, loss: 7.850832\n",
      "step 1046, loss: 7.843843\n",
      "step 1047, loss: 7.836859\n",
      "step 1048, loss: 7.829878\n",
      "step 1049, loss: 7.822904\n",
      "step 1050, loss: 7.815933\n",
      "step 1051, loss: 7.808966\n",
      "step 1052, loss: 7.802004\n",
      "step 1053, loss: 7.795046\n",
      "step 1054, loss: 7.788094\n",
      "step 1055, loss: 7.781146\n",
      "step 1056, loss: 7.774201\n",
      "step 1057, loss: 7.767262\n",
      "step 1058, loss: 7.760326\n",
      "step 1059, loss: 7.753397\n",
      "step 1060, loss: 7.746471\n",
      "step 1061, loss: 7.739549\n",
      "step 1062, loss: 7.732632\n",
      "step 1063, loss: 7.725719\n",
      "step 1064, loss: 7.718812\n",
      "step 1065, loss: 7.711908\n",
      "step 1066, loss: 7.705009\n",
      "step 1067, loss: 7.698114\n",
      "step 1068, loss: 7.691225\n",
      "step 1069, loss: 7.684340\n",
      "step 1070, loss: 7.677458\n",
      "step 1071, loss: 7.670581\n",
      "step 1072, loss: 7.663710\n",
      "step 1073, loss: 7.656841\n",
      "step 1074, loss: 7.649979\n",
      "step 1075, loss: 7.643119\n",
      "step 1076, loss: 7.636267\n",
      "step 1077, loss: 7.629416\n",
      "step 1078, loss: 7.622571\n",
      "step 1079, loss: 7.615731\n",
      "step 1080, loss: 7.608895\n",
      "step 1081, loss: 7.602063\n",
      "step 1082, loss: 7.595235\n",
      "step 1083, loss: 7.588412\n",
      "step 1084, loss: 7.581593\n",
      "step 1085, loss: 7.574780\n",
      "step 1086, loss: 7.567970\n",
      "step 1087, loss: 7.561164\n",
      "step 1088, loss: 7.554365\n",
      "step 1089, loss: 7.547568\n",
      "step 1090, loss: 7.540776\n",
      "step 1091, loss: 7.533990\n",
      "step 1092, loss: 7.527207\n",
      "step 1093, loss: 7.520428\n",
      "step 1094, loss: 7.513655\n",
      "step 1095, loss: 7.506884\n",
      "step 1096, loss: 7.500119\n",
      "step 1097, loss: 7.493359\n",
      "step 1098, loss: 7.486603\n",
      "step 1099, loss: 7.479850\n",
      "step 1100, loss: 7.473104\n",
      "step 1101, loss: 7.466361\n",
      "step 1102, loss: 7.459622\n",
      "step 1103, loss: 7.452889\n",
      "step 1104, loss: 7.446159\n",
      "step 1105, loss: 7.439434\n",
      "step 1106, loss: 7.432713\n",
      "step 1107, loss: 7.425997\n",
      "step 1108, loss: 7.419285\n",
      "step 1109, loss: 7.412577\n",
      "step 1110, loss: 7.405874\n",
      "step 1111, loss: 7.399176\n",
      "step 1112, loss: 7.392482\n",
      "step 1113, loss: 7.385791\n",
      "step 1114, loss: 7.379105\n",
      "step 1115, loss: 7.372424\n",
      "step 1116, loss: 7.365748\n",
      "step 1117, loss: 7.359076\n",
      "step 1118, loss: 7.352408\n",
      "step 1119, loss: 7.345743\n",
      "step 1120, loss: 7.339085\n",
      "step 1121, loss: 7.332431\n",
      "step 1122, loss: 7.325780\n",
      "step 1123, loss: 7.319133\n",
      "step 1124, loss: 7.312493\n",
      "step 1125, loss: 7.305855\n",
      "step 1126, loss: 7.299222\n",
      "step 1127, loss: 7.292594\n",
      "step 1128, loss: 7.285970\n",
      "step 1129, loss: 7.279351\n",
      "step 1130, loss: 7.272736\n",
      "step 1131, loss: 7.266124\n",
      "step 1132, loss: 7.259518\n",
      "step 1133, loss: 7.252917\n",
      "step 1134, loss: 7.246318\n",
      "step 1135, loss: 7.239724\n",
      "step 1136, loss: 7.233136\n",
      "step 1137, loss: 7.226550\n",
      "step 1138, loss: 7.219971\n",
      "step 1139, loss: 7.213394\n",
      "step 1140, loss: 7.206823\n",
      "step 1141, loss: 7.200255\n",
      "step 1142, loss: 7.193693\n",
      "step 1143, loss: 7.187134\n",
      "step 1144, loss: 7.180580\n",
      "step 1145, loss: 7.174030\n",
      "step 1146, loss: 7.167485\n",
      "step 1147, loss: 7.160944\n",
      "step 1148, loss: 7.154407\n",
      "step 1149, loss: 7.147874\n",
      "step 1150, loss: 7.141346\n",
      "step 1151, loss: 7.134822\n",
      "step 1152, loss: 7.128302\n",
      "step 1153, loss: 7.121787\n",
      "step 1154, loss: 7.115277\n",
      "step 1155, loss: 7.108770\n",
      "step 1156, loss: 7.102267\n",
      "step 1157, loss: 7.095771\n",
      "step 1158, loss: 7.089277\n",
      "step 1159, loss: 7.082788\n",
      "step 1160, loss: 7.076303\n",
      "step 1161, loss: 7.069822\n",
      "step 1162, loss: 7.063346\n",
      "step 1163, loss: 7.056874\n",
      "step 1164, loss: 7.050407\n",
      "step 1165, loss: 7.043943\n",
      "step 1166, loss: 7.037484\n",
      "step 1167, loss: 7.031030\n",
      "step 1168, loss: 7.024580\n",
      "step 1169, loss: 7.018134\n",
      "step 1170, loss: 7.011691\n",
      "step 1171, loss: 7.005254\n",
      "step 1172, loss: 6.998821\n",
      "step 1173, loss: 6.992393\n",
      "step 1174, loss: 6.985968\n",
      "step 1175, loss: 6.979548\n",
      "step 1176, loss: 6.973131\n",
      "step 1177, loss: 6.966719\n",
      "step 1178, loss: 6.960312\n",
      "step 1179, loss: 6.953909\n",
      "step 1180, loss: 6.947510\n",
      "step 1181, loss: 6.941116\n",
      "step 1182, loss: 6.934725\n",
      "step 1183, loss: 6.928339\n",
      "step 1184, loss: 6.921958\n",
      "step 1185, loss: 6.915580\n",
      "step 1186, loss: 6.909207\n",
      "step 1187, loss: 6.902839\n",
      "step 1188, loss: 6.896474\n",
      "step 1189, loss: 6.890113\n",
      "step 1190, loss: 6.883757\n",
      "step 1191, loss: 6.877405\n",
      "step 1192, loss: 6.871057\n",
      "step 1193, loss: 6.864715\n",
      "step 1194, loss: 6.858375\n",
      "step 1195, loss: 6.852040\n",
      "step 1196, loss: 6.845710\n",
      "step 1197, loss: 6.839384\n",
      "step 1198, loss: 6.833062\n",
      "step 1199, loss: 6.826745\n",
      "step 1200, loss: 6.820430\n",
      "step 1201, loss: 6.814122\n",
      "step 1202, loss: 6.807817\n",
      "step 1203, loss: 6.801516\n",
      "step 1204, loss: 6.795219\n",
      "step 1205, loss: 6.788927\n",
      "step 1206, loss: 6.782639\n",
      "step 1207, loss: 6.776355\n",
      "step 1208, loss: 6.770076\n",
      "step 1209, loss: 6.763800\n",
      "step 1210, loss: 6.757528\n",
      "step 1211, loss: 6.751262\n",
      "step 1212, loss: 6.744999\n",
      "step 1213, loss: 6.738741\n",
      "step 1214, loss: 6.732486\n",
      "step 1215, loss: 6.726236\n",
      "step 1216, loss: 6.719990\n",
      "step 1217, loss: 6.713749\n",
      "step 1218, loss: 6.707512\n",
      "step 1219, loss: 6.701279\n",
      "step 1220, loss: 6.695050\n",
      "step 1221, loss: 6.688825\n",
      "step 1222, loss: 6.682604\n",
      "step 1223, loss: 6.676389\n",
      "step 1224, loss: 6.670177\n",
      "step 1225, loss: 6.663969\n",
      "step 1226, loss: 6.657763\n",
      "step 1227, loss: 6.651565\n",
      "step 1228, loss: 6.645370\n",
      "step 1229, loss: 6.639179\n",
      "step 1230, loss: 6.632991\n",
      "step 1231, loss: 6.626809\n",
      "step 1232, loss: 6.620630\n",
      "step 1233, loss: 6.614456\n",
      "step 1234, loss: 6.608286\n",
      "step 1235, loss: 6.602120\n",
      "step 1236, loss: 6.595959\n",
      "step 1237, loss: 6.589801\n",
      "step 1238, loss: 6.583647\n",
      "step 1239, loss: 6.577498\n",
      "step 1240, loss: 6.571353\n",
      "step 1241, loss: 6.565212\n",
      "step 1242, loss: 6.559075\n",
      "step 1243, loss: 6.552943\n",
      "step 1244, loss: 6.546814\n",
      "step 1245, loss: 6.540691\n",
      "step 1246, loss: 6.534571\n",
      "step 1247, loss: 6.528455\n",
      "step 1248, loss: 6.522343\n",
      "step 1249, loss: 6.516235\n",
      "step 1250, loss: 6.510132\n",
      "step 1251, loss: 6.504033\n",
      "step 1252, loss: 6.497938\n",
      "step 1253, loss: 6.491848\n",
      "step 1254, loss: 6.485760\n",
      "step 1255, loss: 6.479677\n",
      "step 1256, loss: 6.473600\n",
      "step 1257, loss: 6.467525\n",
      "step 1258, loss: 6.461454\n",
      "step 1259, loss: 6.455389\n",
      "step 1260, loss: 6.449326\n",
      "step 1261, loss: 6.443270\n",
      "step 1262, loss: 6.437216\n",
      "step 1263, loss: 6.431166\n",
      "step 1264, loss: 6.425121\n",
      "step 1265, loss: 6.419080\n",
      "step 1266, loss: 6.413042\n",
      "step 1267, loss: 6.407009\n",
      "step 1268, loss: 6.400981\n",
      "step 1269, loss: 6.394956\n",
      "step 1270, loss: 6.388936\n",
      "step 1271, loss: 6.382919\n",
      "step 1272, loss: 6.376906\n",
      "step 1273, loss: 6.370898\n",
      "step 1274, loss: 6.364894\n",
      "step 1275, loss: 6.358894\n",
      "step 1276, loss: 6.352899\n",
      "step 1277, loss: 6.346907\n",
      "step 1278, loss: 6.340919\n",
      "step 1279, loss: 6.334935\n",
      "step 1280, loss: 6.328956\n",
      "step 1281, loss: 6.322981\n",
      "step 1282, loss: 6.317008\n",
      "step 1283, loss: 6.311042\n",
      "step 1284, loss: 6.305079\n",
      "step 1285, loss: 6.299120\n",
      "step 1286, loss: 6.293166\n",
      "step 1287, loss: 6.287216\n",
      "step 1288, loss: 6.281268\n",
      "step 1289, loss: 6.275326\n",
      "step 1290, loss: 6.269387\n",
      "step 1291, loss: 6.263453\n",
      "step 1292, loss: 6.257523\n",
      "step 1293, loss: 6.251596\n",
      "step 1294, loss: 6.245674\n",
      "step 1295, loss: 6.239756\n",
      "step 1296, loss: 6.233842\n",
      "step 1297, loss: 6.227933\n",
      "step 1298, loss: 6.222026\n",
      "step 1299, loss: 6.216125\n",
      "step 1300, loss: 6.210227\n",
      "step 1301, loss: 6.204334\n",
      "step 1302, loss: 6.198443\n",
      "step 1303, loss: 6.192558\n",
      "step 1304, loss: 6.186678\n",
      "step 1305, loss: 6.180800\n",
      "step 1306, loss: 6.174926\n",
      "step 1307, loss: 6.169058\n",
      "step 1308, loss: 6.163192\n",
      "step 1309, loss: 6.157331\n",
      "step 1310, loss: 6.151474\n",
      "step 1311, loss: 6.145621\n",
      "step 1312, loss: 6.139772\n",
      "step 1313, loss: 6.133928\n",
      "step 1314, loss: 6.128087\n",
      "step 1315, loss: 6.122249\n",
      "step 1316, loss: 6.116416\n",
      "step 1317, loss: 6.110589\n",
      "step 1318, loss: 6.104764\n",
      "step 1319, loss: 6.098943\n",
      "step 1320, loss: 6.093126\n",
      "step 1321, loss: 6.087313\n",
      "step 1322, loss: 6.081504\n",
      "step 1323, loss: 6.075700\n",
      "step 1324, loss: 6.069900\n",
      "step 1325, loss: 6.064104\n",
      "step 1326, loss: 6.058311\n",
      "step 1327, loss: 6.052523\n",
      "step 1328, loss: 6.046738\n",
      "step 1329, loss: 6.040958\n",
      "step 1330, loss: 6.035182\n",
      "step 1331, loss: 6.029410\n",
      "step 1332, loss: 6.023640\n",
      "step 1333, loss: 6.017877\n",
      "step 1334, loss: 6.012117\n",
      "step 1335, loss: 6.006361\n",
      "step 1336, loss: 6.000608\n",
      "step 1337, loss: 5.994860\n",
      "step 1338, loss: 5.989116\n",
      "step 1339, loss: 5.983376\n",
      "step 1340, loss: 5.977640\n",
      "step 1341, loss: 5.971907\n",
      "step 1342, loss: 5.966178\n",
      "step 1343, loss: 5.960455\n",
      "step 1344, loss: 5.954734\n",
      "step 1345, loss: 5.949018\n",
      "step 1346, loss: 5.943305\n",
      "step 1347, loss: 5.937598\n",
      "step 1348, loss: 5.931893\n",
      "step 1349, loss: 5.926193\n",
      "step 1350, loss: 5.920497\n",
      "step 1351, loss: 5.914804\n",
      "step 1352, loss: 5.909116\n",
      "step 1353, loss: 5.903431\n",
      "step 1354, loss: 5.897751\n",
      "step 1355, loss: 5.892075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1356, loss: 5.886403\n",
      "step 1357, loss: 5.880733\n",
      "step 1358, loss: 5.875070\n",
      "step 1359, loss: 5.869409\n",
      "step 1360, loss: 5.863752\n",
      "step 1361, loss: 5.858100\n",
      "step 1362, loss: 5.852452\n",
      "step 1363, loss: 5.846807\n",
      "step 1364, loss: 5.841166\n",
      "step 1365, loss: 5.835529\n",
      "step 1366, loss: 5.829896\n",
      "step 1367, loss: 5.824268\n",
      "step 1368, loss: 5.818643\n",
      "step 1369, loss: 5.813022\n",
      "step 1370, loss: 5.807405\n",
      "step 1371, loss: 5.801791\n",
      "step 1372, loss: 5.796182\n",
      "step 1373, loss: 5.790577\n",
      "step 1374, loss: 5.784977\n",
      "step 1375, loss: 5.779379\n",
      "step 1376, loss: 5.773786\n",
      "step 1377, loss: 5.768196\n",
      "step 1378, loss: 5.762611\n",
      "step 1379, loss: 5.757030\n",
      "step 1380, loss: 5.751451\n",
      "step 1381, loss: 5.745878\n",
      "step 1382, loss: 5.740308\n",
      "step 1383, loss: 5.734743\n",
      "step 1384, loss: 5.729181\n",
      "step 1385, loss: 5.723623\n",
      "step 1386, loss: 5.718069\n",
      "step 1387, loss: 5.712519\n",
      "step 1388, loss: 5.706973\n",
      "step 1389, loss: 5.701430\n",
      "step 1390, loss: 5.695891\n",
      "step 1391, loss: 5.690357\n",
      "step 1392, loss: 5.684827\n",
      "step 1393, loss: 5.679300\n",
      "step 1394, loss: 5.673777\n",
      "step 1395, loss: 5.668258\n",
      "step 1396, loss: 5.662744\n",
      "step 1397, loss: 5.657233\n",
      "step 1398, loss: 5.651726\n",
      "step 1399, loss: 5.646222\n",
      "step 1400, loss: 5.640723\n",
      "step 1401, loss: 5.635228\n",
      "step 1402, loss: 5.629736\n",
      "step 1403, loss: 5.624249\n",
      "step 1404, loss: 5.618765\n",
      "step 1405, loss: 5.613286\n",
      "step 1406, loss: 5.607809\n",
      "step 1407, loss: 5.602337\n",
      "step 1408, loss: 5.596869\n",
      "step 1409, loss: 5.591405\n",
      "step 1410, loss: 5.585944\n",
      "step 1411, loss: 5.580488\n",
      "step 1412, loss: 5.575035\n",
      "step 1413, loss: 5.569586\n",
      "step 1414, loss: 5.564141\n",
      "step 1415, loss: 5.558701\n",
      "step 1416, loss: 5.553264\n",
      "step 1417, loss: 5.547830\n",
      "step 1418, loss: 5.542400\n",
      "step 1419, loss: 5.536975\n",
      "step 1420, loss: 5.531553\n",
      "step 1421, loss: 5.526136\n",
      "step 1422, loss: 5.520722\n",
      "step 1423, loss: 5.515311\n",
      "step 1424, loss: 5.509905\n",
      "step 1425, loss: 5.504503\n",
      "step 1426, loss: 5.499104\n",
      "step 1427, loss: 5.493711\n",
      "step 1428, loss: 5.488319\n",
      "step 1429, loss: 5.482932\n",
      "step 1430, loss: 5.477550\n",
      "step 1431, loss: 5.472170\n",
      "step 1432, loss: 5.466794\n",
      "step 1433, loss: 5.461423\n",
      "step 1434, loss: 5.456055\n",
      "step 1435, loss: 5.450690\n",
      "step 1436, loss: 5.445331\n",
      "step 1437, loss: 5.439974\n",
      "step 1438, loss: 5.434622\n",
      "step 1439, loss: 5.429274\n",
      "step 1440, loss: 5.423929\n",
      "step 1441, loss: 5.418588\n",
      "step 1442, loss: 5.413251\n",
      "step 1443, loss: 5.407918\n",
      "step 1444, loss: 5.402588\n",
      "step 1445, loss: 5.397263\n",
      "step 1446, loss: 5.391940\n",
      "step 1447, loss: 5.386623\n",
      "step 1448, loss: 5.381309\n",
      "step 1449, loss: 5.375998\n",
      "step 1450, loss: 5.370692\n",
      "step 1451, loss: 5.365389\n",
      "step 1452, loss: 5.360090\n",
      "step 1453, loss: 5.354795\n",
      "step 1454, loss: 5.349504\n",
      "step 1455, loss: 5.344216\n",
      "step 1456, loss: 5.338933\n",
      "step 1457, loss: 5.333653\n",
      "step 1458, loss: 5.328376\n",
      "step 1459, loss: 5.323104\n",
      "step 1460, loss: 5.317836\n",
      "step 1461, loss: 5.312572\n",
      "step 1462, loss: 5.307310\n",
      "step 1463, loss: 5.302053\n",
      "step 1464, loss: 5.296800\n",
      "step 1465, loss: 5.291550\n",
      "step 1466, loss: 5.286305\n",
      "step 1467, loss: 5.281063\n",
      "step 1468, loss: 5.275825\n",
      "step 1469, loss: 5.270590\n",
      "step 1470, loss: 5.265359\n",
      "step 1471, loss: 5.260132\n",
      "step 1472, loss: 5.254910\n",
      "step 1473, loss: 5.249691\n",
      "step 1474, loss: 5.244474\n",
      "step 1475, loss: 5.239264\n",
      "step 1476, loss: 5.234055\n",
      "step 1477, loss: 5.228851\n",
      "step 1478, loss: 5.223650\n",
      "step 1479, loss: 5.218454\n",
      "step 1480, loss: 5.213261\n",
      "step 1481, loss: 5.208072\n",
      "step 1482, loss: 5.202887\n",
      "step 1483, loss: 5.197704\n",
      "step 1484, loss: 5.192527\n",
      "step 1485, loss: 5.187353\n",
      "step 1486, loss: 5.182183\n",
      "step 1487, loss: 5.177016\n",
      "step 1488, loss: 5.171853\n",
      "step 1489, loss: 5.166694\n",
      "step 1490, loss: 5.161539\n",
      "step 1491, loss: 5.156387\n",
      "step 1492, loss: 5.151239\n",
      "step 1493, loss: 5.146096\n",
      "step 1494, loss: 5.140954\n",
      "step 1495, loss: 5.135818\n",
      "step 1496, loss: 5.130685\n",
      "step 1497, loss: 5.125556\n",
      "step 1498, loss: 5.120431\n",
      "step 1499, loss: 5.115309\n",
      "step 1500, loss: 5.110191\n",
      "step 1501, loss: 5.105076\n",
      "step 1502, loss: 5.099966\n",
      "step 1503, loss: 5.094859\n",
      "step 1504, loss: 5.089756\n",
      "step 1505, loss: 5.084657\n",
      "step 1506, loss: 5.079561\n",
      "step 1507, loss: 5.074469\n",
      "step 1508, loss: 5.069381\n",
      "step 1509, loss: 5.064297\n",
      "step 1510, loss: 5.059216\n",
      "step 1511, loss: 5.054140\n",
      "step 1512, loss: 5.049066\n",
      "step 1513, loss: 5.043997\n",
      "step 1514, loss: 5.038931\n",
      "step 1515, loss: 5.033869\n",
      "step 1516, loss: 5.028811\n",
      "step 1517, loss: 5.023757\n",
      "step 1518, loss: 5.018706\n",
      "step 1519, loss: 5.013659\n",
      "step 1520, loss: 5.008615\n",
      "step 1521, loss: 5.003575\n",
      "step 1522, loss: 4.998539\n",
      "step 1523, loss: 4.993506\n",
      "step 1524, loss: 4.988478\n",
      "step 1525, loss: 4.983452\n",
      "step 1526, loss: 4.978431\n",
      "step 1527, loss: 4.973413\n",
      "step 1528, loss: 4.968399\n",
      "step 1529, loss: 4.963388\n",
      "step 1530, loss: 4.958382\n",
      "step 1531, loss: 4.953380\n",
      "step 1532, loss: 4.948380\n",
      "step 1533, loss: 4.943385\n",
      "step 1534, loss: 4.938392\n",
      "step 1535, loss: 4.933404\n",
      "step 1536, loss: 4.928420\n",
      "step 1537, loss: 4.923439\n",
      "step 1538, loss: 4.918461\n",
      "step 1539, loss: 4.913487\n",
      "step 1540, loss: 4.908518\n",
      "step 1541, loss: 4.903551\n",
      "step 1542, loss: 4.898589\n",
      "step 1543, loss: 4.893630\n",
      "step 1544, loss: 4.888674\n",
      "step 1545, loss: 4.883723\n",
      "step 1546, loss: 4.878775\n",
      "step 1547, loss: 4.873831\n",
      "step 1548, loss: 4.868890\n",
      "step 1549, loss: 4.863953\n",
      "step 1550, loss: 4.859019\n",
      "step 1551, loss: 4.854090\n",
      "step 1552, loss: 4.849163\n",
      "step 1553, loss: 4.844241\n",
      "step 1554, loss: 4.839322\n",
      "step 1555, loss: 4.834407\n",
      "step 1556, loss: 4.829496\n",
      "step 1557, loss: 4.824588\n",
      "step 1558, loss: 4.819683\n",
      "step 1559, loss: 4.814783\n",
      "step 1560, loss: 4.809886\n",
      "step 1561, loss: 4.804992\n",
      "step 1562, loss: 4.800102\n",
      "step 1563, loss: 4.795217\n",
      "step 1564, loss: 4.790334\n",
      "step 1565, loss: 4.785455\n",
      "step 1566, loss: 4.780581\n",
      "step 1567, loss: 4.775708\n",
      "step 1568, loss: 4.770840\n",
      "step 1569, loss: 4.765976\n",
      "step 1570, loss: 4.761115\n",
      "step 1571, loss: 4.756258\n",
      "step 1572, loss: 4.751405\n",
      "step 1573, loss: 4.746554\n",
      "step 1574, loss: 4.741708\n",
      "step 1575, loss: 4.736865\n",
      "step 1576, loss: 4.732027\n",
      "step 1577, loss: 4.727191\n",
      "step 1578, loss: 4.722359\n",
      "step 1579, loss: 4.717531\n",
      "step 1580, loss: 4.712707\n",
      "step 1581, loss: 4.707886\n",
      "step 1582, loss: 4.703068\n",
      "step 1583, loss: 4.698255\n",
      "step 1584, loss: 4.693444\n",
      "step 1585, loss: 4.688638\n",
      "step 1586, loss: 4.683835\n",
      "step 1587, loss: 4.679035\n",
      "step 1588, loss: 4.674240\n",
      "step 1589, loss: 4.669447\n",
      "step 1590, loss: 4.664659\n",
      "step 1591, loss: 4.659873\n",
      "step 1592, loss: 4.655092\n",
      "step 1593, loss: 4.650314\n",
      "step 1594, loss: 4.645540\n",
      "step 1595, loss: 4.640769\n",
      "step 1596, loss: 4.636002\n",
      "step 1597, loss: 4.631238\n",
      "step 1598, loss: 4.626478\n",
      "step 1599, loss: 4.621721\n",
      "step 1600, loss: 4.616969\n",
      "step 1601, loss: 4.612219\n",
      "step 1602, loss: 4.607473\n",
      "step 1603, loss: 4.602731\n",
      "step 1604, loss: 4.597992\n",
      "step 1605, loss: 4.593257\n",
      "step 1606, loss: 4.588526\n",
      "step 1607, loss: 4.583798\n",
      "step 1608, loss: 4.579074\n",
      "step 1609, loss: 4.574353\n",
      "step 1610, loss: 4.569635\n",
      "step 1611, loss: 4.564921\n",
      "step 1612, loss: 4.560211\n",
      "step 1613, loss: 4.555504\n",
      "step 1614, loss: 4.550801\n",
      "step 1615, loss: 4.546102\n",
      "step 1616, loss: 4.541405\n",
      "step 1617, loss: 4.536713\n",
      "step 1618, loss: 4.532024\n",
      "step 1619, loss: 4.527338\n",
      "step 1620, loss: 4.522656\n",
      "step 1621, loss: 4.517977\n",
      "step 1622, loss: 4.513303\n",
      "step 1623, loss: 4.508631\n",
      "step 1624, loss: 4.503963\n",
      "step 1625, loss: 4.499300\n",
      "step 1626, loss: 4.494639\n",
      "step 1627, loss: 4.489982\n",
      "step 1628, loss: 4.485328\n",
      "step 1629, loss: 4.480679\n",
      "step 1630, loss: 4.476031\n",
      "step 1631, loss: 4.471388\n",
      "step 1632, loss: 4.466748\n",
      "step 1633, loss: 4.462113\n",
      "step 1634, loss: 4.457480\n",
      "step 1635, loss: 4.452851\n",
      "step 1636, loss: 4.448225\n",
      "step 1637, loss: 4.443604\n",
      "step 1638, loss: 4.438986\n",
      "step 1639, loss: 4.434370\n",
      "step 1640, loss: 4.429759\n",
      "step 1641, loss: 4.425151\n",
      "step 1642, loss: 4.420546\n",
      "step 1643, loss: 4.415946\n",
      "step 1644, loss: 4.411348\n",
      "step 1645, loss: 4.406754\n",
      "step 1646, loss: 4.402164\n",
      "step 1647, loss: 4.397576\n",
      "step 1648, loss: 4.392992\n",
      "step 1649, loss: 4.388413\n",
      "step 1650, loss: 4.383836\n",
      "step 1651, loss: 4.379263\n",
      "step 1652, loss: 4.374693\n",
      "step 1653, loss: 4.370127\n",
      "step 1654, loss: 4.365564\n",
      "step 1655, loss: 4.361005\n",
      "step 1656, loss: 4.356450\n",
      "step 1657, loss: 4.351897\n",
      "step 1658, loss: 4.347349\n",
      "step 1659, loss: 4.342803\n",
      "step 1660, loss: 4.338262\n",
      "step 1661, loss: 4.333723\n",
      "step 1662, loss: 4.329188\n",
      "step 1663, loss: 4.324657\n",
      "step 1664, loss: 4.320129\n",
      "step 1665, loss: 4.315605\n",
      "step 1666, loss: 4.311084\n",
      "step 1667, loss: 4.306567\n",
      "step 1668, loss: 4.302053\n",
      "step 1669, loss: 4.297543\n",
      "step 1670, loss: 4.293036\n",
      "step 1671, loss: 4.288532\n",
      "step 1672, loss: 4.284031\n",
      "step 1673, loss: 4.279535\n",
      "step 1674, loss: 4.275042\n",
      "step 1675, loss: 4.270552\n",
      "step 1676, loss: 4.266065\n",
      "step 1677, loss: 4.261583\n",
      "step 1678, loss: 4.257102\n",
      "step 1679, loss: 4.252626\n",
      "step 1680, loss: 4.248155\n",
      "step 1681, loss: 4.243685\n",
      "step 1682, loss: 4.239219\n",
      "step 1683, loss: 4.234756\n",
      "step 1684, loss: 4.230298\n",
      "step 1685, loss: 4.225842\n",
      "step 1686, loss: 4.221390\n",
      "step 1687, loss: 4.216941\n",
      "step 1688, loss: 4.212496\n",
      "step 1689, loss: 4.208055\n",
      "step 1690, loss: 4.203616\n",
      "step 1691, loss: 4.199182\n",
      "step 1692, loss: 4.194750\n",
      "step 1693, loss: 4.190322\n",
      "step 1694, loss: 4.185897\n",
      "step 1695, loss: 4.181477\n",
      "step 1696, loss: 4.177059\n",
      "step 1697, loss: 4.172644\n",
      "step 1698, loss: 4.168233\n",
      "step 1699, loss: 4.163826\n",
      "step 1700, loss: 4.159422\n",
      "step 1701, loss: 4.155022\n",
      "step 1702, loss: 4.150624\n",
      "step 1703, loss: 4.146231\n",
      "step 1704, loss: 4.141840\n",
      "step 1705, loss: 4.137454\n",
      "step 1706, loss: 4.133070\n",
      "step 1707, loss: 4.128689\n",
      "step 1708, loss: 4.124312\n",
      "step 1709, loss: 4.119939\n",
      "step 1710, loss: 4.115568\n",
      "step 1711, loss: 4.111201\n",
      "step 1712, loss: 4.106838\n",
      "step 1713, loss: 4.102479\n",
      "step 1714, loss: 4.098122\n",
      "step 1715, loss: 4.093768\n",
      "step 1716, loss: 4.089419\n",
      "step 1717, loss: 4.085073\n",
      "step 1718, loss: 4.080729\n",
      "step 1719, loss: 4.076390\n",
      "step 1720, loss: 4.072053\n",
      "step 1721, loss: 4.067720\n",
      "step 1722, loss: 4.063391\n",
      "step 1723, loss: 4.059065\n",
      "step 1724, loss: 4.054742\n",
      "step 1725, loss: 4.050423\n",
      "step 1726, loss: 4.046107\n",
      "step 1727, loss: 4.041795\n",
      "step 1728, loss: 4.037485\n",
      "step 1729, loss: 4.033179\n",
      "step 1730, loss: 4.028877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1731, loss: 4.024578\n",
      "step 1732, loss: 4.020282\n",
      "step 1733, loss: 4.015990\n",
      "step 1734, loss: 4.011700\n",
      "step 1735, loss: 4.007414\n",
      "step 1736, loss: 4.003132\n",
      "step 1737, loss: 3.998853\n",
      "step 1738, loss: 3.994577\n",
      "step 1739, loss: 3.990304\n",
      "step 1740, loss: 3.986036\n",
      "step 1741, loss: 3.981769\n",
      "step 1742, loss: 3.977508\n",
      "step 1743, loss: 3.973248\n",
      "step 1744, loss: 3.968993\n",
      "step 1745, loss: 3.964740\n",
      "step 1746, loss: 3.960492\n",
      "step 1747, loss: 3.956246\n",
      "step 1748, loss: 3.952004\n",
      "step 1749, loss: 3.947766\n",
      "step 1750, loss: 3.943530\n",
      "step 1751, loss: 3.939297\n",
      "step 1752, loss: 3.935068\n",
      "step 1753, loss: 3.930843\n",
      "step 1754, loss: 3.926620\n",
      "step 1755, loss: 3.922402\n",
      "step 1756, loss: 3.918186\n",
      "step 1757, loss: 3.913974\n",
      "step 1758, loss: 3.909764\n",
      "step 1759, loss: 3.905559\n",
      "step 1760, loss: 3.901356\n",
      "step 1761, loss: 3.897158\n",
      "step 1762, loss: 3.892962\n",
      "step 1763, loss: 3.888769\n",
      "step 1764, loss: 3.884580\n",
      "step 1765, loss: 3.880394\n",
      "step 1766, loss: 3.876212\n",
      "step 1767, loss: 3.872032\n",
      "step 1768, loss: 3.867857\n",
      "step 1769, loss: 3.863684\n",
      "step 1770, loss: 3.859514\n",
      "step 1771, loss: 3.855349\n",
      "step 1772, loss: 3.851186\n",
      "step 1773, loss: 3.847026\n",
      "step 1774, loss: 3.842870\n",
      "step 1775, loss: 3.838718\n",
      "step 1776, loss: 3.834568\n",
      "step 1777, loss: 3.830422\n",
      "step 1778, loss: 3.826279\n",
      "step 1779, loss: 3.822139\n",
      "step 1780, loss: 3.818003\n",
      "step 1781, loss: 3.813870\n",
      "step 1782, loss: 3.809740\n",
      "step 1783, loss: 3.805614\n",
      "step 1784, loss: 3.801491\n",
      "step 1785, loss: 3.797371\n",
      "step 1786, loss: 3.793253\n",
      "step 1787, loss: 3.789140\n",
      "step 1788, loss: 3.785030\n",
      "step 1789, loss: 3.780923\n",
      "step 1790, loss: 3.776820\n",
      "step 1791, loss: 3.772719\n",
      "step 1792, loss: 3.768622\n",
      "step 1793, loss: 3.764529\n",
      "step 1794, loss: 3.760438\n",
      "step 1795, loss: 3.756350\n",
      "step 1796, loss: 3.752267\n",
      "step 1797, loss: 3.748186\n",
      "step 1798, loss: 3.744108\n",
      "step 1799, loss: 3.740035\n",
      "step 1800, loss: 3.735963\n",
      "step 1801, loss: 3.731895\n",
      "step 1802, loss: 3.727830\n",
      "step 1803, loss: 3.723769\n",
      "step 1804, loss: 3.719711\n",
      "step 1805, loss: 3.715657\n",
      "step 1806, loss: 3.711606\n",
      "step 1807, loss: 3.707557\n",
      "step 1808, loss: 3.703512\n",
      "step 1809, loss: 3.699470\n",
      "step 1810, loss: 3.695431\n",
      "step 1811, loss: 3.691396\n",
      "step 1812, loss: 3.687365\n",
      "step 1813, loss: 3.683335\n",
      "step 1814, loss: 3.679309\n",
      "step 1815, loss: 3.675287\n",
      "step 1816, loss: 3.671268\n",
      "step 1817, loss: 3.667252\n",
      "step 1818, loss: 3.663239\n",
      "step 1819, loss: 3.659230\n",
      "step 1820, loss: 3.655223\n",
      "step 1821, loss: 3.651221\n",
      "step 1822, loss: 3.647220\n",
      "step 1823, loss: 3.643224\n",
      "step 1824, loss: 3.639231\n",
      "step 1825, loss: 3.635241\n",
      "step 1826, loss: 3.631254\n",
      "step 1827, loss: 3.627270\n",
      "step 1828, loss: 3.623290\n",
      "step 1829, loss: 3.619313\n",
      "step 1830, loss: 3.615338\n",
      "step 1831, loss: 3.611368\n",
      "step 1832, loss: 3.607400\n",
      "step 1833, loss: 3.603436\n",
      "step 1834, loss: 3.599475\n",
      "step 1835, loss: 3.595516\n",
      "step 1836, loss: 3.591562\n",
      "step 1837, loss: 3.587610\n",
      "step 1838, loss: 3.583661\n",
      "step 1839, loss: 3.579716\n",
      "step 1840, loss: 3.575774\n",
      "step 1841, loss: 3.571835\n",
      "step 1842, loss: 3.567899\n",
      "step 1843, loss: 3.563966\n",
      "step 1844, loss: 3.560037\n",
      "step 1845, loss: 3.556110\n",
      "step 1846, loss: 3.552187\n",
      "step 1847, loss: 3.548268\n",
      "step 1848, loss: 3.544351\n",
      "step 1849, loss: 3.540438\n",
      "step 1850, loss: 3.536527\n",
      "step 1851, loss: 3.532620\n",
      "step 1852, loss: 3.528716\n",
      "step 1853, loss: 3.524815\n",
      "step 1854, loss: 3.520917\n",
      "step 1855, loss: 3.517023\n",
      "step 1856, loss: 3.513132\n",
      "step 1857, loss: 3.509243\n",
      "step 1858, loss: 3.505359\n",
      "step 1859, loss: 3.501477\n",
      "step 1860, loss: 3.497599\n",
      "step 1861, loss: 3.493723\n",
      "step 1862, loss: 3.489851\n",
      "step 1863, loss: 3.485982\n",
      "step 1864, loss: 3.482116\n",
      "step 1865, loss: 3.478253\n",
      "step 1866, loss: 3.474394\n",
      "step 1867, loss: 3.470537\n",
      "step 1868, loss: 3.466683\n",
      "step 1869, loss: 3.462834\n",
      "step 1870, loss: 3.458986\n",
      "step 1871, loss: 3.455142\n",
      "step 1872, loss: 3.451302\n",
      "step 1873, loss: 3.447465\n",
      "step 1874, loss: 3.443630\n",
      "step 1875, loss: 3.439799\n",
      "step 1876, loss: 3.435971\n",
      "step 1877, loss: 3.432146\n",
      "step 1878, loss: 3.428324\n",
      "step 1879, loss: 3.424506\n",
      "step 1880, loss: 3.420689\n",
      "step 1881, loss: 3.416878\n",
      "step 1882, loss: 3.413068\n",
      "step 1883, loss: 3.409262\n",
      "step 1884, loss: 3.405459\n",
      "step 1885, loss: 3.401659\n",
      "step 1886, loss: 3.397862\n",
      "step 1887, loss: 3.394068\n",
      "step 1888, loss: 3.390277\n",
      "step 1889, loss: 3.386490\n",
      "step 1890, loss: 3.382706\n",
      "step 1891, loss: 3.378925\n",
      "step 1892, loss: 3.375146\n",
      "step 1893, loss: 3.371372\n",
      "step 1894, loss: 3.367599\n",
      "step 1895, loss: 3.363831\n",
      "step 1896, loss: 3.360065\n",
      "step 1897, loss: 3.356302\n",
      "step 1898, loss: 3.352543\n",
      "step 1899, loss: 3.348787\n",
      "step 1900, loss: 3.345033\n",
      "step 1901, loss: 3.341283\n",
      "step 1902, loss: 3.337536\n",
      "step 1903, loss: 3.333792\n",
      "step 1904, loss: 3.330051\n",
      "step 1905, loss: 3.326314\n",
      "step 1906, loss: 3.322579\n",
      "step 1907, loss: 3.318847\n",
      "step 1908, loss: 3.315119\n",
      "step 1909, loss: 3.311393\n",
      "step 1910, loss: 3.307671\n",
      "step 1911, loss: 3.303952\n",
      "step 1912, loss: 3.300235\n",
      "step 1913, loss: 3.296523\n",
      "step 1914, loss: 3.292813\n",
      "step 1915, loss: 3.289105\n",
      "step 1916, loss: 3.285402\n",
      "step 1917, loss: 3.281701\n",
      "step 1918, loss: 3.278003\n",
      "step 1919, loss: 3.274308\n",
      "step 1920, loss: 3.270617\n",
      "step 1921, loss: 3.266928\n",
      "step 1922, loss: 3.263243\n",
      "step 1923, loss: 3.259561\n",
      "step 1924, loss: 3.255881\n",
      "step 1925, loss: 3.252206\n",
      "step 1926, loss: 3.248532\n",
      "step 1927, loss: 3.244862\n",
      "step 1928, loss: 3.241194\n",
      "step 1929, loss: 3.237530\n",
      "step 1930, loss: 3.233869\n",
      "step 1931, loss: 3.230212\n",
      "step 1932, loss: 3.226556\n",
      "step 1933, loss: 3.222905\n",
      "step 1934, loss: 3.219256\n",
      "step 1935, loss: 3.215610\n",
      "step 1936, loss: 3.211968\n",
      "step 1937, loss: 3.208328\n",
      "step 1938, loss: 3.204691\n",
      "step 1939, loss: 3.201058\n",
      "step 1940, loss: 3.197427\n",
      "step 1941, loss: 3.193800\n",
      "step 1942, loss: 3.190175\n",
      "step 1943, loss: 3.186554\n",
      "step 1944, loss: 3.182935\n",
      "step 1945, loss: 3.179320\n",
      "step 1946, loss: 3.175707\n",
      "step 1947, loss: 3.172098\n",
      "step 1948, loss: 3.168492\n",
      "step 1949, loss: 3.164889\n",
      "step 1950, loss: 3.161289\n",
      "step 1951, loss: 3.157691\n",
      "step 1952, loss: 3.154097\n",
      "step 1953, loss: 3.150506\n",
      "step 1954, loss: 3.146918\n",
      "step 1955, loss: 3.143332\n",
      "step 1956, loss: 3.139751\n",
      "step 1957, loss: 3.136172\n",
      "step 1958, loss: 3.132595\n",
      "step 1959, loss: 3.129022\n",
      "step 1960, loss: 3.125452\n",
      "step 1961, loss: 3.121885\n",
      "step 1962, loss: 3.118321\n",
      "step 1963, loss: 3.114760\n",
      "step 1964, loss: 3.111202\n",
      "step 1965, loss: 3.107647\n",
      "step 1966, loss: 3.104095\n",
      "step 1967, loss: 3.100546\n",
      "step 1968, loss: 3.096999\n",
      "step 1969, loss: 3.093457\n",
      "step 1970, loss: 3.089917\n",
      "step 1971, loss: 3.086380\n",
      "step 1972, loss: 3.082846\n",
      "step 1973, loss: 3.079315\n",
      "step 1974, loss: 3.075787\n",
      "step 1975, loss: 3.072262\n",
      "step 1976, loss: 3.068740\n",
      "step 1977, loss: 3.065221\n",
      "step 1978, loss: 3.061705\n",
      "step 1979, loss: 3.058191\n",
      "step 1980, loss: 3.054682\n",
      "step 1981, loss: 3.051175\n",
      "step 1982, loss: 3.047671\n",
      "step 1983, loss: 3.044169\n",
      "step 1984, loss: 3.040671\n",
      "step 1985, loss: 3.037175\n",
      "step 1986, loss: 3.033683\n",
      "step 1987, loss: 3.030194\n",
      "step 1988, loss: 3.026707\n",
      "step 1989, loss: 3.023224\n",
      "step 1990, loss: 3.019743\n",
      "step 1991, loss: 3.016266\n",
      "step 1992, loss: 3.012792\n",
      "step 1993, loss: 3.009320\n",
      "step 1994, loss: 3.005851\n",
      "step 1995, loss: 3.002385\n",
      "step 1996, loss: 2.998923\n",
      "step 1997, loss: 2.995463\n",
      "step 1998, loss: 2.992006\n",
      "step 1999, loss: 2.988553\n",
      "step 2000, loss: 2.985102\n",
      "step 2001, loss: 2.981654\n",
      "step 2002, loss: 2.978209\n",
      "step 2003, loss: 2.974767\n",
      "step 2004, loss: 2.971328\n",
      "step 2005, loss: 2.967892\n",
      "step 2006, loss: 2.964459\n",
      "step 2007, loss: 2.961028\n",
      "step 2008, loss: 2.957601\n",
      "step 2009, loss: 2.954177\n",
      "step 2010, loss: 2.950756\n",
      "step 2011, loss: 2.947337\n",
      "step 2012, loss: 2.943922\n",
      "step 2013, loss: 2.940509\n",
      "step 2014, loss: 2.937099\n",
      "step 2015, loss: 2.933692\n",
      "step 2016, loss: 2.930289\n",
      "step 2017, loss: 2.926888\n",
      "step 2018, loss: 2.923490\n",
      "step 2019, loss: 2.920094\n",
      "step 2020, loss: 2.916703\n",
      "step 2021, loss: 2.913314\n",
      "step 2022, loss: 2.909928\n",
      "step 2023, loss: 2.906544\n",
      "step 2024, loss: 2.903164\n",
      "step 2025, loss: 2.899786\n",
      "step 2026, loss: 2.896412\n",
      "step 2027, loss: 2.893039\n",
      "step 2028, loss: 2.889671\n",
      "step 2029, loss: 2.886305\n",
      "step 2030, loss: 2.882942\n",
      "step 2031, loss: 2.879581\n",
      "step 2032, loss: 2.876224\n",
      "step 2033, loss: 2.872870\n",
      "step 2034, loss: 2.869519\n",
      "step 2035, loss: 2.866171\n",
      "step 2036, loss: 2.862825\n",
      "step 2037, loss: 2.859482\n",
      "step 2038, loss: 2.856142\n",
      "step 2039, loss: 2.852805\n",
      "step 2040, loss: 2.849471\n",
      "step 2041, loss: 2.846140\n",
      "step 2042, loss: 2.842812\n",
      "step 2043, loss: 2.839487\n",
      "step 2044, loss: 2.836164\n",
      "step 2045, loss: 2.832844\n",
      "step 2046, loss: 2.829528\n",
      "step 2047, loss: 2.826214\n",
      "step 2048, loss: 2.822902\n",
      "step 2049, loss: 2.819595\n",
      "step 2050, loss: 2.816290\n",
      "step 2051, loss: 2.812987\n",
      "step 2052, loss: 2.809688\n",
      "step 2053, loss: 2.806392\n",
      "step 2054, loss: 2.803098\n",
      "step 2055, loss: 2.799807\n",
      "step 2056, loss: 2.796519\n",
      "step 2057, loss: 2.793234\n",
      "step 2058, loss: 2.789952\n",
      "step 2059, loss: 2.786672\n",
      "step 2060, loss: 2.783396\n",
      "step 2061, loss: 2.780122\n",
      "step 2062, loss: 2.776851\n",
      "step 2063, loss: 2.773584\n",
      "step 2064, loss: 2.770319\n",
      "step 2065, loss: 2.767056\n",
      "step 2066, loss: 2.763797\n",
      "step 2067, loss: 2.760540\n",
      "step 2068, loss: 2.757286\n",
      "step 2069, loss: 2.754036\n",
      "step 2070, loss: 2.750788\n",
      "step 2071, loss: 2.747543\n",
      "step 2072, loss: 2.744301\n",
      "step 2073, loss: 2.741061\n",
      "step 2074, loss: 2.737824\n",
      "step 2075, loss: 2.734590\n",
      "step 2076, loss: 2.731360\n",
      "step 2077, loss: 2.728132\n",
      "step 2078, loss: 2.724907\n",
      "step 2079, loss: 2.721684\n",
      "step 2080, loss: 2.718465\n",
      "step 2081, loss: 2.715248\n",
      "step 2082, loss: 2.712034\n",
      "step 2083, loss: 2.708823\n",
      "step 2084, loss: 2.705615\n",
      "step 2085, loss: 2.702409\n",
      "step 2086, loss: 2.699207\n",
      "step 2087, loss: 2.696007\n",
      "step 2088, loss: 2.692810\n",
      "step 2089, loss: 2.689616\n",
      "step 2090, loss: 2.686425\n",
      "step 2091, loss: 2.683236\n",
      "step 2092, loss: 2.680050\n",
      "step 2093, loss: 2.676867\n",
      "step 2094, loss: 2.673688\n",
      "step 2095, loss: 2.670510\n",
      "step 2096, loss: 2.667336\n",
      "step 2097, loss: 2.664164\n",
      "step 2098, loss: 2.660995\n",
      "step 2099, loss: 2.657829\n",
      "step 2100, loss: 2.654666\n",
      "step 2101, loss: 2.651505\n",
      "step 2102, loss: 2.648348\n",
      "step 2103, loss: 2.645194\n",
      "step 2104, loss: 2.642042\n",
      "step 2105, loss: 2.638892\n",
      "step 2106, loss: 2.635746\n",
      "step 2107, loss: 2.632602\n",
      "step 2108, loss: 2.629461\n",
      "step 2109, loss: 2.626323\n",
      "step 2110, loss: 2.623188\n",
      "step 2111, loss: 2.620056\n",
      "step 2112, loss: 2.616926\n",
      "step 2113, loss: 2.613799\n",
      "step 2114, loss: 2.610675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2115, loss: 2.607553\n",
      "step 2116, loss: 2.604435\n",
      "step 2117, loss: 2.601319\n",
      "step 2118, loss: 2.598206\n",
      "step 2119, loss: 2.595096\n",
      "step 2120, loss: 2.591989\n",
      "step 2121, loss: 2.588884\n",
      "step 2122, loss: 2.585782\n",
      "step 2123, loss: 2.582683\n",
      "step 2124, loss: 2.579587\n",
      "step 2125, loss: 2.576492\n",
      "step 2126, loss: 2.573401\n",
      "step 2127, loss: 2.570314\n",
      "step 2128, loss: 2.567228\n",
      "step 2129, loss: 2.564146\n",
      "step 2130, loss: 2.561066\n",
      "step 2131, loss: 2.557989\n",
      "step 2132, loss: 2.554915\n",
      "step 2133, loss: 2.551843\n",
      "step 2134, loss: 2.548774\n",
      "step 2135, loss: 2.545709\n",
      "step 2136, loss: 2.542645\n",
      "step 2137, loss: 2.539585\n",
      "step 2138, loss: 2.536527\n",
      "step 2139, loss: 2.533472\n",
      "step 2140, loss: 2.530420\n",
      "step 2141, loss: 2.527370\n",
      "step 2142, loss: 2.524324\n",
      "step 2143, loss: 2.521279\n",
      "step 2144, loss: 2.518238\n",
      "step 2145, loss: 2.515200\n",
      "step 2146, loss: 2.512164\n",
      "step 2147, loss: 2.509131\n",
      "step 2148, loss: 2.506100\n",
      "step 2149, loss: 2.503073\n",
      "step 2150, loss: 2.500048\n",
      "step 2151, loss: 2.497025\n",
      "step 2152, loss: 2.494006\n",
      "step 2153, loss: 2.490989\n",
      "step 2154, loss: 2.487976\n",
      "step 2155, loss: 2.484964\n",
      "step 2156, loss: 2.481956\n",
      "step 2157, loss: 2.478950\n",
      "step 2158, loss: 2.475947\n",
      "step 2159, loss: 2.472946\n",
      "step 2160, loss: 2.469949\n",
      "step 2161, loss: 2.466954\n",
      "step 2162, loss: 2.463962\n",
      "step 2163, loss: 2.460972\n",
      "step 2164, loss: 2.457985\n",
      "step 2165, loss: 2.455001\n",
      "step 2166, loss: 2.452020\n",
      "step 2167, loss: 2.449041\n",
      "step 2168, loss: 2.446065\n",
      "step 2169, loss: 2.443092\n",
      "step 2170, loss: 2.440121\n",
      "step 2171, loss: 2.437153\n",
      "step 2172, loss: 2.434188\n",
      "step 2173, loss: 2.431226\n",
      "step 2174, loss: 2.428266\n",
      "step 2175, loss: 2.425309\n",
      "step 2176, loss: 2.422354\n",
      "step 2177, loss: 2.419403\n",
      "step 2178, loss: 2.416454\n",
      "step 2179, loss: 2.413507\n",
      "step 2180, loss: 2.410564\n",
      "step 2181, loss: 2.407623\n",
      "step 2182, loss: 2.404685\n",
      "step 2183, loss: 2.401749\n",
      "step 2184, loss: 2.398816\n",
      "step 2185, loss: 2.395886\n",
      "step 2186, loss: 2.392958\n",
      "step 2187, loss: 2.390033\n",
      "step 2188, loss: 2.387111\n",
      "step 2189, loss: 2.384192\n",
      "step 2190, loss: 2.381275\n",
      "step 2191, loss: 2.378361\n",
      "step 2192, loss: 2.375449\n",
      "step 2193, loss: 2.372540\n",
      "step 2194, loss: 2.369634\n",
      "step 2195, loss: 2.366731\n",
      "step 2196, loss: 2.363830\n",
      "step 2197, loss: 2.360932\n",
      "step 2198, loss: 2.358036\n",
      "step 2199, loss: 2.355143\n",
      "step 2200, loss: 2.352253\n",
      "step 2201, loss: 2.349365\n",
      "step 2202, loss: 2.346480\n",
      "step 2203, loss: 2.343598\n",
      "step 2204, loss: 2.340719\n",
      "step 2205, loss: 2.337842\n",
      "step 2206, loss: 2.334968\n",
      "step 2207, loss: 2.332096\n",
      "step 2208, loss: 2.329227\n",
      "step 2209, loss: 2.326360\n",
      "step 2210, loss: 2.323497\n",
      "step 2211, loss: 2.320636\n",
      "step 2212, loss: 2.317777\n",
      "step 2213, loss: 2.314921\n",
      "step 2214, loss: 2.312068\n",
      "step 2215, loss: 2.309218\n",
      "step 2216, loss: 2.306370\n",
      "step 2217, loss: 2.303525\n",
      "step 2218, loss: 2.300682\n",
      "step 2219, loss: 2.297842\n",
      "step 2220, loss: 2.295005\n",
      "step 2221, loss: 2.292170\n",
      "step 2222, loss: 2.289338\n",
      "step 2223, loss: 2.286509\n",
      "step 2224, loss: 2.283682\n",
      "step 2225, loss: 2.280857\n",
      "step 2226, loss: 2.278036\n",
      "step 2227, loss: 2.275216\n",
      "step 2228, loss: 2.272400\n",
      "step 2229, loss: 2.269587\n",
      "step 2230, loss: 2.266775\n",
      "step 2231, loss: 2.263967\n",
      "step 2232, loss: 2.261161\n",
      "step 2233, loss: 2.258357\n",
      "step 2234, loss: 2.255557\n",
      "step 2235, loss: 2.252758\n",
      "step 2236, loss: 2.249963\n",
      "step 2237, loss: 2.247170\n",
      "step 2238, loss: 2.244380\n",
      "step 2239, loss: 2.241592\n",
      "step 2240, loss: 2.238807\n",
      "step 2241, loss: 2.236025\n",
      "step 2242, loss: 2.233245\n",
      "step 2243, loss: 2.230468\n",
      "step 2244, loss: 2.227693\n",
      "step 2245, loss: 2.224921\n",
      "step 2246, loss: 2.222151\n",
      "step 2247, loss: 2.219384\n",
      "step 2248, loss: 2.216620\n",
      "step 2249, loss: 2.213858\n",
      "step 2250, loss: 2.211099\n",
      "step 2251, loss: 2.208342\n",
      "step 2252, loss: 2.205588\n",
      "step 2253, loss: 2.202837\n",
      "step 2254, loss: 2.200088\n",
      "step 2255, loss: 2.197342\n",
      "step 2256, loss: 2.194598\n",
      "step 2257, loss: 2.191857\n",
      "step 2258, loss: 2.189119\n",
      "step 2259, loss: 2.186383\n",
      "step 2260, loss: 2.183649\n",
      "step 2261, loss: 2.180919\n",
      "step 2262, loss: 2.178190\n",
      "step 2263, loss: 2.175465\n",
      "step 2264, loss: 2.172741\n",
      "step 2265, loss: 2.170021\n",
      "step 2266, loss: 2.167303\n",
      "step 2267, loss: 2.164588\n",
      "step 2268, loss: 2.161875\n",
      "step 2269, loss: 2.159165\n",
      "step 2270, loss: 2.156457\n",
      "step 2271, loss: 2.153752\n",
      "step 2272, loss: 2.151049\n",
      "step 2273, loss: 2.148349\n",
      "step 2274, loss: 2.145652\n",
      "step 2275, loss: 2.142957\n",
      "step 2276, loss: 2.140264\n",
      "step 2277, loss: 2.137574\n",
      "step 2278, loss: 2.134887\n",
      "step 2279, loss: 2.132202\n",
      "step 2280, loss: 2.129520\n",
      "step 2281, loss: 2.126840\n",
      "step 2282, loss: 2.124163\n",
      "step 2283, loss: 2.121488\n",
      "step 2284, loss: 2.118816\n",
      "step 2285, loss: 2.116147\n",
      "step 2286, loss: 2.113480\n",
      "step 2287, loss: 2.110815\n",
      "step 2288, loss: 2.108153\n",
      "step 2289, loss: 2.105494\n",
      "step 2290, loss: 2.102837\n",
      "step 2291, loss: 2.100182\n",
      "step 2292, loss: 2.097530\n",
      "step 2293, loss: 2.094881\n",
      "step 2294, loss: 2.092234\n",
      "step 2295, loss: 2.089590\n",
      "step 2296, loss: 2.086948\n",
      "step 2297, loss: 2.084309\n",
      "step 2298, loss: 2.081672\n",
      "step 2299, loss: 2.079038\n",
      "step 2300, loss: 2.076406\n",
      "step 2301, loss: 2.073778\n",
      "step 2302, loss: 2.071151\n",
      "step 2303, loss: 2.068527\n",
      "step 2304, loss: 2.065905\n",
      "step 2305, loss: 2.063286\n",
      "step 2306, loss: 2.060669\n",
      "step 2307, loss: 2.058055\n",
      "step 2308, loss: 2.055444\n",
      "step 2309, loss: 2.052835\n",
      "step 2310, loss: 2.050228\n",
      "step 2311, loss: 2.047624\n",
      "step 2312, loss: 2.045022\n",
      "step 2313, loss: 2.042423\n",
      "step 2314, loss: 2.039826\n",
      "step 2315, loss: 2.037232\n",
      "step 2316, loss: 2.034641\n",
      "step 2317, loss: 2.032052\n",
      "step 2318, loss: 2.029465\n",
      "step 2319, loss: 2.026881\n",
      "step 2320, loss: 2.024299\n",
      "step 2321, loss: 2.021720\n",
      "step 2322, loss: 2.019144\n",
      "step 2323, loss: 2.016569\n",
      "step 2324, loss: 2.013998\n",
      "step 2325, loss: 2.011428\n",
      "step 2326, loss: 2.008862\n",
      "step 2327, loss: 2.006297\n",
      "step 2328, loss: 2.003735\n",
      "step 2329, loss: 2.001176\n",
      "step 2330, loss: 1.998619\n",
      "step 2331, loss: 1.996065\n",
      "step 2332, loss: 1.993513\n",
      "step 2333, loss: 1.990963\n",
      "step 2334, loss: 1.988416\n",
      "step 2335, loss: 1.985872\n",
      "step 2336, loss: 1.983330\n",
      "step 2337, loss: 1.980790\n",
      "step 2338, loss: 1.978253\n",
      "step 2339, loss: 1.975718\n",
      "step 2340, loss: 1.973186\n",
      "step 2341, loss: 1.970656\n",
      "step 2342, loss: 1.968129\n",
      "step 2343, loss: 1.965604\n",
      "step 2344, loss: 1.963081\n",
      "step 2345, loss: 1.960562\n",
      "step 2346, loss: 1.958044\n",
      "step 2347, loss: 1.955529\n",
      "step 2348, loss: 1.953016\n",
      "step 2349, loss: 1.950506\n",
      "step 2350, loss: 1.947998\n",
      "step 2351, loss: 1.945493\n",
      "step 2352, loss: 1.942990\n",
      "step 2353, loss: 1.940490\n",
      "step 2354, loss: 1.937991\n",
      "step 2355, loss: 1.935496\n",
      "step 2356, loss: 1.933003\n",
      "step 2357, loss: 1.930512\n",
      "step 2358, loss: 1.928024\n",
      "step 2359, loss: 1.925538\n",
      "step 2360, loss: 1.923055\n",
      "step 2361, loss: 1.920574\n",
      "step 2362, loss: 1.918095\n",
      "step 2363, loss: 1.915619\n",
      "step 2364, loss: 1.913145\n",
      "step 2365, loss: 1.910674\n",
      "step 2366, loss: 1.908205\n",
      "step 2367, loss: 1.905739\n",
      "step 2368, loss: 1.903275\n",
      "step 2369, loss: 1.900814\n",
      "step 2370, loss: 1.898355\n",
      "step 2371, loss: 1.895898\n",
      "step 2372, loss: 1.893443\n",
      "step 2373, loss: 1.890992\n",
      "step 2374, loss: 1.888542\n",
      "step 2375, loss: 1.886095\n",
      "step 2376, loss: 1.883650\n",
      "step 2377, loss: 1.881208\n",
      "step 2378, loss: 1.878768\n",
      "step 2379, loss: 1.876330\n",
      "step 2380, loss: 1.873895\n",
      "step 2381, loss: 1.871463\n",
      "step 2382, loss: 1.869032\n",
      "step 2383, loss: 1.866604\n",
      "step 2384, loss: 1.864179\n",
      "step 2385, loss: 1.861756\n",
      "step 2386, loss: 1.859335\n",
      "step 2387, loss: 1.856917\n",
      "step 2388, loss: 1.854501\n",
      "step 2389, loss: 1.852087\n",
      "step 2390, loss: 1.849676\n",
      "step 2391, loss: 1.847267\n",
      "step 2392, loss: 1.844861\n",
      "step 2393, loss: 1.842457\n",
      "step 2394, loss: 1.840055\n",
      "step 2395, loss: 1.837656\n",
      "step 2396, loss: 1.835259\n",
      "step 2397, loss: 1.832865\n",
      "step 2398, loss: 1.830473\n",
      "step 2399, loss: 1.828083\n",
      "step 2400, loss: 1.825695\n",
      "step 2401, loss: 1.823310\n",
      "step 2402, loss: 1.820928\n",
      "step 2403, loss: 1.818547\n",
      "step 2404, loss: 1.816170\n",
      "step 2405, loss: 1.813795\n",
      "step 2406, loss: 1.811421\n",
      "step 2407, loss: 1.809051\n",
      "step 2408, loss: 1.806682\n",
      "step 2409, loss: 1.804316\n",
      "step 2410, loss: 1.801952\n",
      "step 2411, loss: 1.799591\n",
      "step 2412, loss: 1.797232\n",
      "step 2413, loss: 1.794876\n",
      "step 2414, loss: 1.792521\n",
      "step 2415, loss: 1.790169\n",
      "step 2416, loss: 1.787820\n",
      "step 2417, loss: 1.785472\n",
      "step 2418, loss: 1.783128\n",
      "step 2419, loss: 1.780785\n",
      "step 2420, loss: 1.778445\n",
      "step 2421, loss: 1.776107\n",
      "step 2422, loss: 1.773772\n",
      "step 2423, loss: 1.771439\n",
      "step 2424, loss: 1.769108\n",
      "step 2425, loss: 1.766779\n",
      "step 2426, loss: 1.764453\n",
      "step 2427, loss: 1.762129\n",
      "step 2428, loss: 1.759808\n",
      "step 2429, loss: 1.757489\n",
      "step 2430, loss: 1.755172\n",
      "step 2431, loss: 1.752857\n",
      "step 2432, loss: 1.750545\n",
      "step 2433, loss: 1.748235\n",
      "step 2434, loss: 1.745928\n",
      "step 2435, loss: 1.743623\n",
      "step 2436, loss: 1.741320\n",
      "step 2437, loss: 1.739019\n",
      "step 2438, loss: 1.736721\n",
      "step 2439, loss: 1.734425\n",
      "step 2440, loss: 1.732132\n",
      "step 2441, loss: 1.729841\n",
      "step 2442, loss: 1.727552\n",
      "step 2443, loss: 1.725265\n",
      "step 2444, loss: 1.722981\n",
      "step 2445, loss: 1.720699\n",
      "step 2446, loss: 1.718419\n",
      "step 2447, loss: 1.716141\n",
      "step 2448, loss: 1.713866\n",
      "step 2449, loss: 1.711594\n",
      "step 2450, loss: 1.709323\n",
      "step 2451, loss: 1.707055\n",
      "step 2452, loss: 1.704789\n",
      "step 2453, loss: 1.702525\n",
      "step 2454, loss: 1.700264\n",
      "step 2455, loss: 1.698005\n",
      "step 2456, loss: 1.695748\n",
      "step 2457, loss: 1.693494\n",
      "step 2458, loss: 1.691242\n",
      "step 2459, loss: 1.688992\n",
      "step 2460, loss: 1.686745\n",
      "step 2461, loss: 1.684499\n",
      "step 2462, loss: 1.682256\n",
      "step 2463, loss: 1.680016\n",
      "step 2464, loss: 1.677777\n",
      "step 2465, loss: 1.675542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2466, loss: 1.673308\n",
      "step 2467, loss: 1.671076\n",
      "step 2468, loss: 1.668847\n",
      "step 2469, loss: 1.666620\n",
      "step 2470, loss: 1.664396\n",
      "step 2471, loss: 1.662173\n",
      "step 2472, loss: 1.659953\n",
      "step 2473, loss: 1.657735\n",
      "step 2474, loss: 1.655520\n",
      "step 2475, loss: 1.653306\n",
      "step 2476, loss: 1.651095\n",
      "step 2477, loss: 1.648887\n",
      "step 2478, loss: 1.646680\n",
      "step 2479, loss: 1.644476\n",
      "step 2480, loss: 1.642274\n",
      "step 2481, loss: 1.640074\n",
      "step 2482, loss: 1.637876\n",
      "step 2483, loss: 1.635681\n",
      "step 2484, loss: 1.633488\n",
      "step 2485, loss: 1.631298\n",
      "step 2486, loss: 1.629109\n",
      "step 2487, loss: 1.626922\n",
      "step 2488, loss: 1.624739\n",
      "step 2489, loss: 1.622557\n",
      "step 2490, loss: 1.620378\n",
      "step 2491, loss: 1.618200\n",
      "step 2492, loss: 1.616025\n",
      "step 2493, loss: 1.613853\n",
      "step 2494, loss: 1.611682\n",
      "step 2495, loss: 1.609514\n",
      "step 2496, loss: 1.607348\n",
      "step 2497, loss: 1.605184\n",
      "step 2498, loss: 1.603023\n",
      "step 2499, loss: 1.600863\n",
      "step 2500, loss: 1.598706\n",
      "step 2501, loss: 1.596552\n",
      "step 2502, loss: 1.594399\n",
      "step 2503, loss: 1.592249\n",
      "step 2504, loss: 1.590101\n",
      "step 2505, loss: 1.587955\n",
      "step 2506, loss: 1.585811\n",
      "step 2507, loss: 1.583670\n",
      "step 2508, loss: 1.581530\n",
      "step 2509, loss: 1.579394\n",
      "step 2510, loss: 1.577258\n",
      "step 2511, loss: 1.575126\n",
      "step 2512, loss: 1.572996\n",
      "step 2513, loss: 1.570867\n",
      "step 2514, loss: 1.568741\n",
      "step 2515, loss: 1.566618\n",
      "step 2516, loss: 1.564496\n",
      "step 2517, loss: 1.562377\n",
      "step 2518, loss: 1.560260\n",
      "step 2519, loss: 1.558145\n",
      "step 2520, loss: 1.556032\n",
      "step 2521, loss: 1.553922\n",
      "step 2522, loss: 1.551813\n",
      "step 2523, loss: 1.549707\n",
      "step 2524, loss: 1.547604\n",
      "step 2525, loss: 1.545502\n",
      "step 2526, loss: 1.543403\n",
      "step 2527, loss: 1.541305\n",
      "step 2528, loss: 1.539210\n",
      "step 2529, loss: 1.537118\n",
      "step 2530, loss: 1.535027\n",
      "step 2531, loss: 1.532938\n",
      "step 2532, loss: 1.530852\n",
      "step 2533, loss: 1.528768\n",
      "step 2534, loss: 1.526686\n",
      "step 2535, loss: 1.524606\n",
      "step 2536, loss: 1.522529\n",
      "step 2537, loss: 1.520454\n",
      "step 2538, loss: 1.518380\n",
      "step 2539, loss: 1.516309\n",
      "step 2540, loss: 1.514241\n",
      "step 2541, loss: 1.512174\n",
      "step 2542, loss: 1.510109\n",
      "step 2543, loss: 1.508047\n",
      "step 2544, loss: 1.505987\n",
      "step 2545, loss: 1.503929\n",
      "step 2546, loss: 1.501873\n",
      "step 2547, loss: 1.499820\n",
      "step 2548, loss: 1.497768\n",
      "step 2549, loss: 1.495719\n",
      "step 2550, loss: 1.493672\n",
      "step 2551, loss: 1.491627\n",
      "step 2552, loss: 1.489584\n",
      "step 2553, loss: 1.487543\n",
      "step 2554, loss: 1.485505\n",
      "step 2555, loss: 1.483469\n",
      "step 2556, loss: 1.481434\n",
      "step 2557, loss: 1.479402\n",
      "step 2558, loss: 1.477372\n",
      "step 2559, loss: 1.475345\n",
      "step 2560, loss: 1.473319\n",
      "step 2561, loss: 1.471296\n",
      "step 2562, loss: 1.469275\n",
      "step 2563, loss: 1.467256\n",
      "step 2564, loss: 1.465239\n",
      "step 2565, loss: 1.463224\n",
      "step 2566, loss: 1.461211\n",
      "step 2567, loss: 1.459201\n",
      "step 2568, loss: 1.457192\n",
      "step 2569, loss: 1.455186\n",
      "step 2570, loss: 1.453182\n",
      "step 2571, loss: 1.451180\n",
      "step 2572, loss: 1.449180\n",
      "step 2573, loss: 1.447182\n",
      "step 2574, loss: 1.445187\n",
      "step 2575, loss: 1.443193\n",
      "step 2576, loss: 1.441202\n",
      "step 2577, loss: 1.439213\n",
      "step 2578, loss: 1.437225\n",
      "step 2579, loss: 1.435241\n",
      "step 2580, loss: 1.433258\n",
      "step 2581, loss: 1.431277\n",
      "step 2582, loss: 1.429298\n",
      "step 2583, loss: 1.427322\n",
      "step 2584, loss: 1.425348\n",
      "step 2585, loss: 1.423375\n",
      "step 2586, loss: 1.421405\n",
      "step 2587, loss: 1.419437\n",
      "step 2588, loss: 1.417471\n",
      "step 2589, loss: 1.415508\n",
      "step 2590, loss: 1.413546\n",
      "step 2591, loss: 1.411586\n",
      "step 2592, loss: 1.409628\n",
      "step 2593, loss: 1.407673\n",
      "step 2594, loss: 1.405720\n",
      "step 2595, loss: 1.403769\n",
      "step 2596, loss: 1.401820\n",
      "step 2597, loss: 1.399873\n",
      "step 2598, loss: 1.397928\n",
      "step 2599, loss: 1.395985\n",
      "step 2600, loss: 1.394045\n",
      "step 2601, loss: 1.392106\n",
      "step 2602, loss: 1.390169\n",
      "step 2603, loss: 1.388235\n",
      "step 2604, loss: 1.386303\n",
      "step 2605, loss: 1.384373\n",
      "step 2606, loss: 1.382444\n",
      "step 2607, loss: 1.380518\n",
      "step 2608, loss: 1.378594\n",
      "step 2609, loss: 1.376672\n",
      "step 2610, loss: 1.374753\n",
      "step 2611, loss: 1.372835\n",
      "step 2612, loss: 1.370919\n",
      "step 2613, loss: 1.369006\n",
      "step 2614, loss: 1.367094\n",
      "step 2615, loss: 1.365185\n",
      "step 2616, loss: 1.363278\n",
      "step 2617, loss: 1.361372\n",
      "step 2618, loss: 1.359470\n",
      "step 2619, loss: 1.357568\n",
      "step 2620, loss: 1.355669\n",
      "step 2621, loss: 1.353772\n",
      "step 2622, loss: 1.351877\n",
      "step 2623, loss: 1.349985\n",
      "step 2624, loss: 1.348094\n",
      "step 2625, loss: 1.346205\n",
      "step 2626, loss: 1.344319\n",
      "step 2627, loss: 1.342434\n",
      "step 2628, loss: 1.340551\n",
      "step 2629, loss: 1.338671\n",
      "step 2630, loss: 1.336792\n",
      "step 2631, loss: 1.334916\n",
      "step 2632, loss: 1.333042\n",
      "step 2633, loss: 1.331169\n",
      "step 2634, loss: 1.329299\n",
      "step 2635, loss: 1.327431\n",
      "step 2636, loss: 1.325565\n",
      "step 2637, loss: 1.323700\n",
      "step 2638, loss: 1.321839\n",
      "step 2639, loss: 1.319979\n",
      "step 2640, loss: 1.318121\n",
      "step 2641, loss: 1.316265\n",
      "step 2642, loss: 1.314411\n",
      "step 2643, loss: 1.312559\n",
      "step 2644, loss: 1.310709\n",
      "step 2645, loss: 1.308862\n",
      "step 2646, loss: 1.307016\n",
      "step 2647, loss: 1.305172\n",
      "step 2648, loss: 1.303331\n",
      "step 2649, loss: 1.301491\n",
      "step 2650, loss: 1.299654\n",
      "step 2651, loss: 1.297818\n",
      "step 2652, loss: 1.295984\n",
      "step 2653, loss: 1.294153\n",
      "step 2654, loss: 1.292323\n",
      "step 2655, loss: 1.290496\n",
      "step 2656, loss: 1.288670\n",
      "step 2657, loss: 1.286847\n",
      "step 2658, loss: 1.285025\n",
      "step 2659, loss: 1.283206\n",
      "step 2660, loss: 1.281389\n",
      "step 2661, loss: 1.279573\n",
      "step 2662, loss: 1.277760\n",
      "step 2663, loss: 1.275949\n",
      "step 2664, loss: 1.274139\n",
      "step 2665, loss: 1.272332\n",
      "step 2666, loss: 1.270526\n",
      "step 2667, loss: 1.268723\n",
      "step 2668, loss: 1.266922\n",
      "step 2669, loss: 1.265122\n",
      "step 2670, loss: 1.263325\n",
      "step 2671, loss: 1.261530\n",
      "step 2672, loss: 1.259736\n",
      "step 2673, loss: 1.257945\n",
      "step 2674, loss: 1.256156\n",
      "step 2675, loss: 1.254368\n",
      "step 2676, loss: 1.252583\n",
      "step 2677, loss: 1.250800\n",
      "step 2678, loss: 1.249018\n",
      "step 2679, loss: 1.247239\n",
      "step 2680, loss: 1.245461\n",
      "step 2681, loss: 1.243686\n",
      "step 2682, loss: 1.241913\n",
      "step 2683, loss: 1.240141\n",
      "step 2684, loss: 1.238372\n",
      "step 2685, loss: 1.236604\n",
      "step 2686, loss: 1.234839\n",
      "step 2687, loss: 1.233075\n",
      "step 2688, loss: 1.231313\n",
      "step 2689, loss: 1.229554\n",
      "step 2690, loss: 1.227796\n",
      "step 2691, loss: 1.226040\n",
      "step 2692, loss: 1.224287\n",
      "step 2693, loss: 1.222535\n",
      "step 2694, loss: 1.220786\n",
      "step 2695, loss: 1.219038\n",
      "step 2696, loss: 1.217292\n",
      "step 2697, loss: 1.215549\n",
      "step 2698, loss: 1.213807\n",
      "step 2699, loss: 1.212067\n",
      "step 2700, loss: 1.210329\n",
      "step 2701, loss: 1.208593\n",
      "step 2702, loss: 1.206859\n",
      "step 2703, loss: 1.205126\n",
      "step 2704, loss: 1.203396\n",
      "step 2705, loss: 1.201668\n",
      "step 2706, loss: 1.199942\n",
      "step 2707, loss: 1.198218\n",
      "step 2708, loss: 1.196495\n",
      "step 2709, loss: 1.194775\n",
      "step 2710, loss: 1.193056\n",
      "step 2711, loss: 1.191340\n",
      "step 2712, loss: 1.189625\n",
      "step 2713, loss: 1.187913\n",
      "step 2714, loss: 1.186202\n",
      "step 2715, loss: 1.184494\n",
      "step 2716, loss: 1.182787\n",
      "step 2717, loss: 1.181082\n",
      "step 2718, loss: 1.179379\n",
      "step 2719, loss: 1.177678\n",
      "step 2720, loss: 1.175979\n",
      "step 2721, loss: 1.174282\n",
      "step 2722, loss: 1.172587\n",
      "step 2723, loss: 1.170893\n",
      "step 2724, loss: 1.169202\n",
      "step 2725, loss: 1.167513\n",
      "step 2726, loss: 1.165825\n",
      "step 2727, loss: 1.164140\n",
      "step 2728, loss: 1.162456\n",
      "step 2729, loss: 1.160774\n",
      "step 2730, loss: 1.159095\n",
      "step 2731, loss: 1.157417\n",
      "step 2732, loss: 1.155741\n",
      "step 2733, loss: 1.154067\n",
      "step 2734, loss: 1.152394\n",
      "step 2735, loss: 1.150724\n",
      "step 2736, loss: 1.149056\n",
      "step 2737, loss: 1.147390\n",
      "step 2738, loss: 1.145725\n",
      "step 2739, loss: 1.144063\n",
      "step 2740, loss: 1.142402\n",
      "step 2741, loss: 1.140743\n",
      "step 2742, loss: 1.139086\n",
      "step 2743, loss: 1.137431\n",
      "step 2744, loss: 1.135779\n",
      "step 2745, loss: 1.134127\n",
      "step 2746, loss: 1.132478\n",
      "step 2747, loss: 1.130831\n",
      "step 2748, loss: 1.129185\n",
      "step 2749, loss: 1.127542\n",
      "step 2750, loss: 1.125901\n",
      "step 2751, loss: 1.124261\n",
      "step 2752, loss: 1.122623\n",
      "step 2753, loss: 1.120987\n",
      "step 2754, loss: 1.119353\n",
      "step 2755, loss: 1.117720\n",
      "step 2756, loss: 1.116090\n",
      "step 2757, loss: 1.114462\n",
      "step 2758, loss: 1.112835\n",
      "step 2759, loss: 1.111211\n",
      "step 2760, loss: 1.109588\n",
      "step 2761, loss: 1.107967\n",
      "step 2762, loss: 1.106348\n",
      "step 2763, loss: 1.104731\n",
      "step 2764, loss: 1.103115\n",
      "step 2765, loss: 1.101502\n",
      "step 2766, loss: 1.099890\n",
      "step 2767, loss: 1.098281\n",
      "step 2768, loss: 1.096673\n",
      "step 2769, loss: 1.095067\n",
      "step 2770, loss: 1.093463\n",
      "step 2771, loss: 1.091861\n",
      "step 2772, loss: 1.090261\n",
      "step 2773, loss: 1.088662\n",
      "step 2774, loss: 1.087065\n",
      "step 2775, loss: 1.085471\n",
      "step 2776, loss: 1.083878\n",
      "step 2777, loss: 1.082286\n",
      "step 2778, loss: 1.080697\n",
      "step 2779, loss: 1.079110\n",
      "step 2780, loss: 1.077524\n",
      "step 2781, loss: 1.075941\n",
      "step 2782, loss: 1.074359\n",
      "step 2783, loss: 1.072779\n",
      "step 2784, loss: 1.071201\n",
      "step 2785, loss: 1.069625\n",
      "step 2786, loss: 1.068050\n",
      "step 2787, loss: 1.066477\n",
      "step 2788, loss: 1.064907\n",
      "step 2789, loss: 1.063338\n",
      "step 2790, loss: 1.061771\n",
      "step 2791, loss: 1.060206\n",
      "step 2792, loss: 1.058642\n",
      "step 2793, loss: 1.057081\n",
      "step 2794, loss: 1.055521\n",
      "step 2795, loss: 1.053963\n",
      "step 2796, loss: 1.052407\n",
      "step 2797, loss: 1.050853\n",
      "step 2798, loss: 1.049301\n",
      "step 2799, loss: 1.047750\n",
      "step 2800, loss: 1.046201\n",
      "step 2801, loss: 1.044654\n",
      "step 2802, loss: 1.043110\n",
      "step 2803, loss: 1.041566\n",
      "step 2804, loss: 1.040025\n",
      "step 2805, loss: 1.038485\n",
      "step 2806, loss: 1.036947\n",
      "step 2807, loss: 1.035411\n",
      "step 2808, loss: 1.033877\n",
      "step 2809, loss: 1.032345\n",
      "step 2810, loss: 1.030814\n",
      "step 2811, loss: 1.029286\n",
      "step 2812, loss: 1.027759\n",
      "step 2813, loss: 1.026234\n",
      "step 2814, loss: 1.024710\n",
      "step 2815, loss: 1.023189\n",
      "step 2816, loss: 1.021669\n",
      "step 2817, loss: 1.020151\n",
      "step 2818, loss: 1.018635\n",
      "step 2819, loss: 1.017121\n",
      "step 2820, loss: 1.015608\n",
      "step 2821, loss: 1.014098\n",
      "step 2822, loss: 1.012589\n",
      "step 2823, loss: 1.011082\n",
      "step 2824, loss: 1.009576\n",
      "step 2825, loss: 1.008073\n",
      "step 2826, loss: 1.006571\n",
      "step 2827, loss: 1.005071\n",
      "step 2828, loss: 1.003573\n",
      "step 2829, loss: 1.002077\n",
      "step 2830, loss: 1.000582\n",
      "step 2831, loss: 0.999090\n",
      "step 2832, loss: 0.997599\n",
      "step 2833, loss: 0.996109\n",
      "step 2834, loss: 0.994622\n",
      "step 2835, loss: 0.993137\n",
      "step 2836, loss: 0.991653\n",
      "step 2837, loss: 0.990170\n",
      "step 2838, loss: 0.988690\n",
      "step 2839, loss: 0.987212\n",
      "step 2840, loss: 0.985735\n",
      "step 2841, loss: 0.984260\n",
      "step 2842, loss: 0.982787\n",
      "step 2843, loss: 0.981315\n",
      "step 2844, loss: 0.979846\n",
      "step 2845, loss: 0.978378\n",
      "step 2846, loss: 0.976911\n",
      "step 2847, loss: 0.975447\n",
      "step 2848, loss: 0.973985\n",
      "step 2849, loss: 0.972524\n",
      "step 2850, loss: 0.971065\n",
      "step 2851, loss: 0.969607\n",
      "step 2852, loss: 0.968152\n",
      "step 2853, loss: 0.966698\n",
      "step 2854, loss: 0.965246\n",
      "step 2855, loss: 0.963795\n",
      "step 2856, loss: 0.962347\n",
      "step 2857, loss: 0.960900\n",
      "step 2858, loss: 0.959455\n",
      "step 2859, loss: 0.958012\n",
      "step 2860, loss: 0.956570\n",
      "step 2861, loss: 0.955130\n",
      "step 2862, loss: 0.953692\n",
      "step 2863, loss: 0.952256\n",
      "step 2864, loss: 0.950821\n",
      "step 2865, loss: 0.949389\n",
      "step 2866, loss: 0.947957\n",
      "step 2867, loss: 0.946528\n",
      "step 2868, loss: 0.945100\n",
      "step 2869, loss: 0.943674\n",
      "step 2870, loss: 0.942250\n",
      "step 2871, loss: 0.940828\n",
      "step 2872, loss: 0.939407\n",
      "step 2873, loss: 0.937988\n",
      "step 2874, loss: 0.936571\n",
      "step 2875, loss: 0.935155\n",
      "step 2876, loss: 0.933742\n",
      "step 2877, loss: 0.932330\n",
      "step 2878, loss: 0.930919\n",
      "step 2879, loss: 0.929510\n",
      "step 2880, loss: 0.928104\n",
      "step 2881, loss: 0.926698\n",
      "step 2882, loss: 0.925295\n",
      "step 2883, loss: 0.923893\n",
      "step 2884, loss: 0.922493\n",
      "step 2885, loss: 0.921095\n",
      "step 2886, loss: 0.919698\n",
      "step 2887, loss: 0.918303\n",
      "step 2888, loss: 0.916910\n",
      "step 2889, loss: 0.915518\n",
      "step 2890, loss: 0.914128\n",
      "step 2891, loss: 0.912741\n",
      "step 2892, loss: 0.911354\n",
      "step 2893, loss: 0.909969\n",
      "step 2894, loss: 0.908586\n",
      "step 2895, loss: 0.907205\n",
      "step 2896, loss: 0.905826\n",
      "step 2897, loss: 0.904448\n",
      "step 2898, loss: 0.903071\n",
      "step 2899, loss: 0.901697\n",
      "step 2900, loss: 0.900324\n",
      "step 2901, loss: 0.898953\n",
      "step 2902, loss: 0.897584\n",
      "step 2903, loss: 0.896216\n",
      "step 2904, loss: 0.894850\n",
      "step 2905, loss: 0.893486\n",
      "step 2906, loss: 0.892123\n",
      "step 2907, loss: 0.890762\n",
      "step 2908, loss: 0.889403\n",
      "step 2909, loss: 0.888045\n",
      "step 2910, loss: 0.886689\n",
      "step 2911, loss: 0.885335\n",
      "step 2912, loss: 0.883983\n",
      "step 2913, loss: 0.882632\n",
      "step 2914, loss: 0.881283\n",
      "step 2915, loss: 0.879935\n",
      "step 2916, loss: 0.878589\n",
      "step 2917, loss: 0.877245\n",
      "step 2918, loss: 0.875902\n",
      "step 2919, loss: 0.874562\n",
      "step 2920, loss: 0.873222\n",
      "step 2921, loss: 0.871885\n",
      "step 2922, loss: 0.870549\n",
      "step 2923, loss: 0.869215\n",
      "step 2924, loss: 0.867882\n",
      "step 2925, loss: 0.866551\n",
      "step 2926, loss: 0.865222\n",
      "step 2927, loss: 0.863895\n",
      "step 2928, loss: 0.862569\n",
      "step 2929, loss: 0.861245\n",
      "step 2930, loss: 0.859922\n",
      "step 2931, loss: 0.858601\n",
      "step 2932, loss: 0.857282\n",
      "step 2933, loss: 0.855964\n",
      "step 2934, loss: 0.854648\n",
      "step 2935, loss: 0.853334\n",
      "step 2936, loss: 0.852021\n",
      "step 2937, loss: 0.850710\n",
      "step 2938, loss: 0.849401\n",
      "step 2939, loss: 0.848093\n",
      "step 2940, loss: 0.846787\n",
      "step 2941, loss: 0.845483\n",
      "step 2942, loss: 0.844180\n",
      "step 2943, loss: 0.842879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2944, loss: 0.841579\n",
      "step 2945, loss: 0.840281\n",
      "step 2946, loss: 0.838985\n",
      "step 2947, loss: 0.837690\n",
      "step 2948, loss: 0.836397\n",
      "step 2949, loss: 0.835106\n",
      "step 2950, loss: 0.833816\n",
      "step 2951, loss: 0.832528\n",
      "step 2952, loss: 0.831242\n",
      "step 2953, loss: 0.829957\n",
      "step 2954, loss: 0.828673\n",
      "step 2955, loss: 0.827392\n",
      "step 2956, loss: 0.826112\n",
      "step 2957, loss: 0.824833\n",
      "step 2958, loss: 0.823557\n",
      "step 2959, loss: 0.822281\n",
      "step 2960, loss: 0.821008\n",
      "step 2961, loss: 0.819736\n",
      "step 2962, loss: 0.818466\n",
      "step 2963, loss: 0.817197\n",
      "step 2964, loss: 0.815930\n",
      "step 2965, loss: 0.814664\n",
      "step 2966, loss: 0.813401\n",
      "step 2967, loss: 0.812138\n",
      "step 2968, loss: 0.810878\n",
      "step 2969, loss: 0.809619\n",
      "step 2970, loss: 0.808361\n",
      "step 2971, loss: 0.807105\n",
      "step 2972, loss: 0.805851\n",
      "step 2973, loss: 0.804598\n",
      "step 2974, loss: 0.803347\n",
      "step 2975, loss: 0.802098\n",
      "step 2976, loss: 0.800850\n",
      "step 2977, loss: 0.799604\n",
      "step 2978, loss: 0.798359\n",
      "step 2979, loss: 0.797116\n",
      "step 2980, loss: 0.795875\n",
      "step 2981, loss: 0.794635\n",
      "step 2982, loss: 0.793397\n",
      "step 2983, loss: 0.792160\n",
      "step 2984, loss: 0.790925\n",
      "step 2985, loss: 0.789691\n",
      "step 2986, loss: 0.788459\n",
      "step 2987, loss: 0.787229\n",
      "step 2988, loss: 0.786000\n",
      "step 2989, loss: 0.784773\n",
      "step 2990, loss: 0.783547\n",
      "step 2991, loss: 0.782323\n",
      "step 2992, loss: 0.781101\n",
      "step 2993, loss: 0.779880\n",
      "step 2994, loss: 0.778660\n",
      "step 2995, loss: 0.777442\n",
      "step 2996, loss: 0.776226\n",
      "step 2997, loss: 0.775012\n",
      "step 2998, loss: 0.773799\n",
      "step 2999, loss: 0.772587\n",
      "step 3000, loss: 0.771377\n",
      "step 3001, loss: 0.770169\n",
      "step 3002, loss: 0.768962\n",
      "step 3003, loss: 0.767756\n",
      "step 3004, loss: 0.766553\n",
      "step 3005, loss: 0.765351\n",
      "step 3006, loss: 0.764150\n",
      "step 3007, loss: 0.762951\n",
      "step 3008, loss: 0.761753\n",
      "step 3009, loss: 0.760557\n",
      "step 3010, loss: 0.759363\n",
      "step 3011, loss: 0.758170\n",
      "step 3012, loss: 0.756979\n",
      "step 3013, loss: 0.755789\n",
      "step 3014, loss: 0.754601\n",
      "step 3015, loss: 0.753414\n",
      "step 3016, loss: 0.752229\n",
      "step 3017, loss: 0.751046\n",
      "step 3018, loss: 0.749864\n",
      "step 3019, loss: 0.748683\n",
      "step 3020, loss: 0.747504\n",
      "step 3021, loss: 0.746327\n",
      "step 3022, loss: 0.745151\n",
      "step 3023, loss: 0.743977\n",
      "step 3024, loss: 0.742804\n",
      "step 3025, loss: 0.741633\n",
      "step 3026, loss: 0.740463\n",
      "step 3027, loss: 0.739295\n",
      "step 3028, loss: 0.738128\n",
      "step 3029, loss: 0.736963\n",
      "step 3030, loss: 0.735799\n",
      "step 3031, loss: 0.734637\n",
      "step 3032, loss: 0.733476\n",
      "step 3033, loss: 0.732317\n",
      "step 3034, loss: 0.731160\n",
      "step 3035, loss: 0.730004\n",
      "step 3036, loss: 0.728849\n",
      "step 3037, loss: 0.727696\n",
      "step 3038, loss: 0.726545\n",
      "step 3039, loss: 0.725395\n",
      "step 3040, loss: 0.724247\n",
      "step 3041, loss: 0.723100\n",
      "step 3042, loss: 0.721954\n",
      "step 3043, loss: 0.720810\n",
      "step 3044, loss: 0.719668\n",
      "step 3045, loss: 0.718527\n",
      "step 3046, loss: 0.717388\n",
      "step 3047, loss: 0.716250\n",
      "step 3048, loss: 0.715113\n",
      "step 3049, loss: 0.713979\n",
      "step 3050, loss: 0.712845\n",
      "step 3051, loss: 0.711713\n",
      "step 3052, loss: 0.710583\n",
      "step 3053, loss: 0.709454\n",
      "step 3054, loss: 0.708327\n",
      "step 3055, loss: 0.707201\n",
      "step 3056, loss: 0.706077\n",
      "step 3057, loss: 0.704954\n",
      "step 3058, loss: 0.703832\n",
      "step 3059, loss: 0.702712\n",
      "step 3060, loss: 0.701594\n",
      "step 3061, loss: 0.700477\n",
      "step 3062, loss: 0.699362\n",
      "step 3063, loss: 0.698248\n",
      "step 3064, loss: 0.697135\n",
      "step 3065, loss: 0.696024\n",
      "step 3066, loss: 0.694915\n",
      "step 3067, loss: 0.693806\n",
      "step 3068, loss: 0.692700\n",
      "step 3069, loss: 0.691595\n",
      "step 3070, loss: 0.690491\n",
      "step 3071, loss: 0.689389\n",
      "step 3072, loss: 0.688288\n",
      "step 3073, loss: 0.687189\n",
      "step 3074, loss: 0.686091\n",
      "step 3075, loss: 0.684995\n",
      "step 3076, loss: 0.683900\n",
      "step 3077, loss: 0.682807\n",
      "step 3078, loss: 0.681715\n",
      "step 3079, loss: 0.680625\n",
      "step 3080, loss: 0.679536\n",
      "step 3081, loss: 0.678449\n",
      "step 3082, loss: 0.677363\n",
      "step 3083, loss: 0.676278\n",
      "step 3084, loss: 0.675195\n",
      "step 3085, loss: 0.674113\n",
      "step 3086, loss: 0.673033\n",
      "step 3087, loss: 0.671955\n",
      "step 3088, loss: 0.670877\n",
      "step 3089, loss: 0.669801\n",
      "step 3090, loss: 0.668727\n",
      "step 3091, loss: 0.667654\n",
      "step 3092, loss: 0.666583\n",
      "step 3093, loss: 0.665513\n",
      "step 3094, loss: 0.664444\n",
      "step 3095, loss: 0.663377\n",
      "step 3096, loss: 0.662311\n",
      "step 3097, loss: 0.661247\n",
      "step 3098, loss: 0.660184\n",
      "step 3099, loss: 0.659123\n",
      "step 3100, loss: 0.658063\n",
      "step 3101, loss: 0.657004\n",
      "step 3102, loss: 0.655947\n",
      "step 3103, loss: 0.654891\n",
      "step 3104, loss: 0.653837\n",
      "step 3105, loss: 0.652784\n",
      "step 3106, loss: 0.651733\n",
      "step 3107, loss: 0.650683\n",
      "step 3108, loss: 0.649635\n",
      "step 3109, loss: 0.648587\n",
      "step 3110, loss: 0.647542\n",
      "step 3111, loss: 0.646497\n",
      "step 3112, loss: 0.645455\n",
      "step 3113, loss: 0.644413\n",
      "step 3114, loss: 0.643373\n",
      "step 3115, loss: 0.642335\n",
      "step 3116, loss: 0.641298\n",
      "step 3117, loss: 0.640262\n",
      "step 3118, loss: 0.639228\n",
      "step 3119, loss: 0.638195\n",
      "step 3120, loss: 0.637163\n",
      "step 3121, loss: 0.636133\n",
      "step 3122, loss: 0.635105\n",
      "step 3123, loss: 0.634078\n",
      "step 3124, loss: 0.633052\n",
      "step 3125, loss: 0.632027\n",
      "step 3126, loss: 0.631004\n",
      "step 3127, loss: 0.629983\n",
      "step 3128, loss: 0.628963\n",
      "step 3129, loss: 0.627944\n",
      "step 3130, loss: 0.626926\n",
      "step 3131, loss: 0.625910\n",
      "step 3132, loss: 0.624896\n",
      "step 3133, loss: 0.623883\n",
      "step 3134, loss: 0.622871\n",
      "step 3135, loss: 0.621860\n",
      "step 3136, loss: 0.620851\n",
      "step 3137, loss: 0.619844\n",
      "step 3138, loss: 0.618837\n",
      "step 3139, loss: 0.617832\n",
      "step 3140, loss: 0.616829\n",
      "step 3141, loss: 0.615827\n",
      "step 3142, loss: 0.614826\n",
      "step 3143, loss: 0.613827\n",
      "step 3144, loss: 0.612829\n",
      "step 3145, loss: 0.611832\n",
      "step 3146, loss: 0.610837\n",
      "step 3147, loss: 0.609843\n",
      "step 3148, loss: 0.608851\n",
      "step 3149, loss: 0.607860\n",
      "step 3150, loss: 0.606870\n",
      "step 3151, loss: 0.605882\n",
      "step 3152, loss: 0.604895\n",
      "step 3153, loss: 0.603909\n",
      "step 3154, loss: 0.602925\n",
      "step 3155, loss: 0.601942\n",
      "step 3156, loss: 0.600961\n",
      "step 3157, loss: 0.599981\n",
      "step 3158, loss: 0.599002\n",
      "step 3159, loss: 0.598025\n",
      "step 3160, loss: 0.597049\n",
      "step 3161, loss: 0.596074\n",
      "step 3162, loss: 0.595101\n",
      "step 3163, loss: 0.594129\n",
      "step 3164, loss: 0.593158\n",
      "step 3165, loss: 0.592189\n",
      "step 3166, loss: 0.591221\n",
      "step 3167, loss: 0.590255\n",
      "step 3168, loss: 0.589290\n",
      "step 3169, loss: 0.588326\n",
      "step 3170, loss: 0.587364\n",
      "step 3171, loss: 0.586402\n",
      "step 3172, loss: 0.585442\n",
      "step 3173, loss: 0.584484\n",
      "step 3174, loss: 0.583527\n",
      "step 3175, loss: 0.582571\n",
      "step 3176, loss: 0.581617\n",
      "step 3177, loss: 0.580664\n",
      "step 3178, loss: 0.579712\n",
      "step 3179, loss: 0.578762\n",
      "step 3180, loss: 0.577813\n",
      "step 3181, loss: 0.576865\n",
      "step 3182, loss: 0.575919\n",
      "step 3183, loss: 0.574974\n",
      "step 3184, loss: 0.574030\n",
      "step 3185, loss: 0.573088\n",
      "step 3186, loss: 0.572147\n",
      "step 3187, loss: 0.571207\n",
      "step 3188, loss: 0.570269\n",
      "step 3189, loss: 0.569332\n",
      "step 3190, loss: 0.568396\n",
      "step 3191, loss: 0.567462\n",
      "step 3192, loss: 0.566529\n",
      "step 3193, loss: 0.565597\n",
      "step 3194, loss: 0.564666\n",
      "step 3195, loss: 0.563737\n",
      "step 3196, loss: 0.562810\n",
      "step 3197, loss: 0.561883\n",
      "step 3198, loss: 0.560958\n",
      "step 3199, loss: 0.560034\n",
      "step 3200, loss: 0.559111\n",
      "step 3201, loss: 0.558190\n",
      "step 3202, loss: 0.557271\n",
      "step 3203, loss: 0.556352\n",
      "step 3204, loss: 0.555435\n",
      "step 3205, loss: 0.554519\n",
      "step 3206, loss: 0.553604\n",
      "step 3207, loss: 0.552691\n",
      "step 3208, loss: 0.551778\n",
      "step 3209, loss: 0.550868\n",
      "step 3210, loss: 0.549958\n",
      "step 3211, loss: 0.549050\n",
      "step 3212, loss: 0.548143\n",
      "step 3213, loss: 0.547238\n",
      "step 3214, loss: 0.546333\n",
      "step 3215, loss: 0.545430\n",
      "step 3216, loss: 0.544528\n",
      "step 3217, loss: 0.543628\n",
      "step 3218, loss: 0.542729\n",
      "step 3219, loss: 0.541831\n",
      "step 3220, loss: 0.540934\n",
      "step 3221, loss: 0.540039\n",
      "step 3222, loss: 0.539145\n",
      "step 3223, loss: 0.538253\n",
      "step 3224, loss: 0.537361\n",
      "step 3225, loss: 0.536471\n",
      "step 3226, loss: 0.535582\n",
      "step 3227, loss: 0.534695\n",
      "step 3228, loss: 0.533809\n",
      "step 3229, loss: 0.532924\n",
      "step 3230, loss: 0.532040\n",
      "step 3231, loss: 0.531158\n",
      "step 3232, loss: 0.530277\n",
      "step 3233, loss: 0.529397\n",
      "step 3234, loss: 0.528518\n",
      "step 3235, loss: 0.527641\n",
      "step 3236, loss: 0.526765\n",
      "step 3237, loss: 0.525891\n",
      "step 3238, loss: 0.525017\n",
      "step 3239, loss: 0.524145\n",
      "step 3240, loss: 0.523274\n",
      "step 3241, loss: 0.522404\n",
      "step 3242, loss: 0.521535\n",
      "step 3243, loss: 0.520668\n",
      "step 3244, loss: 0.519802\n",
      "step 3245, loss: 0.518937\n",
      "step 3246, loss: 0.518074\n",
      "step 3247, loss: 0.517211\n",
      "step 3248, loss: 0.516350\n",
      "step 3249, loss: 0.515491\n",
      "step 3250, loss: 0.514632\n",
      "step 3251, loss: 0.513775\n",
      "step 3252, loss: 0.512919\n",
      "step 3253, loss: 0.512064\n",
      "step 3254, loss: 0.511211\n",
      "step 3255, loss: 0.510359\n",
      "step 3256, loss: 0.509508\n",
      "step 3257, loss: 0.508658\n",
      "step 3258, loss: 0.507810\n",
      "step 3259, loss: 0.506963\n",
      "step 3260, loss: 0.506117\n",
      "step 3261, loss: 0.505272\n",
      "step 3262, loss: 0.504429\n",
      "step 3263, loss: 0.503586\n",
      "step 3264, loss: 0.502745\n",
      "step 3265, loss: 0.501906\n",
      "step 3266, loss: 0.501067\n",
      "step 3267, loss: 0.500230\n",
      "step 3268, loss: 0.499394\n",
      "step 3269, loss: 0.498559\n",
      "step 3270, loss: 0.497726\n",
      "step 3271, loss: 0.496893\n",
      "step 3272, loss: 0.496062\n",
      "step 3273, loss: 0.495232\n",
      "step 3274, loss: 0.494403\n",
      "step 3275, loss: 0.493576\n",
      "step 3276, loss: 0.492749\n",
      "step 3277, loss: 0.491924\n",
      "step 3278, loss: 0.491100\n",
      "step 3279, loss: 0.490277\n",
      "step 3280, loss: 0.489456\n",
      "step 3281, loss: 0.488636\n",
      "step 3282, loss: 0.487817\n",
      "step 3283, loss: 0.486999\n",
      "step 3284, loss: 0.486182\n",
      "step 3285, loss: 0.485367\n",
      "step 3286, loss: 0.484553\n",
      "step 3287, loss: 0.483740\n",
      "step 3288, loss: 0.482928\n",
      "step 3289, loss: 0.482118\n",
      "step 3290, loss: 0.481308\n",
      "step 3291, loss: 0.480500\n",
      "step 3292, loss: 0.479693\n",
      "step 3293, loss: 0.478888\n",
      "step 3294, loss: 0.478083\n",
      "step 3295, loss: 0.477280\n",
      "step 3296, loss: 0.476478\n",
      "step 3297, loss: 0.475677\n",
      "step 3298, loss: 0.474877\n",
      "step 3299, loss: 0.474079\n",
      "step 3300, loss: 0.473282\n",
      "step 3301, loss: 0.472485\n",
      "step 3302, loss: 0.471691\n",
      "step 3303, loss: 0.470897\n",
      "step 3304, loss: 0.470105\n",
      "step 3305, loss: 0.469313\n",
      "step 3306, loss: 0.468523\n",
      "step 3307, loss: 0.467734\n",
      "step 3308, loss: 0.466946\n",
      "step 3309, loss: 0.466160\n",
      "step 3310, loss: 0.465374\n",
      "step 3311, loss: 0.464590\n",
      "step 3312, loss: 0.463807\n",
      "step 3313, loss: 0.463024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3314, loss: 0.462244\n",
      "step 3315, loss: 0.461464\n",
      "step 3316, loss: 0.460685\n",
      "step 3317, loss: 0.459908\n",
      "step 3318, loss: 0.459132\n",
      "step 3319, loss: 0.458357\n",
      "step 3320, loss: 0.457583\n",
      "step 3321, loss: 0.456810\n",
      "step 3322, loss: 0.456039\n",
      "step 3323, loss: 0.455269\n",
      "step 3324, loss: 0.454499\n",
      "step 3325, loss: 0.453731\n",
      "step 3326, loss: 0.452965\n",
      "step 3327, loss: 0.452199\n",
      "step 3328, loss: 0.451434\n",
      "step 3329, loss: 0.450671\n",
      "step 3330, loss: 0.449909\n",
      "step 3331, loss: 0.449148\n",
      "step 3332, loss: 0.448388\n",
      "step 3333, loss: 0.447630\n",
      "step 3334, loss: 0.446872\n",
      "step 3335, loss: 0.446116\n",
      "step 3336, loss: 0.445360\n",
      "step 3337, loss: 0.444606\n",
      "step 3338, loss: 0.443853\n",
      "step 3339, loss: 0.443102\n",
      "step 3340, loss: 0.442351\n",
      "step 3341, loss: 0.441602\n",
      "step 3342, loss: 0.440853\n",
      "step 3343, loss: 0.440106\n",
      "step 3344, loss: 0.439360\n",
      "step 3345, loss: 0.438615\n",
      "step 3346, loss: 0.437872\n",
      "step 3347, loss: 0.437129\n",
      "step 3348, loss: 0.436388\n",
      "step 3349, loss: 0.435648\n",
      "step 3350, loss: 0.434908\n",
      "step 3351, loss: 0.434170\n",
      "step 3352, loss: 0.433433\n",
      "step 3353, loss: 0.432698\n",
      "step 3354, loss: 0.431963\n",
      "step 3355, loss: 0.431230\n",
      "step 3356, loss: 0.430497\n",
      "step 3357, loss: 0.429766\n",
      "step 3358, loss: 0.429036\n",
      "step 3359, loss: 0.428307\n",
      "step 3360, loss: 0.427579\n",
      "step 3361, loss: 0.426852\n",
      "step 3362, loss: 0.426126\n",
      "step 3363, loss: 0.425402\n",
      "step 3364, loss: 0.424678\n",
      "step 3365, loss: 0.423956\n",
      "step 3366, loss: 0.423234\n",
      "step 3367, loss: 0.422514\n",
      "step 3368, loss: 0.421795\n",
      "step 3369, loss: 0.421077\n",
      "step 3370, loss: 0.420360\n",
      "step 3371, loss: 0.419644\n",
      "step 3372, loss: 0.418929\n",
      "step 3373, loss: 0.418216\n",
      "step 3374, loss: 0.417503\n",
      "step 3375, loss: 0.416792\n",
      "step 3376, loss: 0.416082\n",
      "step 3377, loss: 0.415372\n",
      "step 3378, loss: 0.414664\n",
      "step 3379, loss: 0.413957\n",
      "step 3380, loss: 0.413252\n",
      "step 3381, loss: 0.412547\n",
      "step 3382, loss: 0.411843\n",
      "step 3383, loss: 0.411141\n",
      "step 3384, loss: 0.410439\n",
      "step 3385, loss: 0.409739\n",
      "step 3386, loss: 0.409040\n",
      "step 3387, loss: 0.408342\n",
      "step 3388, loss: 0.407644\n",
      "step 3389, loss: 0.406948\n",
      "step 3390, loss: 0.406253\n",
      "step 3391, loss: 0.405560\n",
      "step 3392, loss: 0.404867\n",
      "step 3393, loss: 0.404175\n",
      "step 3394, loss: 0.403485\n",
      "step 3395, loss: 0.402795\n",
      "step 3396, loss: 0.402107\n",
      "step 3397, loss: 0.401420\n",
      "step 3398, loss: 0.400733\n",
      "step 3399, loss: 0.400048\n",
      "step 3400, loss: 0.399364\n",
      "step 3401, loss: 0.398681\n",
      "step 3402, loss: 0.397999\n",
      "step 3403, loss: 0.397318\n",
      "step 3404, loss: 0.396639\n",
      "step 3405, loss: 0.395960\n",
      "step 3406, loss: 0.395282\n",
      "step 3407, loss: 0.394606\n",
      "step 3408, loss: 0.393930\n",
      "step 3409, loss: 0.393256\n",
      "step 3410, loss: 0.392583\n",
      "step 3411, loss: 0.391910\n",
      "step 3412, loss: 0.391239\n",
      "step 3413, loss: 0.390569\n",
      "step 3414, loss: 0.389900\n",
      "step 3415, loss: 0.389232\n",
      "step 3416, loss: 0.388565\n",
      "step 3417, loss: 0.387899\n",
      "step 3418, loss: 0.387234\n",
      "step 3419, loss: 0.386570\n",
      "step 3420, loss: 0.385908\n",
      "step 3421, loss: 0.385246\n",
      "step 3422, loss: 0.384585\n",
      "step 3423, loss: 0.383926\n",
      "step 3424, loss: 0.383267\n",
      "step 3425, loss: 0.382610\n",
      "step 3426, loss: 0.381953\n",
      "step 3427, loss: 0.381298\n",
      "step 3428, loss: 0.380643\n",
      "step 3429, loss: 0.379990\n",
      "step 3430, loss: 0.379338\n",
      "step 3431, loss: 0.378687\n",
      "step 3432, loss: 0.378037\n",
      "step 3433, loss: 0.377387\n",
      "step 3434, loss: 0.376739\n",
      "step 3435, loss: 0.376092\n",
      "step 3436, loss: 0.375446\n",
      "step 3437, loss: 0.374801\n",
      "step 3438, loss: 0.374157\n",
      "step 3439, loss: 0.373514\n",
      "step 3440, loss: 0.372872\n",
      "step 3441, loss: 0.372231\n",
      "step 3442, loss: 0.371592\n",
      "step 3443, loss: 0.370953\n",
      "step 3444, loss: 0.370315\n",
      "step 3445, loss: 0.369678\n",
      "step 3446, loss: 0.369042\n",
      "step 3447, loss: 0.368408\n",
      "step 3448, loss: 0.367774\n",
      "step 3449, loss: 0.367141\n",
      "step 3450, loss: 0.366510\n",
      "step 3451, loss: 0.365879\n",
      "step 3452, loss: 0.365249\n",
      "step 3453, loss: 0.364621\n",
      "step 3454, loss: 0.363993\n",
      "step 3455, loss: 0.363367\n",
      "step 3456, loss: 0.362741\n",
      "step 3457, loss: 0.362116\n",
      "step 3458, loss: 0.361493\n",
      "step 3459, loss: 0.360870\n",
      "step 3460, loss: 0.360248\n",
      "step 3461, loss: 0.359627\n",
      "step 3462, loss: 0.359008\n",
      "step 3463, loss: 0.358389\n",
      "step 3464, loss: 0.357771\n",
      "step 3465, loss: 0.357154\n",
      "step 3466, loss: 0.356539\n",
      "step 3467, loss: 0.355924\n",
      "step 3468, loss: 0.355310\n",
      "step 3469, loss: 0.354697\n",
      "step 3470, loss: 0.354086\n",
      "step 3471, loss: 0.353475\n",
      "step 3472, loss: 0.352865\n",
      "step 3473, loss: 0.352256\n",
      "step 3474, loss: 0.351648\n",
      "step 3475, loss: 0.351042\n",
      "step 3476, loss: 0.350436\n",
      "step 3477, loss: 0.349831\n",
      "step 3478, loss: 0.349227\n",
      "step 3479, loss: 0.348624\n",
      "step 3480, loss: 0.348023\n",
      "step 3481, loss: 0.347422\n",
      "step 3482, loss: 0.346822\n",
      "step 3483, loss: 0.346223\n",
      "step 3484, loss: 0.345625\n",
      "step 3485, loss: 0.345028\n",
      "step 3486, loss: 0.344432\n",
      "step 3487, loss: 0.343837\n",
      "step 3488, loss: 0.343243\n",
      "step 3489, loss: 0.342650\n",
      "step 3490, loss: 0.342058\n",
      "step 3491, loss: 0.341467\n",
      "step 3492, loss: 0.340877\n",
      "step 3493, loss: 0.340288\n",
      "step 3494, loss: 0.339700\n",
      "step 3495, loss: 0.339112\n",
      "step 3496, loss: 0.338526\n",
      "step 3497, loss: 0.337941\n",
      "step 3498, loss: 0.337357\n",
      "step 3499, loss: 0.336773\n",
      "step 3500, loss: 0.336191\n",
      "step 3501, loss: 0.335609\n",
      "step 3502, loss: 0.335029\n",
      "step 3503, loss: 0.334449\n",
      "step 3504, loss: 0.333871\n",
      "step 3505, loss: 0.333293\n",
      "step 3506, loss: 0.332717\n",
      "step 3507, loss: 0.332141\n",
      "step 3508, loss: 0.331566\n",
      "step 3509, loss: 0.330993\n",
      "step 3510, loss: 0.330420\n",
      "step 3511, loss: 0.329848\n",
      "step 3512, loss: 0.329277\n",
      "step 3513, loss: 0.328707\n",
      "step 3514, loss: 0.328138\n",
      "step 3515, loss: 0.327570\n",
      "step 3516, loss: 0.327003\n",
      "step 3517, loss: 0.326437\n",
      "step 3518, loss: 0.325871\n",
      "step 3519, loss: 0.325307\n",
      "step 3520, loss: 0.324744\n",
      "step 3521, loss: 0.324181\n",
      "step 3522, loss: 0.323620\n",
      "step 3523, loss: 0.323059\n",
      "step 3524, loss: 0.322500\n",
      "step 3525, loss: 0.321941\n",
      "step 3526, loss: 0.321383\n",
      "step 3527, loss: 0.320827\n",
      "step 3528, loss: 0.320271\n",
      "step 3529, loss: 0.319716\n",
      "step 3530, loss: 0.319162\n",
      "step 3531, loss: 0.318608\n",
      "step 3532, loss: 0.318056\n",
      "step 3533, loss: 0.317505\n",
      "step 3534, loss: 0.316955\n",
      "step 3535, loss: 0.316405\n",
      "step 3536, loss: 0.315857\n",
      "step 3537, loss: 0.315309\n",
      "step 3538, loss: 0.314763\n",
      "step 3539, loss: 0.314217\n",
      "step 3540, loss: 0.313672\n",
      "step 3541, loss: 0.313128\n",
      "step 3542, loss: 0.312585\n",
      "step 3543, loss: 0.312043\n",
      "step 3544, loss: 0.311502\n",
      "step 3545, loss: 0.310962\n",
      "step 3546, loss: 0.310423\n",
      "step 3547, loss: 0.309884\n",
      "step 3548, loss: 0.309347\n",
      "step 3549, loss: 0.308811\n",
      "step 3550, loss: 0.308275\n",
      "step 3551, loss: 0.307741\n",
      "step 3552, loss: 0.307207\n",
      "step 3553, loss: 0.306675\n",
      "step 3554, loss: 0.306143\n",
      "step 3555, loss: 0.305612\n",
      "step 3556, loss: 0.305082\n",
      "step 3557, loss: 0.304552\n",
      "step 3558, loss: 0.304024\n",
      "step 3559, loss: 0.303497\n",
      "step 3560, loss: 0.302971\n",
      "step 3561, loss: 0.302445\n",
      "step 3562, loss: 0.301920\n",
      "step 3563, loss: 0.301396\n",
      "step 3564, loss: 0.300874\n",
      "step 3565, loss: 0.300352\n",
      "step 3566, loss: 0.299831\n",
      "step 3567, loss: 0.299310\n",
      "step 3568, loss: 0.298791\n",
      "step 3569, loss: 0.298272\n",
      "step 3570, loss: 0.297755\n",
      "step 3571, loss: 0.297238\n",
      "step 3572, loss: 0.296723\n",
      "step 3573, loss: 0.296208\n",
      "step 3574, loss: 0.295694\n",
      "step 3575, loss: 0.295180\n",
      "step 3576, loss: 0.294668\n",
      "step 3577, loss: 0.294157\n",
      "step 3578, loss: 0.293646\n",
      "step 3579, loss: 0.293137\n",
      "step 3580, loss: 0.292628\n",
      "step 3581, loss: 0.292120\n",
      "step 3582, loss: 0.291613\n",
      "step 3583, loss: 0.291107\n",
      "step 3584, loss: 0.290601\n",
      "step 3585, loss: 0.290097\n",
      "step 3586, loss: 0.289594\n",
      "step 3587, loss: 0.289091\n",
      "step 3588, loss: 0.288589\n",
      "step 3589, loss: 0.288088\n",
      "step 3590, loss: 0.287588\n",
      "step 3591, loss: 0.287089\n",
      "step 3592, loss: 0.286590\n",
      "step 3593, loss: 0.286093\n",
      "step 3594, loss: 0.285596\n",
      "step 3595, loss: 0.285100\n",
      "step 3596, loss: 0.284606\n",
      "step 3597, loss: 0.284111\n",
      "step 3598, loss: 0.283618\n",
      "step 3599, loss: 0.283126\n",
      "step 3600, loss: 0.282634\n",
      "step 3601, loss: 0.282144\n",
      "step 3602, loss: 0.281654\n",
      "step 3603, loss: 0.281165\n",
      "step 3604, loss: 0.280677\n",
      "step 3605, loss: 0.280190\n",
      "step 3606, loss: 0.279703\n",
      "step 3607, loss: 0.279218\n",
      "step 3608, loss: 0.278733\n",
      "step 3609, loss: 0.278249\n",
      "step 3610, loss: 0.277766\n",
      "step 3611, loss: 0.277284\n",
      "step 3612, loss: 0.276802\n",
      "step 3613, loss: 0.276322\n",
      "step 3614, loss: 0.275842\n",
      "step 3615, loss: 0.275363\n",
      "step 3616, loss: 0.274885\n",
      "step 3617, loss: 0.274408\n",
      "step 3618, loss: 0.273932\n",
      "step 3619, loss: 0.273456\n",
      "step 3620, loss: 0.272982\n",
      "step 3621, loss: 0.272508\n",
      "step 3622, loss: 0.272035\n",
      "step 3623, loss: 0.271563\n",
      "step 3624, loss: 0.271091\n",
      "step 3625, loss: 0.270621\n",
      "step 3626, loss: 0.270151\n",
      "step 3627, loss: 0.269682\n",
      "step 3628, loss: 0.269214\n",
      "step 3629, loss: 0.268747\n",
      "step 3630, loss: 0.268281\n",
      "step 3631, loss: 0.267815\n",
      "step 3632, loss: 0.267350\n",
      "step 3633, loss: 0.266886\n",
      "step 3634, loss: 0.266423\n",
      "step 3635, loss: 0.265961\n",
      "step 3636, loss: 0.265499\n",
      "step 3637, loss: 0.265039\n",
      "step 3638, loss: 0.264579\n",
      "step 3639, loss: 0.264120\n",
      "step 3640, loss: 0.263661\n",
      "step 3641, loss: 0.263204\n",
      "step 3642, loss: 0.262747\n",
      "step 3643, loss: 0.262291\n",
      "step 3644, loss: 0.261836\n",
      "step 3645, loss: 0.261382\n",
      "step 3646, loss: 0.260929\n",
      "step 3647, loss: 0.260476\n",
      "step 3648, loss: 0.260024\n",
      "step 3649, loss: 0.259573\n",
      "step 3650, loss: 0.259123\n",
      "step 3651, loss: 0.258674\n",
      "step 3652, loss: 0.258225\n",
      "step 3653, loss: 0.257777\n",
      "step 3654, loss: 0.257330\n",
      "step 3655, loss: 0.256884\n",
      "step 3656, loss: 0.256439\n",
      "step 3657, loss: 0.255994\n",
      "step 3658, loss: 0.255550\n",
      "step 3659, loss: 0.255107\n",
      "step 3660, loss: 0.254665\n",
      "step 3661, loss: 0.254223\n",
      "step 3662, loss: 0.253783\n",
      "step 3663, loss: 0.253343\n",
      "step 3664, loss: 0.252904\n",
      "step 3665, loss: 0.252465\n",
      "step 3666, loss: 0.252028\n",
      "step 3667, loss: 0.251591\n",
      "step 3668, loss: 0.251155\n",
      "step 3669, loss: 0.250720\n",
      "step 3670, loss: 0.250286\n",
      "step 3671, loss: 0.249852\n",
      "step 3672, loss: 0.249419\n",
      "step 3673, loss: 0.248987\n",
      "step 3674, loss: 0.248556\n",
      "step 3675, loss: 0.248126\n",
      "step 3676, loss: 0.247696\n",
      "step 3677, loss: 0.247267\n",
      "step 3678, loss: 0.246839\n",
      "step 3679, loss: 0.246412\n",
      "step 3680, loss: 0.245985\n",
      "step 3681, loss: 0.245560\n",
      "step 3682, loss: 0.245135\n",
      "step 3683, loss: 0.244711\n",
      "step 3684, loss: 0.244287\n",
      "step 3685, loss: 0.243865\n",
      "step 3686, loss: 0.243443\n",
      "step 3687, loss: 0.243021\n",
      "step 3688, loss: 0.242601\n",
      "step 3689, loss: 0.242182\n",
      "step 3690, loss: 0.241763\n",
      "step 3691, loss: 0.241345\n",
      "step 3692, loss: 0.240927\n",
      "step 3693, loss: 0.240511\n",
      "step 3694, loss: 0.240095\n",
      "step 3695, loss: 0.239680\n",
      "step 3696, loss: 0.239266\n",
      "step 3697, loss: 0.238852\n",
      "step 3698, loss: 0.238439\n",
      "step 3699, loss: 0.238027\n",
      "step 3700, loss: 0.237616\n",
      "step 3701, loss: 0.237205\n",
      "step 3702, loss: 0.236795\n",
      "step 3703, loss: 0.236386\n",
      "step 3704, loss: 0.235978\n",
      "step 3705, loss: 0.235571\n",
      "step 3706, loss: 0.235164\n",
      "step 3707, loss: 0.234758\n",
      "step 3708, loss: 0.234352\n",
      "step 3709, loss: 0.233948\n",
      "step 3710, loss: 0.233544\n",
      "step 3711, loss: 0.233141\n",
      "step 3712, loss: 0.232738\n",
      "step 3713, loss: 0.232337\n",
      "step 3714, loss: 0.231936\n",
      "step 3715, loss: 0.231536\n",
      "step 3716, loss: 0.231136\n",
      "step 3717, loss: 0.230738\n",
      "step 3718, loss: 0.230340\n",
      "step 3719, loss: 0.229942\n",
      "step 3720, loss: 0.229546\n",
      "step 3721, loss: 0.229150\n",
      "step 3722, loss: 0.228755\n",
      "step 3723, loss: 0.228361\n",
      "step 3724, loss: 0.227967\n",
      "step 3725, loss: 0.227574\n",
      "step 3726, loss: 0.227182\n",
      "step 3727, loss: 0.226791\n",
      "step 3728, loss: 0.226400\n",
      "step 3729, loss: 0.226010\n",
      "step 3730, loss: 0.225621\n",
      "step 3731, loss: 0.225232\n",
      "step 3732, loss: 0.224844\n",
      "step 3733, loss: 0.224457\n",
      "step 3734, loss: 0.224071\n",
      "step 3735, loss: 0.223685\n",
      "step 3736, loss: 0.223300\n",
      "step 3737, loss: 0.222916\n",
      "step 3738, loss: 0.222532\n",
      "step 3739, loss: 0.222149\n",
      "step 3740, loss: 0.221767\n",
      "step 3741, loss: 0.221386\n",
      "step 3742, loss: 0.221005\n",
      "step 3743, loss: 0.220625\n",
      "step 3744, loss: 0.220246\n",
      "step 3745, loss: 0.219867\n",
      "step 3746, loss: 0.219489\n",
      "step 3747, loss: 0.219112\n",
      "step 3748, loss: 0.218736\n",
      "step 3749, loss: 0.218360\n",
      "step 3750, loss: 0.217985\n",
      "step 3751, loss: 0.217611\n",
      "step 3752, loss: 0.217237\n",
      "step 3753, loss: 0.216864\n",
      "step 3754, loss: 0.216492\n",
      "step 3755, loss: 0.216120\n",
      "step 3756, loss: 0.215750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3757, loss: 0.215380\n",
      "step 3758, loss: 0.215010\n",
      "step 3759, loss: 0.214642\n",
      "step 3760, loss: 0.214274\n",
      "step 3761, loss: 0.213906\n",
      "step 3762, loss: 0.213540\n",
      "step 3763, loss: 0.213174\n",
      "step 3764, loss: 0.212809\n",
      "step 3765, loss: 0.212444\n",
      "step 3766, loss: 0.212080\n",
      "step 3767, loss: 0.211717\n",
      "step 3768, loss: 0.211355\n",
      "step 3769, loss: 0.210993\n",
      "step 3770, loss: 0.210632\n",
      "step 3771, loss: 0.210271\n",
      "step 3772, loss: 0.209912\n",
      "step 3773, loss: 0.209553\n",
      "step 3774, loss: 0.209194\n",
      "step 3775, loss: 0.208836\n",
      "step 3776, loss: 0.208479\n",
      "step 3777, loss: 0.208123\n",
      "step 3778, loss: 0.207767\n",
      "step 3779, loss: 0.207413\n",
      "step 3780, loss: 0.207058\n",
      "step 3781, loss: 0.206704\n",
      "step 3782, loss: 0.206352\n",
      "step 3783, loss: 0.205999\n",
      "step 3784, loss: 0.205648\n",
      "step 3785, loss: 0.205297\n",
      "step 3786, loss: 0.204946\n",
      "step 3787, loss: 0.204597\n",
      "step 3788, loss: 0.204248\n",
      "step 3789, loss: 0.203899\n",
      "step 3790, loss: 0.203552\n",
      "step 3791, loss: 0.203205\n",
      "step 3792, loss: 0.202858\n",
      "step 3793, loss: 0.202513\n",
      "step 3794, loss: 0.202168\n",
      "step 3795, loss: 0.201823\n",
      "step 3796, loss: 0.201480\n",
      "step 3797, loss: 0.201137\n",
      "step 3798, loss: 0.200794\n",
      "step 3799, loss: 0.200453\n",
      "step 3800, loss: 0.200111\n",
      "step 3801, loss: 0.199771\n",
      "step 3802, loss: 0.199431\n",
      "step 3803, loss: 0.199092\n",
      "step 3804, loss: 0.198754\n",
      "step 3805, loss: 0.198416\n",
      "step 3806, loss: 0.198079\n",
      "step 3807, loss: 0.197742\n",
      "step 3808, loss: 0.197406\n",
      "step 3809, loss: 0.197071\n",
      "step 3810, loss: 0.196736\n",
      "step 3811, loss: 0.196402\n",
      "step 3812, loss: 0.196069\n",
      "step 3813, loss: 0.195736\n",
      "step 3814, loss: 0.195405\n",
      "step 3815, loss: 0.195073\n",
      "step 3816, loss: 0.194742\n",
      "step 3817, loss: 0.194412\n",
      "step 3818, loss: 0.194083\n",
      "step 3819, loss: 0.193754\n",
      "step 3820, loss: 0.193426\n",
      "step 3821, loss: 0.193098\n",
      "step 3822, loss: 0.192771\n",
      "step 3823, loss: 0.192445\n",
      "step 3824, loss: 0.192119\n",
      "step 3825, loss: 0.191794\n",
      "step 3826, loss: 0.191470\n",
      "step 3827, loss: 0.191146\n",
      "step 3828, loss: 0.190823\n",
      "step 3829, loss: 0.190500\n",
      "step 3830, loss: 0.190178\n",
      "step 3831, loss: 0.189857\n",
      "step 3832, loss: 0.189536\n",
      "step 3833, loss: 0.189216\n",
      "step 3834, loss: 0.188897\n",
      "step 3835, loss: 0.188578\n",
      "step 3836, loss: 0.188260\n",
      "step 3837, loss: 0.187942\n",
      "step 3838, loss: 0.187626\n",
      "step 3839, loss: 0.187309\n",
      "step 3840, loss: 0.186994\n",
      "step 3841, loss: 0.186679\n",
      "step 3842, loss: 0.186365\n",
      "step 3843, loss: 0.186051\n",
      "step 3844, loss: 0.185738\n",
      "step 3845, loss: 0.185425\n",
      "step 3846, loss: 0.185113\n",
      "step 3847, loss: 0.184802\n",
      "step 3848, loss: 0.184492\n",
      "step 3849, loss: 0.184182\n",
      "step 3850, loss: 0.183872\n",
      "step 3851, loss: 0.183563\n",
      "step 3852, loss: 0.183255\n",
      "step 3853, loss: 0.182947\n",
      "step 3854, loss: 0.182640\n",
      "step 3855, loss: 0.182334\n",
      "step 3856, loss: 0.182028\n",
      "step 3857, loss: 0.181723\n",
      "step 3858, loss: 0.181418\n",
      "step 3859, loss: 0.181114\n",
      "step 3860, loss: 0.180811\n",
      "step 3861, loss: 0.180508\n",
      "step 3862, loss: 0.180206\n",
      "step 3863, loss: 0.179904\n",
      "step 3864, loss: 0.179603\n",
      "step 3865, loss: 0.179303\n",
      "step 3866, loss: 0.179003\n",
      "step 3867, loss: 0.178704\n",
      "step 3868, loss: 0.178405\n",
      "step 3869, loss: 0.178107\n",
      "step 3870, loss: 0.177810\n",
      "step 3871, loss: 0.177513\n",
      "step 3872, loss: 0.177216\n",
      "step 3873, loss: 0.176921\n",
      "step 3874, loss: 0.176626\n",
      "step 3875, loss: 0.176331\n",
      "step 3876, loss: 0.176037\n",
      "step 3877, loss: 0.175744\n",
      "step 3878, loss: 0.175451\n",
      "step 3879, loss: 0.175159\n",
      "step 3880, loss: 0.174867\n",
      "step 3881, loss: 0.174576\n",
      "step 3882, loss: 0.174286\n",
      "step 3883, loss: 0.173996\n",
      "step 3884, loss: 0.173706\n",
      "step 3885, loss: 0.173418\n",
      "step 3886, loss: 0.173130\n",
      "step 3887, loss: 0.172842\n",
      "step 3888, loss: 0.172555\n",
      "step 3889, loss: 0.172269\n",
      "step 3890, loss: 0.171983\n",
      "step 3891, loss: 0.171698\n",
      "step 3892, loss: 0.171413\n",
      "step 3893, loss: 0.171129\n",
      "step 3894, loss: 0.170845\n",
      "step 3895, loss: 0.170562\n",
      "step 3896, loss: 0.170280\n",
      "step 3897, loss: 0.169998\n",
      "step 3898, loss: 0.169716\n",
      "step 3899, loss: 0.169436\n",
      "step 3900, loss: 0.169156\n",
      "step 3901, loss: 0.168876\n",
      "step 3902, loss: 0.168597\n",
      "step 3903, loss: 0.168318\n",
      "step 3904, loss: 0.168041\n",
      "step 3905, loss: 0.167763\n",
      "step 3906, loss: 0.167486\n",
      "step 3907, loss: 0.167210\n",
      "step 3908, loss: 0.166934\n",
      "step 3909, loss: 0.166659\n",
      "step 3910, loss: 0.166385\n",
      "step 3911, loss: 0.166111\n",
      "step 3912, loss: 0.165837\n",
      "step 3913, loss: 0.165564\n",
      "step 3914, loss: 0.165292\n",
      "step 3915, loss: 0.165020\n",
      "step 3916, loss: 0.164749\n",
      "step 3917, loss: 0.164478\n",
      "step 3918, loss: 0.164208\n",
      "step 3919, loss: 0.163938\n",
      "step 3920, loss: 0.163669\n",
      "step 3921, loss: 0.163401\n",
      "step 3922, loss: 0.163133\n",
      "step 3923, loss: 0.162865\n",
      "step 3924, loss: 0.162598\n",
      "step 3925, loss: 0.162332\n",
      "step 3926, loss: 0.162066\n",
      "step 3927, loss: 0.161801\n",
      "step 3928, loss: 0.161536\n",
      "step 3929, loss: 0.161272\n",
      "step 3930, loss: 0.161008\n",
      "step 3931, loss: 0.160745\n",
      "step 3932, loss: 0.160483\n",
      "step 3933, loss: 0.160221\n",
      "step 3934, loss: 0.159959\n",
      "step 3935, loss: 0.159698\n",
      "step 3936, loss: 0.159438\n",
      "step 3937, loss: 0.159178\n",
      "step 3938, loss: 0.158919\n",
      "step 3939, loss: 0.158660\n",
      "step 3940, loss: 0.158401\n",
      "step 3941, loss: 0.158144\n",
      "step 3942, loss: 0.157887\n",
      "step 3943, loss: 0.157630\n",
      "step 3944, loss: 0.157374\n",
      "step 3945, loss: 0.157118\n",
      "step 3946, loss: 0.156863\n",
      "step 3947, loss: 0.156608\n",
      "step 3948, loss: 0.156354\n",
      "step 3949, loss: 0.156100\n",
      "step 3950, loss: 0.155847\n",
      "step 3951, loss: 0.155595\n",
      "step 3952, loss: 0.155343\n",
      "step 3953, loss: 0.155091\n",
      "step 3954, loss: 0.154840\n",
      "step 3955, loss: 0.154590\n",
      "step 3956, loss: 0.154340\n",
      "step 3957, loss: 0.154091\n",
      "step 3958, loss: 0.153842\n",
      "step 3959, loss: 0.153593\n",
      "step 3960, loss: 0.153346\n",
      "step 3961, loss: 0.153098\n",
      "step 3962, loss: 0.152851\n",
      "step 3963, loss: 0.152605\n",
      "step 3964, loss: 0.152359\n",
      "step 3965, loss: 0.152114\n",
      "step 3966, loss: 0.151869\n",
      "step 3967, loss: 0.151625\n",
      "step 3968, loss: 0.151382\n",
      "step 3969, loss: 0.151139\n",
      "step 3970, loss: 0.150896\n",
      "step 3971, loss: 0.150654\n",
      "step 3972, loss: 0.150412\n",
      "step 3973, loss: 0.150171\n",
      "step 3974, loss: 0.149930\n",
      "step 3975, loss: 0.149690\n",
      "step 3976, loss: 0.149450\n",
      "step 3977, loss: 0.149211\n",
      "step 3978, loss: 0.148973\n",
      "step 3979, loss: 0.148735\n",
      "step 3980, loss: 0.148497\n",
      "step 3981, loss: 0.148260\n",
      "step 3982, loss: 0.148023\n",
      "step 3983, loss: 0.147787\n",
      "step 3984, loss: 0.147551\n",
      "step 3985, loss: 0.147316\n",
      "step 3986, loss: 0.147082\n",
      "step 3987, loss: 0.146847\n",
      "step 3988, loss: 0.146614\n",
      "step 3989, loss: 0.146380\n",
      "step 3990, loss: 0.146148\n",
      "step 3991, loss: 0.145916\n",
      "step 3992, loss: 0.145684\n",
      "step 3993, loss: 0.145453\n",
      "step 3994, loss: 0.145222\n",
      "step 3995, loss: 0.144992\n",
      "step 3996, loss: 0.144762\n",
      "step 3997, loss: 0.144532\n",
      "step 3998, loss: 0.144304\n",
      "step 3999, loss: 0.144075\n",
      "step 4000, loss: 0.143848\n",
      "step 4001, loss: 0.143620\n",
      "step 4002, loss: 0.143393\n",
      "step 4003, loss: 0.143167\n",
      "step 4004, loss: 0.142941\n",
      "step 4005, loss: 0.142715\n",
      "step 4006, loss: 0.142490\n",
      "step 4007, loss: 0.142266\n",
      "step 4008, loss: 0.142042\n",
      "step 4009, loss: 0.141818\n",
      "step 4010, loss: 0.141595\n",
      "step 4011, loss: 0.141373\n",
      "step 4012, loss: 0.141151\n",
      "step 4013, loss: 0.140929\n",
      "step 4014, loss: 0.140708\n",
      "step 4015, loss: 0.140487\n",
      "step 4016, loss: 0.140266\n",
      "step 4017, loss: 0.140047\n",
      "step 4018, loss: 0.139827\n",
      "step 4019, loss: 0.139608\n",
      "step 4020, loss: 0.139390\n",
      "step 4021, loss: 0.139172\n",
      "step 4022, loss: 0.138954\n",
      "step 4023, loss: 0.138737\n",
      "step 4024, loss: 0.138520\n",
      "step 4025, loss: 0.138304\n",
      "step 4026, loss: 0.138088\n",
      "step 4027, loss: 0.137873\n",
      "step 4028, loss: 0.137659\n",
      "step 4029, loss: 0.137444\n",
      "step 4030, loss: 0.137230\n",
      "step 4031, loss: 0.137017\n",
      "step 4032, loss: 0.136804\n",
      "step 4033, loss: 0.136591\n",
      "step 4034, loss: 0.136379\n",
      "step 4035, loss: 0.136168\n",
      "step 4036, loss: 0.135957\n",
      "step 4037, loss: 0.135746\n",
      "step 4038, loss: 0.135536\n",
      "step 4039, loss: 0.135326\n",
      "step 4040, loss: 0.135117\n",
      "step 4041, loss: 0.134908\n",
      "step 4042, loss: 0.134699\n",
      "step 4043, loss: 0.134491\n",
      "step 4044, loss: 0.134284\n",
      "step 4045, loss: 0.134077\n",
      "step 4046, loss: 0.133870\n",
      "step 4047, loss: 0.133664\n",
      "step 4048, loss: 0.133458\n",
      "step 4049, loss: 0.133253\n",
      "step 4050, loss: 0.133048\n",
      "step 4051, loss: 0.132844\n",
      "step 4052, loss: 0.132640\n",
      "step 4053, loss: 0.132436\n",
      "step 4054, loss: 0.132233\n",
      "step 4055, loss: 0.132031\n",
      "step 4056, loss: 0.131828\n",
      "step 4057, loss: 0.131627\n",
      "step 4058, loss: 0.131425\n",
      "step 4059, loss: 0.131224\n",
      "step 4060, loss: 0.131024\n",
      "step 4061, loss: 0.130824\n",
      "step 4062, loss: 0.130624\n",
      "step 4063, loss: 0.130425\n",
      "step 4064, loss: 0.130227\n",
      "step 4065, loss: 0.130028\n",
      "step 4066, loss: 0.129830\n",
      "step 4067, loss: 0.129633\n",
      "step 4068, loss: 0.129436\n",
      "step 4069, loss: 0.129239\n",
      "step 4070, loss: 0.129043\n",
      "step 4071, loss: 0.128848\n",
      "step 4072, loss: 0.128652\n",
      "step 4073, loss: 0.128458\n",
      "step 4074, loss: 0.128263\n",
      "step 4075, loss: 0.128069\n",
      "step 4076, loss: 0.127876\n",
      "step 4077, loss: 0.127682\n",
      "step 4078, loss: 0.127490\n",
      "step 4079, loss: 0.127297\n",
      "step 4080, loss: 0.127106\n",
      "step 4081, loss: 0.126914\n",
      "step 4082, loss: 0.126723\n",
      "step 4083, loss: 0.126533\n",
      "step 4084, loss: 0.126342\n",
      "step 4085, loss: 0.126153\n",
      "step 4086, loss: 0.125963\n",
      "step 4087, loss: 0.125774\n",
      "step 4088, loss: 0.125586\n",
      "step 4089, loss: 0.125398\n",
      "step 4090, loss: 0.125210\n",
      "step 4091, loss: 0.125023\n",
      "step 4092, loss: 0.124836\n",
      "step 4093, loss: 0.124649\n",
      "step 4094, loss: 0.124463\n",
      "step 4095, loss: 0.124278\n",
      "step 4096, loss: 0.124092\n",
      "step 4097, loss: 0.123908\n",
      "step 4098, loss: 0.123723\n",
      "step 4099, loss: 0.123539\n",
      "step 4100, loss: 0.123356\n",
      "step 4101, loss: 0.123172\n",
      "step 4102, loss: 0.122990\n",
      "step 4103, loss: 0.122807\n",
      "step 4104, loss: 0.122625\n",
      "step 4105, loss: 0.122444\n",
      "step 4106, loss: 0.122262\n",
      "step 4107, loss: 0.122082\n",
      "step 4108, loss: 0.121901\n",
      "step 4109, loss: 0.121721\n",
      "step 4110, loss: 0.121542\n",
      "step 4111, loss: 0.121363\n",
      "step 4112, loss: 0.121184\n",
      "step 4113, loss: 0.121005\n",
      "step 4114, loss: 0.120827\n",
      "step 4115, loss: 0.120650\n",
      "step 4116, loss: 0.120473\n",
      "step 4117, loss: 0.120296\n",
      "step 4118, loss: 0.120119\n",
      "step 4119, loss: 0.119943\n",
      "step 4120, loss: 0.119768\n",
      "step 4121, loss: 0.119592\n",
      "step 4122, loss: 0.119417\n",
      "step 4123, loss: 0.119243\n",
      "step 4124, loss: 0.119069\n",
      "step 4125, loss: 0.118895\n",
      "step 4126, loss: 0.118722\n",
      "step 4127, loss: 0.118549\n",
      "step 4128, loss: 0.118376\n",
      "step 4129, loss: 0.118204\n",
      "step 4130, loss: 0.118032\n",
      "step 4131, loss: 0.117861\n",
      "step 4132, loss: 0.117690\n",
      "step 4133, loss: 0.117519\n",
      "step 4134, loss: 0.117349\n",
      "step 4135, loss: 0.117179\n",
      "step 4136, loss: 0.117009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4137, loss: 0.116840\n",
      "step 4138, loss: 0.116672\n",
      "step 4139, loss: 0.116503\n",
      "step 4140, loss: 0.116335\n",
      "step 4141, loss: 0.116168\n",
      "step 4142, loss: 0.116000\n",
      "step 4143, loss: 0.115834\n",
      "step 4144, loss: 0.115667\n",
      "step 4145, loss: 0.115501\n",
      "step 4146, loss: 0.115335\n",
      "step 4147, loss: 0.115170\n",
      "step 4148, loss: 0.115005\n",
      "step 4149, loss: 0.114840\n",
      "step 4150, loss: 0.114676\n",
      "step 4151, loss: 0.114512\n",
      "step 4152, loss: 0.114349\n",
      "step 4153, loss: 0.114186\n",
      "step 4154, loss: 0.114023\n",
      "step 4155, loss: 0.113861\n",
      "step 4156, loss: 0.113699\n",
      "step 4157, loss: 0.113537\n",
      "step 4158, loss: 0.113376\n",
      "step 4159, loss: 0.113215\n",
      "step 4160, loss: 0.113054\n",
      "step 4161, loss: 0.112894\n",
      "step 4162, loss: 0.112734\n",
      "step 4163, loss: 0.112575\n",
      "step 4164, loss: 0.112415\n",
      "step 4165, loss: 0.112257\n",
      "step 4166, loss: 0.112098\n",
      "step 4167, loss: 0.111940\n",
      "step 4168, loss: 0.111782\n",
      "step 4169, loss: 0.111625\n",
      "step 4170, loss: 0.111468\n",
      "step 4171, loss: 0.111312\n",
      "step 4172, loss: 0.111155\n",
      "step 4173, loss: 0.110999\n",
      "step 4174, loss: 0.110844\n",
      "step 4175, loss: 0.110688\n",
      "step 4176, loss: 0.110534\n",
      "step 4177, loss: 0.110379\n",
      "step 4178, loss: 0.110225\n",
      "step 4179, loss: 0.110071\n",
      "step 4180, loss: 0.109918\n",
      "step 4181, loss: 0.109765\n",
      "step 4182, loss: 0.109612\n",
      "step 4183, loss: 0.109460\n",
      "step 4184, loss: 0.109308\n",
      "step 4185, loss: 0.109156\n",
      "step 4186, loss: 0.109005\n",
      "step 4187, loss: 0.108854\n",
      "step 4188, loss: 0.108703\n",
      "step 4189, loss: 0.108553\n",
      "step 4190, loss: 0.108403\n",
      "step 4191, loss: 0.108253\n",
      "step 4192, loss: 0.108104\n",
      "step 4193, loss: 0.107955\n",
      "step 4194, loss: 0.107806\n",
      "step 4195, loss: 0.107658\n",
      "step 4196, loss: 0.107510\n",
      "step 4197, loss: 0.107363\n",
      "step 4198, loss: 0.107215\n",
      "step 4199, loss: 0.107068\n",
      "step 4200, loss: 0.106922\n",
      "step 4201, loss: 0.106776\n",
      "step 4202, loss: 0.106630\n",
      "step 4203, loss: 0.106484\n",
      "step 4204, loss: 0.106339\n",
      "step 4205, loss: 0.106194\n",
      "step 4206, loss: 0.106049\n",
      "step 4207, loss: 0.105905\n",
      "step 4208, loss: 0.105761\n",
      "step 4209, loss: 0.105618\n",
      "step 4210, loss: 0.105474\n",
      "step 4211, loss: 0.105332\n",
      "step 4212, loss: 0.105189\n",
      "step 4213, loss: 0.105047\n",
      "step 4214, loss: 0.104905\n",
      "step 4215, loss: 0.104763\n",
      "step 4216, loss: 0.104622\n",
      "step 4217, loss: 0.104481\n",
      "step 4218, loss: 0.104341\n",
      "step 4219, loss: 0.104200\n",
      "step 4220, loss: 0.104060\n",
      "step 4221, loss: 0.103921\n",
      "step 4222, loss: 0.103781\n",
      "step 4223, loss: 0.103642\n",
      "step 4224, loss: 0.103504\n",
      "step 4225, loss: 0.103365\n",
      "step 4226, loss: 0.103227\n",
      "step 4227, loss: 0.103090\n",
      "step 4228, loss: 0.102952\n",
      "step 4229, loss: 0.102815\n",
      "step 4230, loss: 0.102678\n",
      "step 4231, loss: 0.102542\n",
      "step 4232, loss: 0.102406\n",
      "step 4233, loss: 0.102270\n",
      "step 4234, loss: 0.102134\n",
      "step 4235, loss: 0.101999\n",
      "step 4236, loss: 0.101864\n",
      "step 4237, loss: 0.101730\n",
      "step 4238, loss: 0.101596\n",
      "step 4239, loss: 0.101462\n",
      "step 4240, loss: 0.101328\n",
      "step 4241, loss: 0.101195\n",
      "step 4242, loss: 0.101062\n",
      "step 4243, loss: 0.100929\n",
      "step 4244, loss: 0.100797\n",
      "step 4245, loss: 0.100665\n",
      "step 4246, loss: 0.100533\n",
      "step 4247, loss: 0.100402\n",
      "step 4248, loss: 0.100271\n",
      "step 4249, loss: 0.100140\n",
      "step 4250, loss: 0.100009\n",
      "step 4251, loss: 0.099879\n",
      "step 4252, loss: 0.099749\n",
      "step 4253, loss: 0.099619\n",
      "step 4254, loss: 0.099490\n",
      "step 4255, loss: 0.099361\n",
      "step 4256, loss: 0.099232\n",
      "step 4257, loss: 0.099104\n",
      "step 4258, loss: 0.098976\n",
      "step 4259, loss: 0.098848\n",
      "step 4260, loss: 0.098721\n",
      "step 4261, loss: 0.098594\n",
      "step 4262, loss: 0.098467\n",
      "step 4263, loss: 0.098340\n",
      "step 4264, loss: 0.098214\n",
      "step 4265, loss: 0.098088\n",
      "step 4266, loss: 0.097962\n",
      "step 4267, loss: 0.097837\n",
      "step 4268, loss: 0.097712\n",
      "step 4269, loss: 0.097587\n",
      "step 4270, loss: 0.097462\n",
      "step 4271, loss: 0.097338\n",
      "step 4272, loss: 0.097214\n",
      "step 4273, loss: 0.097090\n",
      "step 4274, loss: 0.096967\n",
      "step 4275, loss: 0.096844\n",
      "step 4276, loss: 0.096721\n",
      "step 4277, loss: 0.096599\n",
      "step 4278, loss: 0.096477\n",
      "step 4279, loss: 0.096355\n",
      "step 4280, loss: 0.096233\n",
      "step 4281, loss: 0.096112\n",
      "step 4282, loss: 0.095991\n",
      "step 4283, loss: 0.095870\n",
      "step 4284, loss: 0.095749\n",
      "step 4285, loss: 0.095629\n",
      "step 4286, loss: 0.095509\n",
      "step 4287, loss: 0.095390\n",
      "step 4288, loss: 0.095270\n",
      "step 4289, loss: 0.095151\n",
      "step 4290, loss: 0.095032\n",
      "step 4291, loss: 0.094914\n",
      "step 4292, loss: 0.094796\n",
      "step 4293, loss: 0.094678\n",
      "step 4294, loss: 0.094560\n",
      "step 4295, loss: 0.094443\n",
      "step 4296, loss: 0.094326\n",
      "step 4297, loss: 0.094209\n",
      "step 4298, loss: 0.094092\n",
      "step 4299, loss: 0.093976\n",
      "step 4300, loss: 0.093860\n",
      "step 4301, loss: 0.093744\n",
      "step 4302, loss: 0.093629\n",
      "step 4303, loss: 0.093514\n",
      "step 4304, loss: 0.093399\n",
      "step 4305, loss: 0.093284\n",
      "step 4306, loss: 0.093170\n",
      "step 4307, loss: 0.093056\n",
      "step 4308, loss: 0.092942\n",
      "step 4309, loss: 0.092829\n",
      "step 4310, loss: 0.092715\n",
      "step 4311, loss: 0.092602\n",
      "step 4312, loss: 0.092490\n",
      "step 4313, loss: 0.092377\n",
      "step 4314, loss: 0.092265\n",
      "step 4315, loss: 0.092153\n",
      "step 4316, loss: 0.092041\n",
      "step 4317, loss: 0.091930\n",
      "step 4318, loss: 0.091819\n",
      "step 4319, loss: 0.091708\n",
      "step 4320, loss: 0.091597\n",
      "step 4321, loss: 0.091487\n",
      "step 4322, loss: 0.091377\n",
      "step 4323, loss: 0.091267\n",
      "step 4324, loss: 0.091158\n",
      "step 4325, loss: 0.091048\n",
      "step 4326, loss: 0.090939\n",
      "step 4327, loss: 0.090831\n",
      "step 4328, loss: 0.090722\n",
      "step 4329, loss: 0.090614\n",
      "step 4330, loss: 0.090506\n",
      "step 4331, loss: 0.090398\n",
      "step 4332, loss: 0.090291\n",
      "step 4333, loss: 0.090184\n",
      "step 4334, loss: 0.090077\n",
      "step 4335, loss: 0.089970\n",
      "step 4336, loss: 0.089863\n",
      "step 4337, loss: 0.089757\n",
      "step 4338, loss: 0.089651\n",
      "step 4339, loss: 0.089546\n",
      "step 4340, loss: 0.089440\n",
      "step 4341, loss: 0.089335\n",
      "step 4342, loss: 0.089230\n",
      "step 4343, loss: 0.089126\n",
      "step 4344, loss: 0.089021\n",
      "step 4345, loss: 0.088917\n",
      "step 4346, loss: 0.088813\n",
      "step 4347, loss: 0.088709\n",
      "step 4348, loss: 0.088606\n",
      "step 4349, loss: 0.088503\n",
      "step 4350, loss: 0.088400\n",
      "step 4351, loss: 0.088297\n",
      "step 4352, loss: 0.088195\n",
      "step 4353, loss: 0.088093\n",
      "step 4354, loss: 0.087991\n",
      "step 4355, loss: 0.087889\n",
      "step 4356, loss: 0.087788\n",
      "step 4357, loss: 0.087687\n",
      "step 4358, loss: 0.087586\n",
      "step 4359, loss: 0.087485\n",
      "step 4360, loss: 0.087385\n",
      "step 4361, loss: 0.087284\n",
      "step 4362, loss: 0.087184\n",
      "step 4363, loss: 0.087085\n",
      "step 4364, loss: 0.086985\n",
      "step 4365, loss: 0.086886\n",
      "step 4366, loss: 0.086787\n",
      "step 4367, loss: 0.086688\n",
      "step 4368, loss: 0.086590\n",
      "step 4369, loss: 0.086492\n",
      "step 4370, loss: 0.086394\n",
      "step 4371, loss: 0.086296\n",
      "step 4372, loss: 0.086198\n",
      "step 4373, loss: 0.086101\n",
      "step 4374, loss: 0.086004\n",
      "step 4375, loss: 0.085907\n",
      "step 4376, loss: 0.085810\n",
      "step 4377, loss: 0.085714\n",
      "step 4378, loss: 0.085618\n",
      "step 4379, loss: 0.085522\n",
      "step 4380, loss: 0.085426\n",
      "step 4381, loss: 0.085331\n",
      "step 4382, loss: 0.085236\n",
      "step 4383, loss: 0.085141\n",
      "step 4384, loss: 0.085046\n",
      "step 4385, loss: 0.084951\n",
      "step 4386, loss: 0.084857\n",
      "step 4387, loss: 0.084763\n",
      "step 4388, loss: 0.084669\n",
      "step 4389, loss: 0.084576\n",
      "step 4390, loss: 0.084482\n",
      "step 4391, loss: 0.084389\n",
      "step 4392, loss: 0.084296\n",
      "step 4393, loss: 0.084203\n",
      "step 4394, loss: 0.084111\n",
      "step 4395, loss: 0.084019\n",
      "step 4396, loss: 0.083927\n",
      "step 4397, loss: 0.083835\n",
      "step 4398, loss: 0.083743\n",
      "step 4399, loss: 0.083652\n",
      "step 4400, loss: 0.083561\n",
      "step 4401, loss: 0.083470\n",
      "step 4402, loss: 0.083379\n",
      "step 4403, loss: 0.083289\n",
      "step 4404, loss: 0.083198\n",
      "step 4405, loss: 0.083108\n",
      "step 4406, loss: 0.083019\n",
      "step 4407, loss: 0.082929\n",
      "step 4408, loss: 0.082840\n",
      "step 4409, loss: 0.082751\n",
      "step 4410, loss: 0.082662\n",
      "step 4411, loss: 0.082573\n",
      "step 4412, loss: 0.082485\n",
      "step 4413, loss: 0.082396\n",
      "step 4414, loss: 0.082308\n",
      "step 4415, loss: 0.082220\n",
      "step 4416, loss: 0.082133\n",
      "step 4417, loss: 0.082045\n",
      "step 4418, loss: 0.081958\n",
      "step 4419, loss: 0.081871\n",
      "step 4420, loss: 0.081784\n",
      "step 4421, loss: 0.081698\n",
      "step 4422, loss: 0.081611\n",
      "step 4423, loss: 0.081525\n",
      "step 4424, loss: 0.081439\n",
      "step 4425, loss: 0.081353\n",
      "step 4426, loss: 0.081268\n",
      "step 4427, loss: 0.081183\n",
      "step 4428, loss: 0.081098\n",
      "step 4429, loss: 0.081013\n",
      "step 4430, loss: 0.080928\n",
      "step 4431, loss: 0.080844\n",
      "step 4432, loss: 0.080759\n",
      "step 4433, loss: 0.080675\n",
      "step 4434, loss: 0.080591\n",
      "step 4435, loss: 0.080508\n",
      "step 4436, loss: 0.080424\n",
      "step 4437, loss: 0.080341\n",
      "step 4438, loss: 0.080258\n",
      "step 4439, loss: 0.080175\n",
      "step 4440, loss: 0.080093\n",
      "step 4441, loss: 0.080010\n",
      "step 4442, loss: 0.079928\n",
      "step 4443, loss: 0.079846\n",
      "step 4444, loss: 0.079764\n",
      "step 4445, loss: 0.079683\n",
      "step 4446, loss: 0.079601\n",
      "step 4447, loss: 0.079520\n",
      "step 4448, loss: 0.079439\n",
      "step 4449, loss: 0.079358\n",
      "step 4450, loss: 0.079278\n",
      "step 4451, loss: 0.079197\n",
      "step 4452, loss: 0.079117\n",
      "step 4453, loss: 0.079037\n",
      "step 4454, loss: 0.078957\n",
      "step 4455, loss: 0.078878\n",
      "step 4456, loss: 0.078798\n",
      "step 4457, loss: 0.078719\n",
      "step 4458, loss: 0.078640\n",
      "step 4459, loss: 0.078561\n",
      "step 4460, loss: 0.078482\n",
      "step 4461, loss: 0.078404\n",
      "step 4462, loss: 0.078326\n",
      "step 4463, loss: 0.078248\n",
      "step 4464, loss: 0.078170\n",
      "step 4465, loss: 0.078092\n",
      "step 4466, loss: 0.078015\n",
      "step 4467, loss: 0.077937\n",
      "step 4468, loss: 0.077860\n",
      "step 4469, loss: 0.077783\n",
      "step 4470, loss: 0.077706\n",
      "step 4471, loss: 0.077630\n",
      "step 4472, loss: 0.077554\n",
      "step 4473, loss: 0.077477\n",
      "step 4474, loss: 0.077402\n",
      "step 4475, loss: 0.077326\n",
      "step 4476, loss: 0.077250\n",
      "step 4477, loss: 0.077175\n",
      "step 4478, loss: 0.077099\n",
      "step 4479, loss: 0.077024\n",
      "step 4480, loss: 0.076949\n",
      "step 4481, loss: 0.076875\n",
      "step 4482, loss: 0.076800\n",
      "step 4483, loss: 0.076726\n",
      "step 4484, loss: 0.076652\n",
      "step 4485, loss: 0.076578\n",
      "step 4486, loss: 0.076504\n",
      "step 4487, loss: 0.076431\n",
      "step 4488, loss: 0.076357\n",
      "step 4489, loss: 0.076284\n",
      "step 4490, loss: 0.076211\n",
      "step 4491, loss: 0.076138\n",
      "step 4492, loss: 0.076066\n",
      "step 4493, loss: 0.075993\n",
      "step 4494, loss: 0.075921\n",
      "step 4495, loss: 0.075849\n",
      "step 4496, loss: 0.075777\n",
      "step 4497, loss: 0.075705\n",
      "step 4498, loss: 0.075633\n",
      "step 4499, loss: 0.075562\n",
      "step 4500, loss: 0.075491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4501, loss: 0.075420\n",
      "step 4502, loss: 0.075349\n",
      "step 4503, loss: 0.075278\n",
      "step 4504, loss: 0.075208\n",
      "step 4505, loss: 0.075137\n",
      "step 4506, loss: 0.075067\n",
      "step 4507, loss: 0.074997\n",
      "step 4508, loss: 0.074927\n",
      "step 4509, loss: 0.074858\n",
      "step 4510, loss: 0.074788\n",
      "step 4511, loss: 0.074719\n",
      "step 4512, loss: 0.074650\n",
      "step 4513, loss: 0.074581\n",
      "step 4514, loss: 0.074512\n",
      "step 4515, loss: 0.074443\n",
      "step 4516, loss: 0.074375\n",
      "step 4517, loss: 0.074306\n",
      "step 4518, loss: 0.074238\n",
      "step 4519, loss: 0.074170\n",
      "step 4520, loss: 0.074102\n",
      "step 4521, loss: 0.074035\n",
      "step 4522, loss: 0.073967\n",
      "step 4523, loss: 0.073900\n",
      "step 4524, loss: 0.073833\n",
      "step 4525, loss: 0.073766\n",
      "step 4526, loss: 0.073699\n",
      "step 4527, loss: 0.073633\n",
      "step 4528, loss: 0.073566\n",
      "step 4529, loss: 0.073500\n",
      "step 4530, loss: 0.073434\n",
      "step 4531, loss: 0.073368\n",
      "step 4532, loss: 0.073302\n",
      "step 4533, loss: 0.073236\n",
      "step 4534, loss: 0.073171\n",
      "step 4535, loss: 0.073105\n",
      "step 4536, loss: 0.073040\n",
      "step 4537, loss: 0.072975\n",
      "step 4538, loss: 0.072910\n",
      "step 4539, loss: 0.072846\n",
      "step 4540, loss: 0.072781\n",
      "step 4541, loss: 0.072717\n",
      "step 4542, loss: 0.072653\n",
      "step 4543, loss: 0.072589\n",
      "step 4544, loss: 0.072525\n",
      "step 4545, loss: 0.072461\n",
      "step 4546, loss: 0.072398\n",
      "step 4547, loss: 0.072334\n",
      "step 4548, loss: 0.072271\n",
      "step 4549, loss: 0.072208\n",
      "step 4550, loss: 0.072145\n",
      "step 4551, loss: 0.072082\n",
      "step 4552, loss: 0.072020\n",
      "step 4553, loss: 0.071957\n",
      "step 4554, loss: 0.071895\n",
      "step 4555, loss: 0.071833\n",
      "step 4556, loss: 0.071771\n",
      "step 4557, loss: 0.071709\n",
      "step 4558, loss: 0.071647\n",
      "step 4559, loss: 0.071585\n",
      "step 4560, loss: 0.071524\n",
      "step 4561, loss: 0.071463\n",
      "step 4562, loss: 0.071402\n",
      "step 4563, loss: 0.071341\n",
      "step 4564, loss: 0.071280\n",
      "step 4565, loss: 0.071219\n",
      "step 4566, loss: 0.071159\n",
      "step 4567, loss: 0.071099\n",
      "step 4568, loss: 0.071038\n",
      "step 4569, loss: 0.070978\n",
      "step 4570, loss: 0.070919\n",
      "step 4571, loss: 0.070859\n",
      "step 4572, loss: 0.070799\n",
      "step 4573, loss: 0.070740\n",
      "step 4574, loss: 0.070680\n",
      "step 4575, loss: 0.070621\n",
      "step 4576, loss: 0.070562\n",
      "step 4577, loss: 0.070503\n",
      "step 4578, loss: 0.070445\n",
      "step 4579, loss: 0.070386\n",
      "step 4580, loss: 0.070328\n",
      "step 4581, loss: 0.070269\n",
      "step 4582, loss: 0.070211\n",
      "step 4583, loss: 0.070153\n",
      "step 4584, loss: 0.070095\n",
      "step 4585, loss: 0.070038\n",
      "step 4586, loss: 0.069980\n",
      "step 4587, loss: 0.069923\n",
      "step 4588, loss: 0.069866\n",
      "step 4589, loss: 0.069808\n",
      "step 4590, loss: 0.069751\n",
      "step 4591, loss: 0.069695\n",
      "step 4592, loss: 0.069638\n",
      "step 4593, loss: 0.069581\n",
      "step 4594, loss: 0.069525\n",
      "step 4595, loss: 0.069469\n",
      "step 4596, loss: 0.069412\n",
      "step 4597, loss: 0.069356\n",
      "step 4598, loss: 0.069300\n",
      "step 4599, loss: 0.069245\n",
      "step 4600, loss: 0.069189\n",
      "step 4601, loss: 0.069134\n",
      "step 4602, loss: 0.069078\n",
      "step 4603, loss: 0.069023\n",
      "step 4604, loss: 0.068968\n",
      "step 4605, loss: 0.068913\n",
      "step 4606, loss: 0.068858\n",
      "step 4607, loss: 0.068804\n",
      "step 4608, loss: 0.068749\n",
      "step 4609, loss: 0.068695\n",
      "step 4610, loss: 0.068641\n",
      "step 4611, loss: 0.068587\n",
      "step 4612, loss: 0.068533\n",
      "step 4613, loss: 0.068479\n",
      "step 4614, loss: 0.068425\n",
      "step 4615, loss: 0.068371\n",
      "step 4616, loss: 0.068318\n",
      "step 4617, loss: 0.068265\n",
      "step 4618, loss: 0.068211\n",
      "step 4619, loss: 0.068158\n",
      "step 4620, loss: 0.068105\n",
      "step 4621, loss: 0.068053\n",
      "step 4622, loss: 0.068000\n",
      "step 4623, loss: 0.067947\n",
      "step 4624, loss: 0.067895\n",
      "step 4625, loss: 0.067843\n",
      "step 4626, loss: 0.067791\n",
      "step 4627, loss: 0.067739\n",
      "step 4628, loss: 0.067687\n",
      "step 4629, loss: 0.067635\n",
      "step 4630, loss: 0.067583\n",
      "step 4631, loss: 0.067532\n",
      "step 4632, loss: 0.067480\n",
      "step 4633, loss: 0.067429\n",
      "step 4634, loss: 0.067378\n",
      "step 4635, loss: 0.067327\n",
      "step 4636, loss: 0.067276\n",
      "step 4637, loss: 0.067225\n",
      "step 4638, loss: 0.067175\n",
      "step 4639, loss: 0.067124\n",
      "step 4640, loss: 0.067074\n",
      "step 4641, loss: 0.067024\n",
      "step 4642, loss: 0.066974\n",
      "step 4643, loss: 0.066924\n",
      "step 4644, loss: 0.066874\n",
      "step 4645, loss: 0.066824\n",
      "step 4646, loss: 0.066774\n",
      "step 4647, loss: 0.066725\n",
      "step 4648, loss: 0.066675\n",
      "step 4649, loss: 0.066626\n",
      "step 4650, loss: 0.066577\n",
      "step 4651, loss: 0.066528\n",
      "step 4652, loss: 0.066479\n",
      "step 4653, loss: 0.066430\n",
      "step 4654, loss: 0.066381\n",
      "step 4655, loss: 0.066333\n",
      "step 4656, loss: 0.066284\n",
      "step 4657, loss: 0.066236\n",
      "step 4658, loss: 0.066188\n",
      "step 4659, loss: 0.066140\n",
      "step 4660, loss: 0.066092\n",
      "step 4661, loss: 0.066044\n",
      "step 4662, loss: 0.065996\n",
      "step 4663, loss: 0.065949\n",
      "step 4664, loss: 0.065901\n",
      "step 4665, loss: 0.065854\n",
      "step 4666, loss: 0.065806\n",
      "step 4667, loss: 0.065759\n",
      "step 4668, loss: 0.065712\n",
      "step 4669, loss: 0.065665\n",
      "step 4670, loss: 0.065618\n",
      "step 4671, loss: 0.065572\n",
      "step 4672, loss: 0.065525\n",
      "step 4673, loss: 0.065479\n",
      "step 4674, loss: 0.065432\n",
      "step 4675, loss: 0.065386\n",
      "step 4676, loss: 0.065340\n",
      "step 4677, loss: 0.065294\n",
      "step 4678, loss: 0.065248\n",
      "step 4679, loss: 0.065202\n",
      "step 4680, loss: 0.065157\n",
      "step 4681, loss: 0.065111\n",
      "step 4682, loss: 0.065066\n",
      "step 4683, loss: 0.065020\n",
      "step 4684, loss: 0.064975\n",
      "step 4685, loss: 0.064930\n",
      "step 4686, loss: 0.064885\n",
      "step 4687, loss: 0.064840\n",
      "step 4688, loss: 0.064795\n",
      "step 4689, loss: 0.064750\n",
      "step 4690, loss: 0.064706\n",
      "step 4691, loss: 0.064661\n",
      "step 4692, loss: 0.064617\n",
      "step 4693, loss: 0.064573\n",
      "step 4694, loss: 0.064528\n",
      "step 4695, loss: 0.064484\n",
      "step 4696, loss: 0.064440\n",
      "step 4697, loss: 0.064396\n",
      "step 4698, loss: 0.064353\n",
      "step 4699, loss: 0.064309\n",
      "step 4700, loss: 0.064266\n",
      "step 4701, loss: 0.064222\n",
      "step 4702, loss: 0.064179\n",
      "step 4703, loss: 0.064136\n",
      "step 4704, loss: 0.064092\n",
      "step 4705, loss: 0.064049\n",
      "step 4706, loss: 0.064006\n",
      "step 4707, loss: 0.063964\n",
      "step 4708, loss: 0.063921\n",
      "step 4709, loss: 0.063878\n",
      "step 4710, loss: 0.063836\n",
      "step 4711, loss: 0.063793\n",
      "step 4712, loss: 0.063751\n",
      "step 4713, loss: 0.063709\n",
      "step 4714, loss: 0.063667\n",
      "step 4715, loss: 0.063625\n",
      "step 4716, loss: 0.063583\n",
      "step 4717, loss: 0.063541\n",
      "step 4718, loss: 0.063499\n",
      "step 4719, loss: 0.063458\n",
      "step 4720, loss: 0.063416\n",
      "step 4721, loss: 0.063375\n",
      "step 4722, loss: 0.063334\n",
      "step 4723, loss: 0.063292\n",
      "step 4724, loss: 0.063251\n",
      "step 4725, loss: 0.063210\n",
      "step 4726, loss: 0.063169\n",
      "step 4727, loss: 0.063129\n",
      "step 4728, loss: 0.063088\n",
      "step 4729, loss: 0.063047\n",
      "step 4730, loss: 0.063007\n",
      "step 4731, loss: 0.062966\n",
      "step 4732, loss: 0.062926\n",
      "step 4733, loss: 0.062886\n",
      "step 4734, loss: 0.062846\n",
      "step 4735, loss: 0.062806\n",
      "step 4736, loss: 0.062766\n",
      "step 4737, loss: 0.062726\n",
      "step 4738, loss: 0.062686\n",
      "step 4739, loss: 0.062647\n",
      "step 4740, loss: 0.062607\n",
      "step 4741, loss: 0.062568\n",
      "step 4742, loss: 0.062528\n",
      "step 4743, loss: 0.062489\n",
      "step 4744, loss: 0.062450\n",
      "step 4745, loss: 0.062411\n",
      "step 4746, loss: 0.062372\n",
      "step 4747, loss: 0.062333\n",
      "step 4748, loss: 0.062294\n",
      "step 4749, loss: 0.062255\n",
      "step 4750, loss: 0.062216\n",
      "step 4751, loss: 0.062178\n",
      "step 4752, loss: 0.062139\n",
      "step 4753, loss: 0.062101\n",
      "step 4754, loss: 0.062063\n",
      "step 4755, loss: 0.062025\n",
      "step 4756, loss: 0.061986\n",
      "step 4757, loss: 0.061948\n",
      "step 4758, loss: 0.061911\n",
      "step 4759, loss: 0.061873\n",
      "step 4760, loss: 0.061835\n",
      "step 4761, loss: 0.061797\n",
      "step 4762, loss: 0.061760\n",
      "step 4763, loss: 0.061722\n",
      "step 4764, loss: 0.061685\n",
      "step 4765, loss: 0.061648\n",
      "step 4766, loss: 0.061610\n",
      "step 4767, loss: 0.061573\n",
      "step 4768, loss: 0.061536\n",
      "step 4769, loss: 0.061499\n",
      "step 4770, loss: 0.061462\n",
      "step 4771, loss: 0.061426\n",
      "step 4772, loss: 0.061389\n",
      "step 4773, loss: 0.061352\n",
      "step 4774, loss: 0.061316\n",
      "step 4775, loss: 0.061279\n",
      "step 4776, loss: 0.061243\n",
      "step 4777, loss: 0.061207\n",
      "step 4778, loss: 0.061170\n",
      "step 4779, loss: 0.061134\n",
      "step 4780, loss: 0.061098\n",
      "step 4781, loss: 0.061062\n",
      "step 4782, loss: 0.061026\n",
      "step 4783, loss: 0.060991\n",
      "step 4784, loss: 0.060955\n",
      "step 4785, loss: 0.060919\n",
      "step 4786, loss: 0.060884\n",
      "step 4787, loss: 0.060848\n",
      "step 4788, loss: 0.060813\n",
      "step 4789, loss: 0.060778\n",
      "step 4790, loss: 0.060742\n",
      "step 4791, loss: 0.060707\n",
      "step 4792, loss: 0.060672\n",
      "step 4793, loss: 0.060637\n",
      "step 4794, loss: 0.060602\n",
      "step 4795, loss: 0.060568\n",
      "step 4796, loss: 0.060533\n",
      "step 4797, loss: 0.060498\n",
      "step 4798, loss: 0.060464\n",
      "step 4799, loss: 0.060429\n",
      "step 4800, loss: 0.060395\n",
      "step 4801, loss: 0.060360\n",
      "step 4802, loss: 0.060326\n",
      "step 4803, loss: 0.060292\n",
      "step 4804, loss: 0.060258\n",
      "step 4805, loss: 0.060224\n",
      "step 4806, loss: 0.060190\n",
      "step 4807, loss: 0.060156\n",
      "step 4808, loss: 0.060122\n",
      "step 4809, loss: 0.060088\n",
      "step 4810, loss: 0.060055\n",
      "step 4811, loss: 0.060021\n",
      "step 4812, loss: 0.059988\n",
      "step 4813, loss: 0.059954\n",
      "step 4814, loss: 0.059921\n",
      "step 4815, loss: 0.059888\n",
      "step 4816, loss: 0.059855\n",
      "step 4817, loss: 0.059821\n",
      "step 4818, loss: 0.059788\n",
      "step 4819, loss: 0.059755\n",
      "step 4820, loss: 0.059723\n",
      "step 4821, loss: 0.059690\n",
      "step 4822, loss: 0.059657\n",
      "step 4823, loss: 0.059624\n",
      "step 4824, loss: 0.059592\n",
      "step 4825, loss: 0.059559\n",
      "step 4826, loss: 0.059527\n",
      "step 4827, loss: 0.059494\n",
      "step 4828, loss: 0.059462\n",
      "step 4829, loss: 0.059430\n",
      "step 4830, loss: 0.059398\n",
      "step 4831, loss: 0.059365\n",
      "step 4832, loss: 0.059334\n",
      "step 4833, loss: 0.059302\n",
      "step 4834, loss: 0.059270\n",
      "step 4835, loss: 0.059238\n",
      "step 4836, loss: 0.059206\n",
      "step 4837, loss: 0.059174\n",
      "step 4838, loss: 0.059143\n",
      "step 4839, loss: 0.059111\n",
      "step 4840, loss: 0.059080\n",
      "step 4841, loss: 0.059048\n",
      "step 4842, loss: 0.059017\n",
      "step 4843, loss: 0.058986\n",
      "step 4844, loss: 0.058955\n",
      "step 4845, loss: 0.058924\n",
      "step 4846, loss: 0.058893\n",
      "step 4847, loss: 0.058862\n",
      "step 4848, loss: 0.058831\n",
      "step 4849, loss: 0.058800\n",
      "step 4850, loss: 0.058769\n",
      "step 4851, loss: 0.058738\n",
      "step 4852, loss: 0.058708\n",
      "step 4853, loss: 0.058677\n",
      "step 4854, loss: 0.058647\n",
      "step 4855, loss: 0.058616\n",
      "step 4856, loss: 0.058586\n",
      "step 4857, loss: 0.058555\n",
      "step 4858, loss: 0.058525\n",
      "step 4859, loss: 0.058495\n",
      "step 4860, loss: 0.058465\n",
      "step 4861, loss: 0.058435\n",
      "step 4862, loss: 0.058405\n",
      "step 4863, loss: 0.058375\n",
      "step 4864, loss: 0.058345\n",
      "step 4865, loss: 0.058315\n",
      "step 4866, loss: 0.058286\n",
      "step 4867, loss: 0.058256\n",
      "step 4868, loss: 0.058226\n",
      "step 4869, loss: 0.058197\n",
      "step 4870, loss: 0.058167\n",
      "step 4871, loss: 0.058138\n",
      "step 4872, loss: 0.058109\n",
      "step 4873, loss: 0.058079\n",
      "step 4874, loss: 0.058050\n",
      "step 4875, loss: 0.058021\n",
      "step 4876, loss: 0.057992\n",
      "step 4877, loss: 0.057963\n",
      "step 4878, loss: 0.057934\n",
      "step 4879, loss: 0.057905\n",
      "step 4880, loss: 0.057876\n",
      "step 4881, loss: 0.057847\n",
      "step 4882, loss: 0.057818\n",
      "step 4883, loss: 0.057790\n",
      "step 4884, loss: 0.057761\n",
      "step 4885, loss: 0.057733\n",
      "step 4886, loss: 0.057704\n",
      "step 4887, loss: 0.057676\n",
      "step 4888, loss: 0.057647\n",
      "step 4889, loss: 0.057619\n",
      "step 4890, loss: 0.057591\n",
      "step 4891, loss: 0.057563\n",
      "step 4892, loss: 0.057534\n",
      "step 4893, loss: 0.057506\n",
      "step 4894, loss: 0.057478\n",
      "step 4895, loss: 0.057450\n",
      "step 4896, loss: 0.057422\n",
      "step 4897, loss: 0.057395\n",
      "step 4898, loss: 0.057367\n",
      "step 4899, loss: 0.057339\n",
      "step 4900, loss: 0.057311\n",
      "step 4901, loss: 0.057284\n",
      "step 4902, loss: 0.057256\n",
      "step 4903, loss: 0.057229\n",
      "step 4904, loss: 0.057201\n",
      "step 4905, loss: 0.057174\n",
      "step 4906, loss: 0.057147\n",
      "step 4907, loss: 0.057119\n",
      "step 4908, loss: 0.057092\n",
      "step 4909, loss: 0.057065\n",
      "step 4910, loss: 0.057038\n",
      "step 4911, loss: 0.057011\n",
      "step 4912, loss: 0.056984\n",
      "step 4913, loss: 0.056957\n",
      "step 4914, loss: 0.056930\n",
      "step 4915, loss: 0.056903\n",
      "step 4916, loss: 0.056876\n",
      "step 4917, loss: 0.056850\n",
      "step 4918, loss: 0.056823\n",
      "step 4919, loss: 0.056796\n",
      "step 4920, loss: 0.056770\n",
      "step 4921, loss: 0.056743\n",
      "step 4922, loss: 0.056717\n",
      "step 4923, loss: 0.056690\n",
      "step 4924, loss: 0.056664\n",
      "step 4925, loss: 0.056638\n",
      "step 4926, loss: 0.056612\n",
      "step 4927, loss: 0.056585\n",
      "step 4928, loss: 0.056559\n",
      "step 4929, loss: 0.056533\n",
      "step 4930, loss: 0.056507\n",
      "step 4931, loss: 0.056481\n",
      "step 4932, loss: 0.056455\n",
      "step 4933, loss: 0.056429\n",
      "step 4934, loss: 0.056403\n",
      "step 4935, loss: 0.056378\n",
      "step 4936, loss: 0.056352\n",
      "step 4937, loss: 0.056326\n",
      "step 4938, loss: 0.056301\n",
      "step 4939, loss: 0.056275\n",
      "step 4940, loss: 0.056250\n",
      "step 4941, loss: 0.056224\n",
      "step 4942, loss: 0.056199\n",
      "step 4943, loss: 0.056173\n",
      "step 4944, loss: 0.056148\n",
      "step 4945, loss: 0.056123\n",
      "step 4946, loss: 0.056097\n",
      "step 4947, loss: 0.056072\n",
      "step 4948, loss: 0.056047\n",
      "step 4949, loss: 0.056022\n",
      "step 4950, loss: 0.055997\n",
      "step 4951, loss: 0.055972\n",
      "step 4952, loss: 0.055947\n",
      "step 4953, loss: 0.055922\n",
      "step 4954, loss: 0.055897\n",
      "step 4955, loss: 0.055873\n",
      "step 4956, loss: 0.055848\n",
      "step 4957, loss: 0.055823\n",
      "step 4958, loss: 0.055799\n",
      "step 4959, loss: 0.055774\n",
      "step 4960, loss: 0.055749\n",
      "step 4961, loss: 0.055725\n",
      "step 4962, loss: 0.055700\n",
      "step 4963, loss: 0.055676\n",
      "step 4964, loss: 0.055652\n",
      "step 4965, loss: 0.055627\n",
      "step 4966, loss: 0.055603\n",
      "step 4967, loss: 0.055579\n",
      "step 4968, loss: 0.055555\n",
      "step 4969, loss: 0.055531\n",
      "step 4970, loss: 0.055506\n",
      "step 4971, loss: 0.055482\n",
      "step 4972, loss: 0.055458\n",
      "step 4973, loss: 0.055434\n",
      "step 4974, loss: 0.055410\n",
      "step 4975, loss: 0.055387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4976, loss: 0.055363\n",
      "step 4977, loss: 0.055339\n",
      "step 4978, loss: 0.055315\n",
      "step 4979, loss: 0.055292\n",
      "step 4980, loss: 0.055268\n",
      "step 4981, loss: 0.055244\n",
      "step 4982, loss: 0.055221\n",
      "step 4983, loss: 0.055197\n",
      "step 4984, loss: 0.055174\n",
      "step 4985, loss: 0.055150\n",
      "step 4986, loss: 0.055127\n",
      "step 4987, loss: 0.055104\n",
      "step 4988, loss: 0.055080\n",
      "step 4989, loss: 0.055057\n",
      "step 4990, loss: 0.055034\n",
      "step 4991, loss: 0.055011\n",
      "step 4992, loss: 0.054988\n",
      "step 4993, loss: 0.054964\n",
      "step 4994, loss: 0.054941\n",
      "step 4995, loss: 0.054918\n",
      "step 4996, loss: 0.054895\n",
      "step 4997, loss: 0.054873\n",
      "step 4998, loss: 0.054850\n",
      "step 4999, loss: 0.054827\n",
      "step 5000, loss: 0.054804\n",
      "step 5001, loss: 0.054781\n",
      "step 5002, loss: 0.054758\n",
      "step 5003, loss: 0.054736\n",
      "step 5004, loss: 0.054713\n",
      "step 5005, loss: 0.054691\n",
      "step 5006, loss: 0.054668\n",
      "step 5007, loss: 0.054646\n",
      "step 5008, loss: 0.054623\n",
      "step 5009, loss: 0.054601\n",
      "step 5010, loss: 0.054578\n",
      "step 5011, loss: 0.054556\n",
      "step 5012, loss: 0.054533\n",
      "step 5013, loss: 0.054511\n",
      "step 5014, loss: 0.054489\n",
      "step 5015, loss: 0.054467\n",
      "step 5016, loss: 0.054445\n",
      "step 5017, loss: 0.054422\n",
      "step 5018, loss: 0.054400\n",
      "step 5019, loss: 0.054378\n",
      "step 5020, loss: 0.054356\n",
      "step 5021, loss: 0.054334\n",
      "step 5022, loss: 0.054312\n",
      "step 5023, loss: 0.054290\n",
      "step 5024, loss: 0.054269\n",
      "step 5025, loss: 0.054247\n",
      "step 5026, loss: 0.054225\n",
      "step 5027, loss: 0.054203\n",
      "step 5028, loss: 0.054181\n",
      "step 5029, loss: 0.054160\n",
      "step 5030, loss: 0.054138\n",
      "step 5031, loss: 0.054117\n",
      "step 5032, loss: 0.054095\n",
      "step 5033, loss: 0.054073\n",
      "step 5034, loss: 0.054052\n",
      "step 5035, loss: 0.054030\n",
      "step 5036, loss: 0.054009\n",
      "step 5037, loss: 0.053988\n",
      "step 5038, loss: 0.053966\n",
      "step 5039, loss: 0.053945\n",
      "step 5040, loss: 0.053924\n",
      "step 5041, loss: 0.053902\n",
      "step 5042, loss: 0.053881\n",
      "step 5043, loss: 0.053860\n",
      "step 5044, loss: 0.053839\n",
      "step 5045, loss: 0.053818\n",
      "step 5046, loss: 0.053797\n",
      "step 5047, loss: 0.053776\n",
      "step 5048, loss: 0.053755\n",
      "step 5049, loss: 0.053734\n",
      "step 5050, loss: 0.053713\n",
      "step 5051, loss: 0.053692\n",
      "step 5052, loss: 0.053671\n",
      "step 5053, loss: 0.053650\n",
      "step 5054, loss: 0.053629\n",
      "step 5055, loss: 0.053609\n",
      "step 5056, loss: 0.053588\n",
      "step 5057, loss: 0.053567\n",
      "step 5058, loss: 0.053547\n",
      "step 5059, loss: 0.053526\n",
      "step 5060, loss: 0.053505\n",
      "step 5061, loss: 0.053485\n",
      "step 5062, loss: 0.053464\n",
      "step 5063, loss: 0.053444\n",
      "step 5064, loss: 0.053423\n",
      "step 5065, loss: 0.053403\n",
      "step 5066, loss: 0.053382\n",
      "step 5067, loss: 0.053362\n",
      "step 5068, loss: 0.053342\n",
      "step 5069, loss: 0.053321\n",
      "step 5070, loss: 0.053301\n",
      "step 5071, loss: 0.053281\n",
      "step 5072, loss: 0.053261\n",
      "step 5073, loss: 0.053240\n",
      "step 5074, loss: 0.053220\n",
      "step 5075, loss: 0.053200\n",
      "step 5076, loss: 0.053180\n",
      "step 5077, loss: 0.053160\n",
      "step 5078, loss: 0.053140\n",
      "step 5079, loss: 0.053120\n",
      "step 5080, loss: 0.053100\n",
      "step 5081, loss: 0.053080\n",
      "step 5082, loss: 0.053060\n",
      "step 5083, loss: 0.053040\n",
      "step 5084, loss: 0.053020\n",
      "step 5085, loss: 0.053001\n",
      "step 5086, loss: 0.052981\n",
      "step 5087, loss: 0.052961\n",
      "step 5088, loss: 0.052941\n",
      "step 5089, loss: 0.052922\n",
      "step 5090, loss: 0.052902\n",
      "step 5091, loss: 0.052882\n",
      "step 5092, loss: 0.052863\n",
      "step 5093, loss: 0.052843\n",
      "step 5094, loss: 0.052823\n",
      "step 5095, loss: 0.052804\n",
      "step 5096, loss: 0.052784\n",
      "step 5097, loss: 0.052765\n",
      "step 5098, loss: 0.052746\n",
      "step 5099, loss: 0.052726\n",
      "step 5100, loss: 0.052707\n",
      "step 5101, loss: 0.052687\n",
      "step 5102, loss: 0.052668\n",
      "step 5103, loss: 0.052649\n",
      "step 5104, loss: 0.052630\n",
      "step 5105, loss: 0.052610\n",
      "step 5106, loss: 0.052591\n",
      "step 5107, loss: 0.052572\n",
      "step 5108, loss: 0.052553\n",
      "step 5109, loss: 0.052534\n",
      "step 5110, loss: 0.052514\n",
      "step 5111, loss: 0.052495\n",
      "step 5112, loss: 0.052476\n",
      "step 5113, loss: 0.052457\n",
      "step 5114, loss: 0.052438\n",
      "step 5115, loss: 0.052419\n",
      "step 5116, loss: 0.052400\n",
      "step 5117, loss: 0.052382\n",
      "step 5118, loss: 0.052363\n",
      "step 5119, loss: 0.052344\n",
      "step 5120, loss: 0.052325\n",
      "step 5121, loss: 0.052306\n",
      "step 5122, loss: 0.052287\n",
      "step 5123, loss: 0.052269\n",
      "step 5124, loss: 0.052250\n",
      "step 5125, loss: 0.052231\n",
      "step 5126, loss: 0.052212\n",
      "step 5127, loss: 0.052194\n",
      "step 5128, loss: 0.052175\n",
      "step 5129, loss: 0.052156\n",
      "step 5130, loss: 0.052138\n",
      "step 5131, loss: 0.052119\n",
      "step 5132, loss: 0.052101\n",
      "step 5133, loss: 0.052082\n",
      "step 5134, loss: 0.052064\n",
      "step 5135, loss: 0.052045\n",
      "step 5136, loss: 0.052027\n",
      "step 5137, loss: 0.052009\n",
      "step 5138, loss: 0.051990\n",
      "step 5139, loss: 0.051972\n",
      "step 5140, loss: 0.051953\n",
      "step 5141, loss: 0.051935\n",
      "step 5142, loss: 0.051917\n",
      "step 5143, loss: 0.051898\n",
      "step 5144, loss: 0.051880\n",
      "step 5145, loss: 0.051862\n",
      "step 5146, loss: 0.051844\n",
      "step 5147, loss: 0.051826\n",
      "step 5148, loss: 0.051807\n",
      "step 5149, loss: 0.051789\n",
      "step 5150, loss: 0.051771\n",
      "step 5151, loss: 0.051753\n",
      "step 5152, loss: 0.051735\n",
      "step 5153, loss: 0.051717\n",
      "step 5154, loss: 0.051699\n",
      "step 5155, loss: 0.051681\n",
      "step 5156, loss: 0.051663\n",
      "step 5157, loss: 0.051645\n",
      "step 5158, loss: 0.051627\n",
      "step 5159, loss: 0.051609\n",
      "step 5160, loss: 0.051592\n",
      "step 5161, loss: 0.051574\n",
      "step 5162, loss: 0.051556\n",
      "step 5163, loss: 0.051538\n",
      "step 5164, loss: 0.051520\n",
      "step 5165, loss: 0.051502\n",
      "step 5166, loss: 0.051485\n",
      "step 5167, loss: 0.051467\n",
      "step 5168, loss: 0.051449\n",
      "step 5169, loss: 0.051432\n",
      "step 5170, loss: 0.051414\n",
      "step 5171, loss: 0.051396\n",
      "step 5172, loss: 0.051379\n",
      "step 5173, loss: 0.051361\n",
      "step 5174, loss: 0.051344\n",
      "step 5175, loss: 0.051326\n",
      "step 5176, loss: 0.051308\n",
      "step 5177, loss: 0.051291\n",
      "step 5178, loss: 0.051273\n",
      "step 5179, loss: 0.051256\n",
      "step 5180, loss: 0.051238\n",
      "step 5181, loss: 0.051221\n",
      "step 5182, loss: 0.051204\n",
      "step 5183, loss: 0.051186\n",
      "step 5184, loss: 0.051169\n",
      "step 5185, loss: 0.051152\n",
      "step 5186, loss: 0.051134\n",
      "step 5187, loss: 0.051117\n",
      "step 5188, loss: 0.051100\n",
      "step 5189, loss: 0.051082\n",
      "step 5190, loss: 0.051065\n",
      "step 5191, loss: 0.051048\n",
      "step 5192, loss: 0.051031\n",
      "step 5193, loss: 0.051013\n",
      "step 5194, loss: 0.050996\n",
      "step 5195, loss: 0.050979\n",
      "step 5196, loss: 0.050962\n",
      "step 5197, loss: 0.050945\n",
      "step 5198, loss: 0.050928\n",
      "step 5199, loss: 0.050911\n",
      "step 5200, loss: 0.050894\n",
      "step 5201, loss: 0.050877\n",
      "step 5202, loss: 0.050860\n",
      "step 5203, loss: 0.050843\n",
      "step 5204, loss: 0.050826\n",
      "step 5205, loss: 0.050809\n",
      "step 5206, loss: 0.050792\n",
      "step 5207, loss: 0.050775\n",
      "step 5208, loss: 0.050758\n",
      "step 5209, loss: 0.050741\n",
      "step 5210, loss: 0.050724\n",
      "step 5211, loss: 0.050707\n",
      "step 5212, loss: 0.050690\n",
      "step 5213, loss: 0.050674\n",
      "step 5214, loss: 0.050657\n",
      "step 5215, loss: 0.050640\n",
      "step 5216, loss: 0.050623\n",
      "step 5217, loss: 0.050606\n",
      "step 5218, loss: 0.050590\n",
      "step 5219, loss: 0.050573\n",
      "step 5220, loss: 0.050556\n",
      "step 5221, loss: 0.050540\n",
      "step 5222, loss: 0.050523\n",
      "step 5223, loss: 0.050506\n",
      "step 5224, loss: 0.050490\n",
      "step 5225, loss: 0.050473\n",
      "step 5226, loss: 0.050457\n",
      "step 5227, loss: 0.050440\n",
      "step 5228, loss: 0.050423\n",
      "step 5229, loss: 0.050407\n",
      "step 5230, loss: 0.050390\n",
      "step 5231, loss: 0.050374\n",
      "step 5232, loss: 0.050357\n",
      "step 5233, loss: 0.050341\n",
      "step 5234, loss: 0.050324\n",
      "step 5235, loss: 0.050308\n",
      "step 5236, loss: 0.050292\n",
      "step 5237, loss: 0.050275\n",
      "step 5238, loss: 0.050259\n",
      "step 5239, loss: 0.050242\n",
      "step 5240, loss: 0.050226\n",
      "step 5241, loss: 0.050210\n",
      "step 5242, loss: 0.050193\n",
      "step 5243, loss: 0.050177\n",
      "step 5244, loss: 0.050161\n",
      "step 5245, loss: 0.050144\n",
      "step 5246, loss: 0.050128\n",
      "step 5247, loss: 0.050112\n",
      "step 5248, loss: 0.050096\n",
      "step 5249, loss: 0.050079\n",
      "step 5250, loss: 0.050063\n",
      "step 5251, loss: 0.050047\n",
      "step 5252, loss: 0.050031\n",
      "step 5253, loss: 0.050015\n",
      "step 5254, loss: 0.049998\n",
      "step 5255, loss: 0.049982\n",
      "step 5256, loss: 0.049966\n",
      "step 5257, loss: 0.049950\n",
      "step 5258, loss: 0.049934\n",
      "step 5259, loss: 0.049918\n",
      "step 5260, loss: 0.049902\n",
      "step 5261, loss: 0.049886\n",
      "step 5262, loss: 0.049870\n",
      "step 5263, loss: 0.049854\n",
      "step 5264, loss: 0.049838\n",
      "step 5265, loss: 0.049822\n",
      "step 5266, loss: 0.049806\n",
      "step 5267, loss: 0.049790\n",
      "step 5268, loss: 0.049774\n",
      "step 5269, loss: 0.049758\n",
      "step 5270, loss: 0.049742\n",
      "step 5271, loss: 0.049726\n",
      "step 5272, loss: 0.049710\n",
      "step 5273, loss: 0.049694\n",
      "step 5274, loss: 0.049679\n",
      "step 5275, loss: 0.049663\n",
      "step 5276, loss: 0.049647\n",
      "step 5277, loss: 0.049631\n",
      "step 5278, loss: 0.049615\n",
      "step 5279, loss: 0.049599\n",
      "step 5280, loss: 0.049584\n",
      "step 5281, loss: 0.049568\n",
      "step 5282, loss: 0.049552\n",
      "step 5283, loss: 0.049536\n",
      "step 5284, loss: 0.049521\n",
      "step 5285, loss: 0.049505\n",
      "step 5286, loss: 0.049489\n",
      "step 5287, loss: 0.049474\n",
      "step 5288, loss: 0.049458\n",
      "step 5289, loss: 0.049442\n",
      "step 5290, loss: 0.049427\n",
      "step 5291, loss: 0.049411\n",
      "step 5292, loss: 0.049395\n",
      "step 5293, loss: 0.049380\n",
      "step 5294, loss: 0.049364\n",
      "step 5295, loss: 0.049348\n",
      "step 5296, loss: 0.049333\n",
      "step 5297, loss: 0.049317\n",
      "step 5298, loss: 0.049302\n",
      "step 5299, loss: 0.049286\n",
      "step 5300, loss: 0.049271\n",
      "step 5301, loss: 0.049255\n",
      "step 5302, loss: 0.049240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5303, loss: 0.049224\n",
      "step 5304, loss: 0.049209\n",
      "step 5305, loss: 0.049193\n",
      "step 5306, loss: 0.049178\n",
      "step 5307, loss: 0.049162\n",
      "step 5308, loss: 0.049147\n",
      "step 5309, loss: 0.049131\n",
      "step 5310, loss: 0.049116\n",
      "step 5311, loss: 0.049101\n",
      "step 5312, loss: 0.049085\n",
      "step 5313, loss: 0.049070\n",
      "step 5314, loss: 0.049054\n",
      "step 5315, loss: 0.049039\n",
      "step 5316, loss: 0.049024\n",
      "step 5317, loss: 0.049008\n",
      "step 5318, loss: 0.048993\n",
      "step 5319, loss: 0.048978\n",
      "step 5320, loss: 0.048963\n",
      "step 5321, loss: 0.048947\n",
      "step 5322, loss: 0.048932\n",
      "step 5323, loss: 0.048917\n",
      "step 5324, loss: 0.048901\n",
      "step 5325, loss: 0.048886\n",
      "step 5326, loss: 0.048871\n",
      "step 5327, loss: 0.048856\n",
      "step 5328, loss: 0.048841\n",
      "step 5329, loss: 0.048825\n",
      "step 5330, loss: 0.048810\n",
      "step 5331, loss: 0.048795\n",
      "step 5332, loss: 0.048780\n",
      "step 5333, loss: 0.048765\n",
      "step 5334, loss: 0.048750\n",
      "step 5335, loss: 0.048734\n",
      "step 5336, loss: 0.048719\n",
      "step 5337, loss: 0.048704\n",
      "step 5338, loss: 0.048689\n",
      "step 5339, loss: 0.048674\n",
      "step 5340, loss: 0.048659\n",
      "step 5341, loss: 0.048644\n",
      "step 5342, loss: 0.048629\n",
      "step 5343, loss: 0.048614\n",
      "step 5344, loss: 0.048599\n",
      "step 5345, loss: 0.048584\n",
      "step 5346, loss: 0.048569\n",
      "step 5347, loss: 0.048553\n",
      "step 5348, loss: 0.048538\n",
      "step 5349, loss: 0.048523\n",
      "step 5350, loss: 0.048508\n",
      "step 5351, loss: 0.048493\n",
      "step 5352, loss: 0.048478\n",
      "step 5353, loss: 0.048464\n",
      "step 5354, loss: 0.048449\n",
      "step 5355, loss: 0.048434\n",
      "step 5356, loss: 0.048419\n",
      "step 5357, loss: 0.048404\n",
      "step 5358, loss: 0.048389\n",
      "step 5359, loss: 0.048374\n",
      "step 5360, loss: 0.048359\n",
      "step 5361, loss: 0.048344\n",
      "step 5362, loss: 0.048329\n",
      "step 5363, loss: 0.048314\n",
      "step 5364, loss: 0.048300\n",
      "step 5365, loss: 0.048285\n",
      "step 5366, loss: 0.048270\n",
      "step 5367, loss: 0.048255\n",
      "step 5368, loss: 0.048240\n",
      "step 5369, loss: 0.048225\n",
      "step 5370, loss: 0.048211\n",
      "step 5371, loss: 0.048196\n",
      "step 5372, loss: 0.048181\n",
      "step 5373, loss: 0.048166\n",
      "step 5374, loss: 0.048151\n",
      "step 5375, loss: 0.048137\n",
      "step 5376, loss: 0.048122\n",
      "step 5377, loss: 0.048107\n",
      "step 5378, loss: 0.048092\n",
      "step 5379, loss: 0.048078\n",
      "step 5380, loss: 0.048063\n",
      "step 5381, loss: 0.048048\n",
      "step 5382, loss: 0.048034\n",
      "step 5383, loss: 0.048019\n",
      "step 5384, loss: 0.048004\n",
      "step 5385, loss: 0.047989\n",
      "step 5386, loss: 0.047975\n",
      "step 5387, loss: 0.047960\n",
      "step 5388, loss: 0.047945\n",
      "step 5389, loss: 0.047931\n",
      "step 5390, loss: 0.047916\n",
      "step 5391, loss: 0.047901\n",
      "step 5392, loss: 0.047887\n",
      "step 5393, loss: 0.047872\n",
      "step 5394, loss: 0.047858\n",
      "step 5395, loss: 0.047843\n",
      "step 5396, loss: 0.047828\n",
      "step 5397, loss: 0.047814\n",
      "step 5398, loss: 0.047799\n",
      "step 5399, loss: 0.047785\n",
      "step 5400, loss: 0.047770\n",
      "step 5401, loss: 0.047756\n",
      "step 5402, loss: 0.047741\n",
      "step 5403, loss: 0.047726\n",
      "step 5404, loss: 0.047712\n",
      "step 5405, loss: 0.047697\n",
      "step 5406, loss: 0.047683\n",
      "step 5407, loss: 0.047668\n",
      "step 5408, loss: 0.047654\n",
      "step 5409, loss: 0.047639\n",
      "step 5410, loss: 0.047625\n",
      "step 5411, loss: 0.047610\n",
      "step 5412, loss: 0.047596\n",
      "step 5413, loss: 0.047581\n",
      "step 5414, loss: 0.047567\n",
      "step 5415, loss: 0.047553\n",
      "step 5416, loss: 0.047538\n",
      "step 5417, loss: 0.047524\n",
      "step 5418, loss: 0.047509\n",
      "step 5419, loss: 0.047495\n",
      "step 5420, loss: 0.047480\n",
      "step 5421, loss: 0.047466\n",
      "step 5422, loss: 0.047451\n",
      "step 5423, loss: 0.047437\n",
      "step 5424, loss: 0.047423\n",
      "step 5425, loss: 0.047408\n",
      "step 5426, loss: 0.047394\n",
      "step 5427, loss: 0.047379\n",
      "step 5428, loss: 0.047365\n",
      "step 5429, loss: 0.047351\n",
      "step 5430, loss: 0.047336\n",
      "step 5431, loss: 0.047322\n",
      "step 5432, loss: 0.047308\n",
      "step 5433, loss: 0.047293\n",
      "step 5434, loss: 0.047279\n",
      "step 5435, loss: 0.047265\n",
      "step 5436, loss: 0.047250\n",
      "step 5437, loss: 0.047236\n",
      "step 5438, loss: 0.047222\n",
      "step 5439, loss: 0.047207\n",
      "step 5440, loss: 0.047193\n",
      "step 5441, loss: 0.047179\n",
      "step 5442, loss: 0.047164\n",
      "step 5443, loss: 0.047150\n",
      "step 5444, loss: 0.047136\n",
      "step 5445, loss: 0.047122\n",
      "step 5446, loss: 0.047107\n",
      "step 5447, loss: 0.047093\n",
      "step 5448, loss: 0.047079\n",
      "step 5449, loss: 0.047065\n",
      "step 5450, loss: 0.047050\n",
      "step 5451, loss: 0.047036\n",
      "step 5452, loss: 0.047022\n",
      "step 5453, loss: 0.047008\n",
      "step 5454, loss: 0.046993\n",
      "step 5455, loss: 0.046979\n",
      "step 5456, loss: 0.046965\n",
      "step 5457, loss: 0.046951\n",
      "step 5458, loss: 0.046937\n",
      "step 5459, loss: 0.046922\n",
      "step 5460, loss: 0.046908\n",
      "step 5461, loss: 0.046894\n",
      "step 5462, loss: 0.046880\n",
      "step 5463, loss: 0.046866\n",
      "step 5464, loss: 0.046852\n",
      "step 5465, loss: 0.046837\n",
      "step 5466, loss: 0.046823\n",
      "step 5467, loss: 0.046809\n",
      "step 5468, loss: 0.046795\n",
      "step 5469, loss: 0.046781\n",
      "step 5470, loss: 0.046767\n",
      "step 5471, loss: 0.046753\n",
      "step 5472, loss: 0.046738\n",
      "step 5473, loss: 0.046724\n",
      "step 5474, loss: 0.046710\n",
      "step 5475, loss: 0.046696\n",
      "step 5476, loss: 0.046682\n",
      "step 5477, loss: 0.046668\n",
      "step 5478, loss: 0.046654\n",
      "step 5479, loss: 0.046640\n",
      "step 5480, loss: 0.046626\n",
      "step 5481, loss: 0.046612\n",
      "step 5482, loss: 0.046597\n",
      "step 5483, loss: 0.046583\n",
      "step 5484, loss: 0.046569\n",
      "step 5485, loss: 0.046555\n",
      "step 5486, loss: 0.046541\n",
      "step 5487, loss: 0.046527\n",
      "step 5488, loss: 0.046513\n",
      "step 5489, loss: 0.046499\n",
      "step 5490, loss: 0.046485\n",
      "step 5491, loss: 0.046471\n",
      "step 5492, loss: 0.046457\n",
      "step 5493, loss: 0.046443\n",
      "step 5494, loss: 0.046429\n",
      "step 5495, loss: 0.046415\n",
      "step 5496, loss: 0.046401\n",
      "step 5497, loss: 0.046387\n",
      "step 5498, loss: 0.046373\n",
      "step 5499, loss: 0.046359\n",
      "step 5500, loss: 0.046345\n",
      "step 5501, loss: 0.046331\n",
      "step 5502, loss: 0.046317\n",
      "step 5503, loss: 0.046303\n",
      "step 5504, loss: 0.046289\n",
      "step 5505, loss: 0.046275\n",
      "step 5506, loss: 0.046261\n",
      "step 5507, loss: 0.046247\n",
      "step 5508, loss: 0.046233\n",
      "step 5509, loss: 0.046219\n",
      "step 5510, loss: 0.046205\n",
      "step 5511, loss: 0.046191\n",
      "step 5512, loss: 0.046177\n",
      "step 5513, loss: 0.046163\n",
      "step 5514, loss: 0.046149\n",
      "step 5515, loss: 0.046135\n",
      "step 5516, loss: 0.046121\n",
      "step 5517, loss: 0.046108\n",
      "step 5518, loss: 0.046094\n",
      "step 5519, loss: 0.046080\n",
      "step 5520, loss: 0.046066\n",
      "step 5521, loss: 0.046052\n",
      "step 5522, loss: 0.046038\n",
      "step 5523, loss: 0.046024\n",
      "step 5524, loss: 0.046010\n",
      "step 5525, loss: 0.045996\n",
      "step 5526, loss: 0.045982\n",
      "step 5527, loss: 0.045969\n",
      "step 5528, loss: 0.045955\n",
      "step 5529, loss: 0.045941\n",
      "step 5530, loss: 0.045927\n",
      "step 5531, loss: 0.045913\n",
      "step 5532, loss: 0.045899\n",
      "step 5533, loss: 0.045885\n",
      "step 5534, loss: 0.045871\n",
      "step 5535, loss: 0.045858\n",
      "step 5536, loss: 0.045844\n",
      "step 5537, loss: 0.045830\n",
      "step 5538, loss: 0.045816\n",
      "step 5539, loss: 0.045802\n",
      "step 5540, loss: 0.045788\n",
      "step 5541, loss: 0.045774\n",
      "step 5542, loss: 0.045761\n",
      "step 5543, loss: 0.045747\n",
      "step 5544, loss: 0.045733\n",
      "step 5545, loss: 0.045719\n",
      "step 5546, loss: 0.045705\n",
      "step 5547, loss: 0.045691\n",
      "step 5548, loss: 0.045678\n",
      "step 5549, loss: 0.045664\n",
      "step 5550, loss: 0.045650\n",
      "step 5551, loss: 0.045636\n",
      "step 5552, loss: 0.045622\n",
      "step 5553, loss: 0.045609\n",
      "step 5554, loss: 0.045595\n",
      "step 5555, loss: 0.045581\n",
      "step 5556, loss: 0.045567\n",
      "step 5557, loss: 0.045553\n",
      "step 5558, loss: 0.045540\n",
      "step 5559, loss: 0.045526\n",
      "step 5560, loss: 0.045512\n",
      "step 5561, loss: 0.045498\n",
      "step 5562, loss: 0.045485\n",
      "step 5563, loss: 0.045471\n",
      "step 5564, loss: 0.045457\n",
      "step 5565, loss: 0.045443\n",
      "step 5566, loss: 0.045429\n",
      "step 5567, loss: 0.045416\n",
      "step 5568, loss: 0.045402\n",
      "step 5569, loss: 0.045388\n",
      "step 5570, loss: 0.045374\n",
      "step 5571, loss: 0.045361\n",
      "step 5572, loss: 0.045347\n",
      "step 5573, loss: 0.045333\n",
      "step 5574, loss: 0.045319\n",
      "step 5575, loss: 0.045306\n",
      "step 5576, loss: 0.045292\n",
      "step 5577, loss: 0.045278\n",
      "step 5578, loss: 0.045264\n",
      "step 5579, loss: 0.045251\n",
      "step 5580, loss: 0.045237\n",
      "step 5581, loss: 0.045223\n",
      "step 5582, loss: 0.045210\n",
      "step 5583, loss: 0.045196\n",
      "step 5584, loss: 0.045182\n",
      "step 5585, loss: 0.045168\n",
      "step 5586, loss: 0.045155\n",
      "step 5587, loss: 0.045141\n",
      "step 5588, loss: 0.045127\n",
      "step 5589, loss: 0.045114\n",
      "step 5590, loss: 0.045100\n",
      "step 5591, loss: 0.045086\n",
      "step 5592, loss: 0.045073\n",
      "step 5593, loss: 0.045059\n",
      "step 5594, loss: 0.045045\n",
      "step 5595, loss: 0.045032\n",
      "step 5596, loss: 0.045018\n",
      "step 5597, loss: 0.045004\n",
      "step 5598, loss: 0.044990\n",
      "step 5599, loss: 0.044977\n",
      "step 5600, loss: 0.044963\n",
      "step 5601, loss: 0.044949\n",
      "step 5602, loss: 0.044936\n",
      "step 5603, loss: 0.044922\n",
      "step 5604, loss: 0.044908\n",
      "step 5605, loss: 0.044895\n",
      "step 5606, loss: 0.044881\n",
      "step 5607, loss: 0.044867\n",
      "step 5608, loss: 0.044854\n",
      "step 5609, loss: 0.044840\n",
      "step 5610, loss: 0.044826\n",
      "step 5611, loss: 0.044813\n",
      "step 5612, loss: 0.044799\n",
      "step 5613, loss: 0.044785\n",
      "step 5614, loss: 0.044772\n",
      "step 5615, loss: 0.044758\n",
      "step 5616, loss: 0.044744\n",
      "step 5617, loss: 0.044731\n",
      "step 5618, loss: 0.044717\n",
      "step 5619, loss: 0.044703\n",
      "step 5620, loss: 0.044690\n",
      "step 5621, loss: 0.044676\n",
      "step 5622, loss: 0.044663\n",
      "step 5623, loss: 0.044649\n",
      "step 5624, loss: 0.044635\n",
      "step 5625, loss: 0.044622\n",
      "step 5626, loss: 0.044608\n",
      "step 5627, loss: 0.044594\n",
      "step 5628, loss: 0.044581\n",
      "step 5629, loss: 0.044567\n",
      "step 5630, loss: 0.044554\n",
      "step 5631, loss: 0.044540\n",
      "step 5632, loss: 0.044526\n",
      "step 5633, loss: 0.044513\n",
      "step 5634, loss: 0.044499\n",
      "step 5635, loss: 0.044486\n",
      "step 5636, loss: 0.044472\n",
      "step 5637, loss: 0.044458\n",
      "step 5638, loss: 0.044445\n",
      "step 5639, loss: 0.044431\n",
      "step 5640, loss: 0.044418\n",
      "step 5641, loss: 0.044404\n",
      "step 5642, loss: 0.044390\n",
      "step 5643, loss: 0.044377\n",
      "step 5644, loss: 0.044363\n",
      "step 5645, loss: 0.044350\n",
      "step 5646, loss: 0.044336\n",
      "step 5647, loss: 0.044323\n",
      "step 5648, loss: 0.044309\n",
      "step 5649, loss: 0.044295\n",
      "step 5650, loss: 0.044282\n",
      "step 5651, loss: 0.044268\n",
      "step 5652, loss: 0.044255\n",
      "step 5653, loss: 0.044241\n",
      "step 5654, loss: 0.044227\n",
      "step 5655, loss: 0.044214\n",
      "step 5656, loss: 0.044200\n",
      "step 5657, loss: 0.044187\n",
      "step 5658, loss: 0.044173\n",
      "step 5659, loss: 0.044160\n",
      "step 5660, loss: 0.044146\n",
      "step 5661, loss: 0.044132\n",
      "step 5662, loss: 0.044119\n",
      "step 5663, loss: 0.044105\n",
      "step 5664, loss: 0.044092\n",
      "step 5665, loss: 0.044078\n",
      "step 5666, loss: 0.044065\n",
      "step 5667, loss: 0.044051\n",
      "step 5668, loss: 0.044037\n",
      "step 5669, loss: 0.044024\n",
      "step 5670, loss: 0.044010\n",
      "step 5671, loss: 0.043997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5672, loss: 0.043983\n",
      "step 5673, loss: 0.043970\n",
      "step 5674, loss: 0.043956\n",
      "step 5675, loss: 0.043942\n",
      "step 5676, loss: 0.043929\n",
      "step 5677, loss: 0.043915\n",
      "step 5678, loss: 0.043902\n",
      "step 5679, loss: 0.043888\n",
      "step 5680, loss: 0.043875\n",
      "step 5681, loss: 0.043861\n",
      "step 5682, loss: 0.043848\n",
      "step 5683, loss: 0.043834\n",
      "step 5684, loss: 0.043820\n",
      "step 5685, loss: 0.043807\n",
      "step 5686, loss: 0.043793\n",
      "step 5687, loss: 0.043780\n",
      "step 5688, loss: 0.043766\n",
      "step 5689, loss: 0.043753\n",
      "step 5690, loss: 0.043739\n",
      "step 5691, loss: 0.043726\n",
      "step 5692, loss: 0.043712\n",
      "step 5693, loss: 0.043699\n",
      "step 5694, loss: 0.043685\n",
      "step 5695, loss: 0.043672\n",
      "step 5696, loss: 0.043658\n",
      "step 5697, loss: 0.043645\n",
      "step 5698, loss: 0.043631\n",
      "step 5699, loss: 0.043618\n",
      "step 5700, loss: 0.043604\n",
      "step 5701, loss: 0.043591\n",
      "step 5702, loss: 0.043577\n",
      "step 5703, loss: 0.043564\n",
      "step 5704, loss: 0.043550\n",
      "step 5705, loss: 0.043537\n",
      "step 5706, loss: 0.043523\n",
      "step 5707, loss: 0.043510\n",
      "step 5708, loss: 0.043496\n",
      "step 5709, loss: 0.043483\n",
      "step 5710, loss: 0.043469\n",
      "step 5711, loss: 0.043456\n",
      "step 5712, loss: 0.043442\n",
      "step 5713, loss: 0.043429\n",
      "step 5714, loss: 0.043415\n",
      "step 5715, loss: 0.043402\n",
      "step 5716, loss: 0.043388\n",
      "step 5717, loss: 0.043375\n",
      "step 5718, loss: 0.043361\n",
      "step 5719, loss: 0.043348\n",
      "step 5720, loss: 0.043334\n",
      "step 5721, loss: 0.043321\n",
      "step 5722, loss: 0.043307\n",
      "step 5723, loss: 0.043294\n",
      "step 5724, loss: 0.043280\n",
      "step 5725, loss: 0.043267\n",
      "step 5726, loss: 0.043253\n",
      "step 5727, loss: 0.043240\n",
      "step 5728, loss: 0.043226\n",
      "step 5729, loss: 0.043213\n",
      "step 5730, loss: 0.043199\n",
      "step 5731, loss: 0.043186\n",
      "step 5732, loss: 0.043172\n",
      "step 5733, loss: 0.043159\n",
      "step 5734, loss: 0.043145\n",
      "step 5735, loss: 0.043132\n",
      "step 5736, loss: 0.043119\n",
      "step 5737, loss: 0.043105\n",
      "step 5738, loss: 0.043092\n",
      "step 5739, loss: 0.043078\n",
      "step 5740, loss: 0.043065\n",
      "step 5741, loss: 0.043051\n",
      "step 5742, loss: 0.043038\n",
      "step 5743, loss: 0.043024\n",
      "step 5744, loss: 0.043011\n",
      "step 5745, loss: 0.042997\n",
      "step 5746, loss: 0.042984\n",
      "step 5747, loss: 0.042970\n",
      "step 5748, loss: 0.042957\n",
      "step 5749, loss: 0.042943\n",
      "step 5750, loss: 0.042930\n",
      "step 5751, loss: 0.042916\n",
      "step 5752, loss: 0.042903\n",
      "step 5753, loss: 0.042890\n",
      "step 5754, loss: 0.042876\n",
      "step 5755, loss: 0.042863\n",
      "step 5756, loss: 0.042849\n",
      "step 5757, loss: 0.042836\n",
      "step 5758, loss: 0.042822\n",
      "step 5759, loss: 0.042809\n",
      "step 5760, loss: 0.042795\n",
      "step 5761, loss: 0.042782\n",
      "step 5762, loss: 0.042768\n",
      "step 5763, loss: 0.042755\n",
      "step 5764, loss: 0.042742\n",
      "step 5765, loss: 0.042728\n",
      "step 5766, loss: 0.042715\n",
      "step 5767, loss: 0.042701\n",
      "step 5768, loss: 0.042688\n",
      "step 5769, loss: 0.042674\n",
      "step 5770, loss: 0.042661\n",
      "step 5771, loss: 0.042647\n",
      "step 5772, loss: 0.042634\n",
      "step 5773, loss: 0.042620\n",
      "step 5774, loss: 0.042607\n",
      "step 5775, loss: 0.042594\n",
      "step 5776, loss: 0.042580\n",
      "step 5777, loss: 0.042567\n",
      "step 5778, loss: 0.042553\n",
      "step 5779, loss: 0.042540\n",
      "step 5780, loss: 0.042526\n",
      "step 5781, loss: 0.042513\n",
      "step 5782, loss: 0.042499\n",
      "step 5783, loss: 0.042486\n",
      "step 5784, loss: 0.042473\n",
      "step 5785, loss: 0.042459\n",
      "step 5786, loss: 0.042446\n",
      "step 5787, loss: 0.042432\n",
      "step 5788, loss: 0.042419\n",
      "step 5789, loss: 0.042405\n",
      "step 5790, loss: 0.042392\n",
      "step 5791, loss: 0.042378\n",
      "step 5792, loss: 0.042365\n",
      "step 5793, loss: 0.042351\n",
      "step 5794, loss: 0.042338\n",
      "step 5795, loss: 0.042325\n",
      "step 5796, loss: 0.042311\n",
      "step 5797, loss: 0.042298\n",
      "step 5798, loss: 0.042284\n",
      "step 5799, loss: 0.042271\n",
      "step 5800, loss: 0.042257\n",
      "step 5801, loss: 0.042244\n",
      "step 5802, loss: 0.042230\n",
      "step 5803, loss: 0.042217\n",
      "step 5804, loss: 0.042203\n",
      "step 5805, loss: 0.042190\n",
      "step 5806, loss: 0.042176\n",
      "step 5807, loss: 0.042163\n",
      "step 5808, loss: 0.042150\n",
      "step 5809, loss: 0.042136\n",
      "step 5810, loss: 0.042123\n",
      "step 5811, loss: 0.042109\n",
      "step 5812, loss: 0.042096\n",
      "step 5813, loss: 0.042082\n",
      "step 5814, loss: 0.042069\n",
      "step 5815, loss: 0.042055\n",
      "step 5816, loss: 0.042042\n",
      "step 5817, loss: 0.042028\n",
      "step 5818, loss: 0.042015\n",
      "step 5819, loss: 0.042001\n",
      "step 5820, loss: 0.041988\n",
      "step 5821, loss: 0.041975\n",
      "step 5822, loss: 0.041961\n",
      "step 5823, loss: 0.041948\n",
      "step 5824, loss: 0.041934\n",
      "step 5825, loss: 0.041921\n",
      "step 5826, loss: 0.041907\n",
      "step 5827, loss: 0.041894\n",
      "step 5828, loss: 0.041880\n",
      "step 5829, loss: 0.041867\n",
      "step 5830, loss: 0.041854\n",
      "step 5831, loss: 0.041840\n",
      "step 5832, loss: 0.041827\n",
      "step 5833, loss: 0.041813\n",
      "step 5834, loss: 0.041800\n",
      "step 5835, loss: 0.041786\n",
      "step 5836, loss: 0.041773\n",
      "step 5837, loss: 0.041759\n",
      "step 5838, loss: 0.041746\n",
      "step 5839, loss: 0.041733\n",
      "step 5840, loss: 0.041719\n",
      "step 5841, loss: 0.041706\n",
      "step 5842, loss: 0.041692\n",
      "step 5843, loss: 0.041679\n",
      "step 5844, loss: 0.041665\n",
      "step 5845, loss: 0.041652\n",
      "step 5846, loss: 0.041639\n",
      "step 5847, loss: 0.041625\n",
      "step 5848, loss: 0.041612\n",
      "step 5849, loss: 0.041598\n",
      "step 5850, loss: 0.041585\n",
      "step 5851, loss: 0.041571\n",
      "step 5852, loss: 0.041558\n",
      "step 5853, loss: 0.041545\n",
      "step 5854, loss: 0.041531\n",
      "step 5855, loss: 0.041518\n",
      "step 5856, loss: 0.041504\n",
      "step 5857, loss: 0.041491\n",
      "step 5858, loss: 0.041477\n",
      "step 5859, loss: 0.041464\n",
      "step 5860, loss: 0.041451\n",
      "step 5861, loss: 0.041437\n",
      "step 5862, loss: 0.041424\n",
      "step 5863, loss: 0.041410\n",
      "step 5864, loss: 0.041397\n",
      "step 5865, loss: 0.041383\n",
      "step 5866, loss: 0.041370\n",
      "step 5867, loss: 0.041357\n",
      "step 5868, loss: 0.041343\n",
      "step 5869, loss: 0.041330\n",
      "step 5870, loss: 0.041316\n",
      "step 5871, loss: 0.041303\n",
      "step 5872, loss: 0.041290\n",
      "step 5873, loss: 0.041276\n",
      "step 5874, loss: 0.041263\n",
      "step 5875, loss: 0.041249\n",
      "step 5876, loss: 0.041236\n",
      "step 5877, loss: 0.041222\n",
      "step 5878, loss: 0.041209\n",
      "step 5879, loss: 0.041195\n",
      "step 5880, loss: 0.041182\n",
      "step 5881, loss: 0.041169\n",
      "step 5882, loss: 0.041155\n",
      "step 5883, loss: 0.041142\n",
      "step 5884, loss: 0.041128\n",
      "step 5885, loss: 0.041115\n",
      "step 5886, loss: 0.041101\n",
      "step 5887, loss: 0.041088\n",
      "step 5888, loss: 0.041074\n",
      "step 5889, loss: 0.041061\n",
      "step 5890, loss: 0.041048\n",
      "step 5891, loss: 0.041034\n",
      "step 5892, loss: 0.041021\n",
      "step 5893, loss: 0.041007\n",
      "step 5894, loss: 0.040994\n",
      "step 5895, loss: 0.040980\n",
      "step 5896, loss: 0.040967\n",
      "step 5897, loss: 0.040954\n",
      "step 5898, loss: 0.040940\n",
      "step 5899, loss: 0.040927\n",
      "step 5900, loss: 0.040913\n",
      "step 5901, loss: 0.040900\n",
      "step 5902, loss: 0.040887\n",
      "step 5903, loss: 0.040873\n",
      "step 5904, loss: 0.040860\n",
      "step 5905, loss: 0.040846\n",
      "step 5906, loss: 0.040833\n",
      "step 5907, loss: 0.040819\n",
      "step 5908, loss: 0.040806\n",
      "step 5909, loss: 0.040793\n",
      "step 5910, loss: 0.040779\n",
      "step 5911, loss: 0.040766\n",
      "step 5912, loss: 0.040752\n",
      "step 5913, loss: 0.040739\n",
      "step 5914, loss: 0.040726\n",
      "step 5915, loss: 0.040712\n",
      "step 5916, loss: 0.040699\n",
      "step 5917, loss: 0.040685\n",
      "step 5918, loss: 0.040672\n",
      "step 5919, loss: 0.040658\n",
      "step 5920, loss: 0.040645\n",
      "step 5921, loss: 0.040632\n",
      "step 5922, loss: 0.040618\n",
      "step 5923, loss: 0.040605\n",
      "step 5924, loss: 0.040591\n",
      "step 5925, loss: 0.040578\n",
      "step 5926, loss: 0.040564\n",
      "step 5927, loss: 0.040551\n",
      "step 5928, loss: 0.040538\n",
      "step 5929, loss: 0.040524\n",
      "step 5930, loss: 0.040511\n",
      "step 5931, loss: 0.040497\n",
      "step 5932, loss: 0.040484\n",
      "step 5933, loss: 0.040470\n",
      "step 5934, loss: 0.040457\n",
      "step 5935, loss: 0.040444\n",
      "step 5936, loss: 0.040430\n",
      "step 5937, loss: 0.040417\n",
      "step 5938, loss: 0.040403\n",
      "step 5939, loss: 0.040390\n",
      "step 5940, loss: 0.040376\n",
      "step 5941, loss: 0.040363\n",
      "step 5942, loss: 0.040350\n",
      "step 5943, loss: 0.040336\n",
      "step 5944, loss: 0.040323\n",
      "step 5945, loss: 0.040309\n",
      "step 5946, loss: 0.040296\n",
      "step 5947, loss: 0.040283\n",
      "step 5948, loss: 0.040269\n",
      "step 5949, loss: 0.040256\n",
      "step 5950, loss: 0.040242\n",
      "step 5951, loss: 0.040229\n",
      "step 5952, loss: 0.040215\n",
      "step 5953, loss: 0.040202\n",
      "step 5954, loss: 0.040189\n",
      "step 5955, loss: 0.040175\n",
      "step 5956, loss: 0.040162\n",
      "step 5957, loss: 0.040148\n",
      "step 5958, loss: 0.040135\n",
      "step 5959, loss: 0.040122\n",
      "step 5960, loss: 0.040108\n",
      "step 5961, loss: 0.040095\n",
      "step 5962, loss: 0.040081\n",
      "step 5963, loss: 0.040068\n",
      "step 5964, loss: 0.040054\n",
      "step 5965, loss: 0.040041\n",
      "step 5966, loss: 0.040028\n",
      "step 5967, loss: 0.040014\n",
      "step 5968, loss: 0.040001\n",
      "step 5969, loss: 0.039987\n",
      "step 5970, loss: 0.039974\n",
      "step 5971, loss: 0.039960\n",
      "step 5972, loss: 0.039947\n",
      "step 5973, loss: 0.039934\n",
      "step 5974, loss: 0.039920\n",
      "step 5975, loss: 0.039907\n",
      "step 5976, loss: 0.039893\n",
      "step 5977, loss: 0.039880\n",
      "step 5978, loss: 0.039867\n",
      "step 5979, loss: 0.039853\n",
      "step 5980, loss: 0.039840\n",
      "step 5981, loss: 0.039826\n",
      "step 5982, loss: 0.039813\n",
      "step 5983, loss: 0.039800\n",
      "step 5984, loss: 0.039786\n",
      "step 5985, loss: 0.039773\n",
      "step 5986, loss: 0.039759\n",
      "step 5987, loss: 0.039746\n",
      "step 5988, loss: 0.039732\n",
      "step 5989, loss: 0.039719\n",
      "step 5990, loss: 0.039706\n",
      "step 5991, loss: 0.039692\n",
      "step 5992, loss: 0.039679\n",
      "step 5993, loss: 0.039665\n",
      "step 5994, loss: 0.039652\n",
      "step 5995, loss: 0.039639\n",
      "step 5996, loss: 0.039625\n",
      "step 5997, loss: 0.039612\n",
      "step 5998, loss: 0.039598\n",
      "step 5999, loss: 0.039585\n",
      "step 6000, loss: 0.039571\n",
      "step 6001, loss: 0.039558\n",
      "step 6002, loss: 0.039545\n",
      "step 6003, loss: 0.039531\n",
      "step 6004, loss: 0.039518\n",
      "step 6005, loss: 0.039504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6006, loss: 0.039491\n",
      "step 6007, loss: 0.039478\n",
      "step 6008, loss: 0.039464\n",
      "step 6009, loss: 0.039451\n",
      "step 6010, loss: 0.039437\n",
      "step 6011, loss: 0.039424\n",
      "step 6012, loss: 0.039410\n",
      "step 6013, loss: 0.039397\n",
      "step 6014, loss: 0.039384\n",
      "step 6015, loss: 0.039370\n",
      "step 6016, loss: 0.039357\n",
      "step 6017, loss: 0.039343\n",
      "step 6018, loss: 0.039330\n",
      "step 6019, loss: 0.039317\n",
      "step 6020, loss: 0.039303\n",
      "step 6021, loss: 0.039290\n",
      "step 6022, loss: 0.039276\n",
      "step 6023, loss: 0.039263\n",
      "step 6024, loss: 0.039250\n",
      "step 6025, loss: 0.039236\n",
      "step 6026, loss: 0.039223\n",
      "step 6027, loss: 0.039209\n",
      "step 6028, loss: 0.039196\n",
      "step 6029, loss: 0.039182\n",
      "step 6030, loss: 0.039169\n",
      "step 6031, loss: 0.039156\n",
      "step 6032, loss: 0.039142\n",
      "step 6033, loss: 0.039129\n",
      "step 6034, loss: 0.039115\n",
      "step 6035, loss: 0.039102\n",
      "step 6036, loss: 0.039089\n",
      "step 6037, loss: 0.039075\n",
      "step 6038, loss: 0.039062\n",
      "step 6039, loss: 0.039048\n",
      "step 6040, loss: 0.039035\n",
      "step 6041, loss: 0.039022\n",
      "step 6042, loss: 0.039008\n",
      "step 6043, loss: 0.038995\n",
      "step 6044, loss: 0.038981\n",
      "step 6045, loss: 0.038968\n",
      "step 6046, loss: 0.038955\n",
      "step 6047, loss: 0.038941\n",
      "step 6048, loss: 0.038928\n",
      "step 6049, loss: 0.038914\n",
      "step 6050, loss: 0.038901\n",
      "step 6051, loss: 0.038888\n",
      "step 6052, loss: 0.038874\n",
      "step 6053, loss: 0.038861\n",
      "step 6054, loss: 0.038847\n",
      "step 6055, loss: 0.038834\n",
      "step 6056, loss: 0.038821\n",
      "step 6057, loss: 0.038807\n",
      "step 6058, loss: 0.038794\n",
      "step 6059, loss: 0.038780\n",
      "step 6060, loss: 0.038767\n",
      "step 6061, loss: 0.038753\n",
      "step 6062, loss: 0.038740\n",
      "step 6063, loss: 0.038727\n",
      "step 6064, loss: 0.038713\n",
      "step 6065, loss: 0.038700\n",
      "step 6066, loss: 0.038686\n",
      "step 6067, loss: 0.038673\n",
      "step 6068, loss: 0.038660\n",
      "step 6069, loss: 0.038646\n",
      "step 6070, loss: 0.038633\n",
      "step 6071, loss: 0.038619\n",
      "step 6072, loss: 0.038606\n",
      "step 6073, loss: 0.038593\n",
      "step 6074, loss: 0.038579\n",
      "step 6075, loss: 0.038566\n",
      "step 6076, loss: 0.038553\n",
      "step 6077, loss: 0.038539\n",
      "step 6078, loss: 0.038526\n",
      "step 6079, loss: 0.038512\n",
      "step 6080, loss: 0.038499\n",
      "step 6081, loss: 0.038485\n",
      "step 6082, loss: 0.038472\n",
      "step 6083, loss: 0.038459\n",
      "step 6084, loss: 0.038445\n",
      "step 6085, loss: 0.038432\n",
      "step 6086, loss: 0.038418\n",
      "step 6087, loss: 0.038405\n",
      "step 6088, loss: 0.038392\n",
      "step 6089, loss: 0.038378\n",
      "step 6090, loss: 0.038365\n",
      "step 6091, loss: 0.038351\n",
      "step 6092, loss: 0.038338\n",
      "step 6093, loss: 0.038325\n",
      "step 6094, loss: 0.038311\n",
      "step 6095, loss: 0.038298\n",
      "step 6096, loss: 0.038285\n",
      "step 6097, loss: 0.038271\n",
      "step 6098, loss: 0.038258\n",
      "step 6099, loss: 0.038244\n",
      "step 6100, loss: 0.038231\n",
      "step 6101, loss: 0.038218\n",
      "step 6102, loss: 0.038204\n",
      "step 6103, loss: 0.038191\n",
      "step 6104, loss: 0.038177\n",
      "step 6105, loss: 0.038164\n",
      "step 6106, loss: 0.038151\n",
      "step 6107, loss: 0.038137\n",
      "step 6108, loss: 0.038124\n",
      "step 6109, loss: 0.038110\n",
      "step 6110, loss: 0.038097\n",
      "step 6111, loss: 0.038084\n",
      "step 6112, loss: 0.038070\n",
      "step 6113, loss: 0.038057\n",
      "step 6114, loss: 0.038043\n",
      "step 6115, loss: 0.038030\n",
      "step 6116, loss: 0.038017\n",
      "step 6117, loss: 0.038003\n",
      "step 6118, loss: 0.037990\n",
      "step 6119, loss: 0.037976\n",
      "step 6120, loss: 0.037963\n",
      "step 6121, loss: 0.037950\n",
      "step 6122, loss: 0.037936\n",
      "step 6123, loss: 0.037923\n",
      "step 6124, loss: 0.037910\n",
      "step 6125, loss: 0.037896\n",
      "step 6126, loss: 0.037883\n",
      "step 6127, loss: 0.037869\n",
      "step 6128, loss: 0.037856\n",
      "step 6129, loss: 0.037843\n",
      "step 6130, loss: 0.037829\n",
      "step 6131, loss: 0.037816\n",
      "step 6132, loss: 0.037802\n",
      "step 6133, loss: 0.037789\n",
      "step 6134, loss: 0.037776\n",
      "step 6135, loss: 0.037762\n",
      "step 6136, loss: 0.037749\n",
      "step 6137, loss: 0.037736\n",
      "step 6138, loss: 0.037722\n",
      "step 6139, loss: 0.037709\n",
      "step 6140, loss: 0.037695\n",
      "step 6141, loss: 0.037682\n",
      "step 6142, loss: 0.037669\n",
      "step 6143, loss: 0.037655\n",
      "step 6144, loss: 0.037642\n",
      "step 6145, loss: 0.037629\n",
      "step 6146, loss: 0.037615\n",
      "step 6147, loss: 0.037602\n",
      "step 6148, loss: 0.037588\n",
      "step 6149, loss: 0.037575\n",
      "step 6150, loss: 0.037562\n",
      "step 6151, loss: 0.037548\n",
      "step 6152, loss: 0.037535\n",
      "step 6153, loss: 0.037522\n",
      "step 6154, loss: 0.037508\n",
      "step 6155, loss: 0.037495\n",
      "step 6156, loss: 0.037481\n",
      "step 6157, loss: 0.037468\n",
      "step 6158, loss: 0.037455\n",
      "step 6159, loss: 0.037441\n",
      "step 6160, loss: 0.037428\n",
      "step 6161, loss: 0.037414\n",
      "step 6162, loss: 0.037401\n",
      "step 6163, loss: 0.037388\n",
      "step 6164, loss: 0.037374\n",
      "step 6165, loss: 0.037361\n",
      "step 6166, loss: 0.037348\n",
      "step 6167, loss: 0.037334\n",
      "step 6168, loss: 0.037321\n",
      "step 6169, loss: 0.037308\n",
      "step 6170, loss: 0.037294\n",
      "step 6171, loss: 0.037281\n",
      "step 6172, loss: 0.037268\n",
      "step 6173, loss: 0.037254\n",
      "step 6174, loss: 0.037241\n",
      "step 6175, loss: 0.037227\n",
      "step 6176, loss: 0.037214\n",
      "step 6177, loss: 0.037201\n",
      "step 6178, loss: 0.037187\n",
      "step 6179, loss: 0.037174\n",
      "step 6180, loss: 0.037160\n",
      "step 6181, loss: 0.037147\n",
      "step 6182, loss: 0.037134\n",
      "step 6183, loss: 0.037120\n",
      "step 6184, loss: 0.037107\n",
      "step 6185, loss: 0.037094\n",
      "step 6186, loss: 0.037080\n",
      "step 6187, loss: 0.037067\n",
      "step 6188, loss: 0.037054\n",
      "step 6189, loss: 0.037040\n",
      "step 6190, loss: 0.037027\n",
      "step 6191, loss: 0.037014\n",
      "step 6192, loss: 0.037000\n",
      "step 6193, loss: 0.036987\n",
      "step 6194, loss: 0.036973\n",
      "step 6195, loss: 0.036960\n",
      "step 6196, loss: 0.036947\n",
      "step 6197, loss: 0.036933\n",
      "step 6198, loss: 0.036920\n",
      "step 6199, loss: 0.036907\n",
      "step 6200, loss: 0.036893\n",
      "step 6201, loss: 0.036880\n",
      "step 6202, loss: 0.036867\n",
      "step 6203, loss: 0.036853\n",
      "step 6204, loss: 0.036840\n",
      "step 6205, loss: 0.036827\n",
      "step 6206, loss: 0.036813\n",
      "step 6207, loss: 0.036800\n",
      "step 6208, loss: 0.036786\n",
      "step 6209, loss: 0.036773\n",
      "step 6210, loss: 0.036760\n",
      "step 6211, loss: 0.036746\n",
      "step 6212, loss: 0.036733\n",
      "step 6213, loss: 0.036720\n",
      "step 6214, loss: 0.036706\n",
      "step 6215, loss: 0.036693\n",
      "step 6216, loss: 0.036680\n",
      "step 6217, loss: 0.036666\n",
      "step 6218, loss: 0.036653\n",
      "step 6219, loss: 0.036640\n",
      "step 6220, loss: 0.036626\n",
      "step 6221, loss: 0.036613\n",
      "step 6222, loss: 0.036600\n",
      "step 6223, loss: 0.036586\n",
      "step 6224, loss: 0.036573\n",
      "step 6225, loss: 0.036560\n",
      "step 6226, loss: 0.036546\n",
      "step 6227, loss: 0.036533\n",
      "step 6228, loss: 0.036520\n",
      "step 6229, loss: 0.036506\n",
      "step 6230, loss: 0.036493\n",
      "step 6231, loss: 0.036480\n",
      "step 6232, loss: 0.036466\n",
      "step 6233, loss: 0.036453\n",
      "step 6234, loss: 0.036440\n",
      "step 6235, loss: 0.036426\n",
      "step 6236, loss: 0.036413\n",
      "step 6237, loss: 0.036400\n",
      "step 6238, loss: 0.036386\n",
      "step 6239, loss: 0.036373\n",
      "step 6240, loss: 0.036360\n",
      "step 6241, loss: 0.036346\n",
      "step 6242, loss: 0.036333\n",
      "step 6243, loss: 0.036320\n",
      "step 6244, loss: 0.036306\n",
      "step 6245, loss: 0.036293\n",
      "step 6246, loss: 0.036280\n",
      "step 6247, loss: 0.036266\n",
      "step 6248, loss: 0.036253\n",
      "step 6249, loss: 0.036240\n",
      "step 6250, loss: 0.036226\n",
      "step 6251, loss: 0.036213\n",
      "step 6252, loss: 0.036200\n",
      "step 6253, loss: 0.036186\n",
      "step 6254, loss: 0.036173\n",
      "step 6255, loss: 0.036160\n",
      "step 6256, loss: 0.036146\n",
      "step 6257, loss: 0.036133\n",
      "step 6258, loss: 0.036120\n",
      "step 6259, loss: 0.036106\n",
      "step 6260, loss: 0.036093\n",
      "step 6261, loss: 0.036080\n",
      "step 6262, loss: 0.036066\n",
      "step 6263, loss: 0.036053\n",
      "step 6264, loss: 0.036040\n",
      "step 6265, loss: 0.036027\n",
      "step 6266, loss: 0.036013\n",
      "step 6267, loss: 0.036000\n",
      "step 6268, loss: 0.035987\n",
      "step 6269, loss: 0.035973\n",
      "step 6270, loss: 0.035960\n",
      "step 6271, loss: 0.035947\n",
      "step 6272, loss: 0.035933\n",
      "step 6273, loss: 0.035920\n",
      "step 6274, loss: 0.035907\n",
      "step 6275, loss: 0.035893\n",
      "step 6276, loss: 0.035880\n",
      "step 6277, loss: 0.035867\n",
      "step 6278, loss: 0.035854\n",
      "step 6279, loss: 0.035840\n",
      "step 6280, loss: 0.035827\n",
      "step 6281, loss: 0.035814\n",
      "step 6282, loss: 0.035800\n",
      "step 6283, loss: 0.035787\n",
      "step 6284, loss: 0.035774\n",
      "step 6285, loss: 0.035760\n",
      "step 6286, loss: 0.035747\n",
      "step 6287, loss: 0.035734\n",
      "step 6288, loss: 0.035720\n",
      "step 6289, loss: 0.035707\n",
      "step 6290, loss: 0.035694\n",
      "step 6291, loss: 0.035681\n",
      "step 6292, loss: 0.035667\n",
      "step 6293, loss: 0.035654\n",
      "step 6294, loss: 0.035641\n",
      "step 6295, loss: 0.035627\n",
      "step 6296, loss: 0.035614\n",
      "step 6297, loss: 0.035601\n",
      "step 6298, loss: 0.035588\n",
      "step 6299, loss: 0.035574\n",
      "step 6300, loss: 0.035561\n",
      "step 6301, loss: 0.035548\n",
      "step 6302, loss: 0.035534\n",
      "step 6303, loss: 0.035521\n",
      "step 6304, loss: 0.035508\n",
      "step 6305, loss: 0.035495\n",
      "step 6306, loss: 0.035481\n",
      "step 6307, loss: 0.035468\n",
      "step 6308, loss: 0.035455\n",
      "step 6309, loss: 0.035441\n",
      "step 6310, loss: 0.035428\n",
      "step 6311, loss: 0.035415\n",
      "step 6312, loss: 0.035402\n",
      "step 6313, loss: 0.035388\n",
      "step 6314, loss: 0.035375\n",
      "step 6315, loss: 0.035362\n",
      "step 6316, loss: 0.035349\n",
      "step 6317, loss: 0.035335\n",
      "step 6318, loss: 0.035322\n",
      "step 6319, loss: 0.035309\n",
      "step 6320, loss: 0.035295\n",
      "step 6321, loss: 0.035282\n",
      "step 6322, loss: 0.035269\n",
      "step 6323, loss: 0.035256\n",
      "step 6324, loss: 0.035242\n",
      "step 6325, loss: 0.035229\n",
      "step 6326, loss: 0.035216\n",
      "step 6327, loss: 0.035203\n",
      "step 6328, loss: 0.035189\n",
      "step 6329, loss: 0.035176\n",
      "step 6330, loss: 0.035163\n",
      "step 6331, loss: 0.035149\n",
      "step 6332, loss: 0.035136\n",
      "step 6333, loss: 0.035123\n",
      "step 6334, loss: 0.035110\n",
      "step 6335, loss: 0.035096\n",
      "step 6336, loss: 0.035083\n",
      "step 6337, loss: 0.035070\n",
      "step 6338, loss: 0.035057\n",
      "step 6339, loss: 0.035044\n",
      "step 6340, loss: 0.035030\n",
      "step 6341, loss: 0.035017\n",
      "step 6342, loss: 0.035004\n",
      "step 6343, loss: 0.034990\n",
      "step 6344, loss: 0.034977\n",
      "step 6345, loss: 0.034964\n",
      "step 6346, loss: 0.034951\n",
      "step 6347, loss: 0.034937\n",
      "step 6348, loss: 0.034924\n",
      "step 6349, loss: 0.034911\n",
      "step 6350, loss: 0.034898\n",
      "step 6351, loss: 0.034884\n",
      "step 6352, loss: 0.034871\n",
      "step 6353, loss: 0.034858\n",
      "step 6354, loss: 0.034845\n",
      "step 6355, loss: 0.034832\n",
      "step 6356, loss: 0.034818\n",
      "step 6357, loss: 0.034805\n",
      "step 6358, loss: 0.034792\n",
      "step 6359, loss: 0.034779\n",
      "step 6360, loss: 0.034765\n",
      "step 6361, loss: 0.034752\n",
      "step 6362, loss: 0.034739\n",
      "step 6363, loss: 0.034726\n",
      "step 6364, loss: 0.034712\n",
      "step 6365, loss: 0.034699\n",
      "step 6366, loss: 0.034686\n",
      "step 6367, loss: 0.034673\n",
      "step 6368, loss: 0.034660\n",
      "step 6369, loss: 0.034646\n",
      "step 6370, loss: 0.034633\n",
      "step 6371, loss: 0.034620\n",
      "step 6372, loss: 0.034607\n",
      "step 6373, loss: 0.034593\n",
      "step 6374, loss: 0.034580\n",
      "step 6375, loss: 0.034567\n",
      "step 6376, loss: 0.034554\n",
      "step 6377, loss: 0.034541\n",
      "step 6378, loss: 0.034527\n",
      "step 6379, loss: 0.034514\n",
      "step 6380, loss: 0.034501\n",
      "step 6381, loss: 0.034488\n",
      "step 6382, loss: 0.034475\n",
      "step 6383, loss: 0.034461\n",
      "step 6384, loss: 0.034448\n",
      "step 6385, loss: 0.034435\n",
      "step 6386, loss: 0.034422\n",
      "step 6387, loss: 0.034409\n",
      "step 6388, loss: 0.034395\n",
      "step 6389, loss: 0.034382\n",
      "step 6390, loss: 0.034369\n",
      "step 6391, loss: 0.034356\n",
      "step 6392, loss: 0.034343\n",
      "step 6393, loss: 0.034329\n",
      "step 6394, loss: 0.034316\n",
      "step 6395, loss: 0.034303\n",
      "step 6396, loss: 0.034290\n",
      "step 6397, loss: 0.034277\n",
      "step 6398, loss: 0.034263\n",
      "step 6399, loss: 0.034250\n",
      "step 6400, loss: 0.034237\n",
      "step 6401, loss: 0.034224\n",
      "step 6402, loss: 0.034211\n",
      "step 6403, loss: 0.034197\n",
      "step 6404, loss: 0.034184\n",
      "step 6405, loss: 0.034171\n",
      "step 6406, loss: 0.034158\n",
      "step 6407, loss: 0.034145\n",
      "step 6408, loss: 0.034132\n",
      "step 6409, loss: 0.034118\n",
      "step 6410, loss: 0.034105\n",
      "step 6411, loss: 0.034092\n",
      "step 6412, loss: 0.034079\n",
      "step 6413, loss: 0.034066\n",
      "step 6414, loss: 0.034053\n",
      "step 6415, loss: 0.034039\n",
      "step 6416, loss: 0.034026\n",
      "step 6417, loss: 0.034013\n",
      "step 6418, loss: 0.034000\n",
      "step 6419, loss: 0.033987\n",
      "step 6420, loss: 0.033974\n",
      "step 6421, loss: 0.033960\n",
      "step 6422, loss: 0.033947\n",
      "step 6423, loss: 0.033934\n",
      "step 6424, loss: 0.033921\n",
      "step 6425, loss: 0.033908\n",
      "step 6426, loss: 0.033895\n",
      "step 6427, loss: 0.033881\n",
      "step 6428, loss: 0.033868\n",
      "step 6429, loss: 0.033855\n",
      "step 6430, loss: 0.033842\n",
      "step 6431, loss: 0.033829\n",
      "step 6432, loss: 0.033816\n",
      "step 6433, loss: 0.033803\n",
      "step 6434, loss: 0.033789\n",
      "step 6435, loss: 0.033776\n",
      "step 6436, loss: 0.033763\n",
      "step 6437, loss: 0.033750\n",
      "step 6438, loss: 0.033737\n",
      "step 6439, loss: 0.033724\n",
      "step 6440, loss: 0.033711\n",
      "step 6441, loss: 0.033697\n",
      "step 6442, loss: 0.033684\n",
      "step 6443, loss: 0.033671\n",
      "step 6444, loss: 0.033658\n",
      "step 6445, loss: 0.033645\n",
      "step 6446, loss: 0.033632\n",
      "step 6447, loss: 0.033619\n",
      "step 6448, loss: 0.033606\n",
      "step 6449, loss: 0.033592\n",
      "step 6450, loss: 0.033579\n",
      "step 6451, loss: 0.033566\n",
      "step 6452, loss: 0.033553\n",
      "step 6453, loss: 0.033540\n",
      "step 6454, loss: 0.033527\n",
      "step 6455, loss: 0.033514\n",
      "step 6456, loss: 0.033501\n",
      "step 6457, loss: 0.033488\n",
      "step 6458, loss: 0.033474\n",
      "step 6459, loss: 0.033461\n",
      "step 6460, loss: 0.033448\n",
      "step 6461, loss: 0.033435\n",
      "step 6462, loss: 0.033422\n",
      "step 6463, loss: 0.033409\n",
      "step 6464, loss: 0.033396\n",
      "step 6465, loss: 0.033383\n",
      "step 6466, loss: 0.033369\n",
      "step 6467, loss: 0.033356\n",
      "step 6468, loss: 0.033343\n",
      "step 6469, loss: 0.033330\n",
      "step 6470, loss: 0.033317\n",
      "step 6471, loss: 0.033304\n",
      "step 6472, loss: 0.033291\n",
      "step 6473, loss: 0.033278\n",
      "step 6474, loss: 0.033265\n",
      "step 6475, loss: 0.033252\n",
      "step 6476, loss: 0.033239\n",
      "step 6477, loss: 0.033226\n",
      "step 6478, loss: 0.033212\n",
      "step 6479, loss: 0.033199\n",
      "step 6480, loss: 0.033186\n",
      "step 6481, loss: 0.033173\n",
      "step 6482, loss: 0.033160\n",
      "step 6483, loss: 0.033147\n",
      "step 6484, loss: 0.033134\n",
      "step 6485, loss: 0.033121\n",
      "step 6486, loss: 0.033108\n",
      "step 6487, loss: 0.033095\n",
      "step 6488, loss: 0.033082\n",
      "step 6489, loss: 0.033069\n",
      "step 6490, loss: 0.033055\n",
      "step 6491, loss: 0.033042\n",
      "step 6492, loss: 0.033029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6493, loss: 0.033016\n",
      "step 6494, loss: 0.033003\n",
      "step 6495, loss: 0.032990\n",
      "step 6496, loss: 0.032977\n",
      "step 6497, loss: 0.032964\n",
      "step 6498, loss: 0.032951\n",
      "step 6499, loss: 0.032938\n",
      "step 6500, loss: 0.032925\n",
      "step 6501, loss: 0.032912\n",
      "step 6502, loss: 0.032899\n",
      "step 6503, loss: 0.032886\n",
      "step 6504, loss: 0.032873\n",
      "step 6505, loss: 0.032860\n",
      "step 6506, loss: 0.032847\n",
      "step 6507, loss: 0.032834\n",
      "step 6508, loss: 0.032821\n",
      "step 6509, loss: 0.032808\n",
      "step 6510, loss: 0.032794\n",
      "step 6511, loss: 0.032781\n",
      "step 6512, loss: 0.032768\n",
      "step 6513, loss: 0.032755\n",
      "step 6514, loss: 0.032742\n",
      "step 6515, loss: 0.032729\n",
      "step 6516, loss: 0.032716\n",
      "step 6517, loss: 0.032703\n",
      "step 6518, loss: 0.032690\n",
      "step 6519, loss: 0.032677\n",
      "step 6520, loss: 0.032664\n",
      "step 6521, loss: 0.032651\n",
      "step 6522, loss: 0.032638\n",
      "step 6523, loss: 0.032625\n",
      "step 6524, loss: 0.032612\n",
      "step 6525, loss: 0.032599\n",
      "step 6526, loss: 0.032586\n",
      "step 6527, loss: 0.032573\n",
      "step 6528, loss: 0.032560\n",
      "step 6529, loss: 0.032547\n",
      "step 6530, loss: 0.032534\n",
      "step 6531, loss: 0.032521\n",
      "step 6532, loss: 0.032508\n",
      "step 6533, loss: 0.032495\n",
      "step 6534, loss: 0.032482\n",
      "step 6535, loss: 0.032469\n",
      "step 6536, loss: 0.032456\n",
      "step 6537, loss: 0.032443\n",
      "step 6538, loss: 0.032430\n",
      "step 6539, loss: 0.032417\n",
      "step 6540, loss: 0.032404\n",
      "step 6541, loss: 0.032391\n",
      "step 6542, loss: 0.032378\n",
      "step 6543, loss: 0.032365\n",
      "step 6544, loss: 0.032352\n",
      "step 6545, loss: 0.032339\n",
      "step 6546, loss: 0.032326\n",
      "step 6547, loss: 0.032313\n",
      "step 6548, loss: 0.032300\n",
      "step 6549, loss: 0.032287\n",
      "step 6550, loss: 0.032274\n",
      "step 6551, loss: 0.032261\n",
      "step 6552, loss: 0.032248\n",
      "step 6553, loss: 0.032236\n",
      "step 6554, loss: 0.032223\n",
      "step 6555, loss: 0.032210\n",
      "step 6556, loss: 0.032197\n",
      "step 6557, loss: 0.032184\n",
      "step 6558, loss: 0.032171\n",
      "step 6559, loss: 0.032158\n",
      "step 6560, loss: 0.032145\n",
      "step 6561, loss: 0.032132\n",
      "step 6562, loss: 0.032119\n",
      "step 6563, loss: 0.032106\n",
      "step 6564, loss: 0.032093\n",
      "step 6565, loss: 0.032080\n",
      "step 6566, loss: 0.032067\n",
      "step 6567, loss: 0.032054\n",
      "step 6568, loss: 0.032041\n",
      "step 6569, loss: 0.032028\n",
      "step 6570, loss: 0.032015\n",
      "step 6571, loss: 0.032002\n",
      "step 6572, loss: 0.031990\n",
      "step 6573, loss: 0.031977\n",
      "step 6574, loss: 0.031964\n",
      "step 6575, loss: 0.031951\n",
      "step 6576, loss: 0.031938\n",
      "step 6577, loss: 0.031925\n",
      "step 6578, loss: 0.031912\n",
      "step 6579, loss: 0.031899\n",
      "step 6580, loss: 0.031886\n",
      "step 6581, loss: 0.031873\n",
      "step 6582, loss: 0.031860\n",
      "step 6583, loss: 0.031847\n",
      "step 6584, loss: 0.031835\n",
      "step 6585, loss: 0.031822\n",
      "step 6586, loss: 0.031809\n",
      "step 6587, loss: 0.031796\n",
      "step 6588, loss: 0.031783\n",
      "step 6589, loss: 0.031770\n",
      "step 6590, loss: 0.031757\n",
      "step 6591, loss: 0.031744\n",
      "step 6592, loss: 0.031731\n",
      "step 6593, loss: 0.031718\n",
      "step 6594, loss: 0.031706\n",
      "step 6595, loss: 0.031693\n",
      "step 6596, loss: 0.031680\n",
      "step 6597, loss: 0.031667\n",
      "step 6598, loss: 0.031654\n",
      "step 6599, loss: 0.031641\n",
      "step 6600, loss: 0.031628\n",
      "step 6601, loss: 0.031615\n",
      "step 6602, loss: 0.031602\n",
      "step 6603, loss: 0.031590\n",
      "step 6604, loss: 0.031577\n",
      "step 6605, loss: 0.031564\n",
      "step 6606, loss: 0.031551\n",
      "step 6607, loss: 0.031538\n",
      "step 6608, loss: 0.031525\n",
      "step 6609, loss: 0.031512\n",
      "step 6610, loss: 0.031500\n",
      "step 6611, loss: 0.031487\n",
      "step 6612, loss: 0.031474\n",
      "step 6613, loss: 0.031461\n",
      "step 6614, loss: 0.031448\n",
      "step 6615, loss: 0.031435\n",
      "step 6616, loss: 0.031422\n",
      "step 6617, loss: 0.031410\n",
      "step 6618, loss: 0.031397\n",
      "step 6619, loss: 0.031384\n",
      "step 6620, loss: 0.031371\n",
      "step 6621, loss: 0.031358\n",
      "step 6622, loss: 0.031345\n",
      "step 6623, loss: 0.031333\n",
      "step 6624, loss: 0.031320\n",
      "step 6625, loss: 0.031307\n",
      "step 6626, loss: 0.031294\n",
      "step 6627, loss: 0.031281\n",
      "step 6628, loss: 0.031268\n",
      "step 6629, loss: 0.031256\n",
      "step 6630, loss: 0.031243\n",
      "step 6631, loss: 0.031230\n",
      "step 6632, loss: 0.031217\n",
      "step 6633, loss: 0.031204\n",
      "step 6634, loss: 0.031192\n",
      "step 6635, loss: 0.031179\n",
      "step 6636, loss: 0.031166\n",
      "step 6637, loss: 0.031153\n",
      "step 6638, loss: 0.031140\n",
      "step 6639, loss: 0.031127\n",
      "step 6640, loss: 0.031115\n",
      "step 6641, loss: 0.031102\n",
      "step 6642, loss: 0.031089\n",
      "step 6643, loss: 0.031076\n",
      "step 6644, loss: 0.031063\n",
      "step 6645, loss: 0.031051\n",
      "step 6646, loss: 0.031038\n",
      "step 6647, loss: 0.031025\n",
      "step 6648, loss: 0.031012\n",
      "step 6649, loss: 0.031000\n",
      "step 6650, loss: 0.030987\n",
      "step 6651, loss: 0.030974\n",
      "step 6652, loss: 0.030961\n",
      "step 6653, loss: 0.030949\n",
      "step 6654, loss: 0.030936\n",
      "step 6655, loss: 0.030923\n",
      "step 6656, loss: 0.030910\n",
      "step 6657, loss: 0.030897\n",
      "step 6658, loss: 0.030885\n",
      "step 6659, loss: 0.030872\n",
      "step 6660, loss: 0.030859\n",
      "step 6661, loss: 0.030846\n",
      "step 6662, loss: 0.030834\n",
      "step 6663, loss: 0.030821\n",
      "step 6664, loss: 0.030808\n",
      "step 6665, loss: 0.030795\n",
      "step 6666, loss: 0.030783\n",
      "step 6667, loss: 0.030770\n",
      "step 6668, loss: 0.030757\n",
      "step 6669, loss: 0.030744\n",
      "step 6670, loss: 0.030732\n",
      "step 6671, loss: 0.030719\n",
      "step 6672, loss: 0.030706\n",
      "step 6673, loss: 0.030694\n",
      "step 6674, loss: 0.030681\n",
      "step 6675, loss: 0.030668\n",
      "step 6676, loss: 0.030655\n",
      "step 6677, loss: 0.030643\n",
      "step 6678, loss: 0.030630\n",
      "step 6679, loss: 0.030617\n",
      "step 6680, loss: 0.030604\n",
      "step 6681, loss: 0.030592\n",
      "step 6682, loss: 0.030579\n",
      "step 6683, loss: 0.030566\n",
      "step 6684, loss: 0.030554\n",
      "step 6685, loss: 0.030541\n",
      "step 6686, loss: 0.030528\n",
      "step 6687, loss: 0.030516\n",
      "step 6688, loss: 0.030503\n",
      "step 6689, loss: 0.030490\n",
      "step 6690, loss: 0.030477\n",
      "step 6691, loss: 0.030465\n",
      "step 6692, loss: 0.030452\n",
      "step 6693, loss: 0.030439\n",
      "step 6694, loss: 0.030427\n",
      "step 6695, loss: 0.030414\n",
      "step 6696, loss: 0.030401\n",
      "step 6697, loss: 0.030389\n",
      "step 6698, loss: 0.030376\n",
      "step 6699, loss: 0.030363\n",
      "step 6700, loss: 0.030351\n",
      "step 6701, loss: 0.030338\n",
      "step 6702, loss: 0.030325\n",
      "step 6703, loss: 0.030313\n",
      "step 6704, loss: 0.030300\n",
      "step 6705, loss: 0.030287\n",
      "step 6706, loss: 0.030275\n",
      "step 6707, loss: 0.030262\n",
      "step 6708, loss: 0.030249\n",
      "step 6709, loss: 0.030237\n",
      "step 6710, loss: 0.030224\n",
      "step 6711, loss: 0.030211\n",
      "step 6712, loss: 0.030199\n",
      "step 6713, loss: 0.030186\n",
      "step 6714, loss: 0.030174\n",
      "step 6715, loss: 0.030161\n",
      "step 6716, loss: 0.030148\n",
      "step 6717, loss: 0.030136\n",
      "step 6718, loss: 0.030123\n",
      "step 6719, loss: 0.030110\n",
      "step 6720, loss: 0.030098\n",
      "step 6721, loss: 0.030085\n",
      "step 6722, loss: 0.030073\n",
      "step 6723, loss: 0.030060\n",
      "step 6724, loss: 0.030047\n",
      "step 6725, loss: 0.030035\n",
      "step 6726, loss: 0.030022\n",
      "step 6727, loss: 0.030009\n",
      "step 6728, loss: 0.029997\n",
      "step 6729, loss: 0.029984\n",
      "step 6730, loss: 0.029972\n",
      "step 6731, loss: 0.029959\n",
      "step 6732, loss: 0.029947\n",
      "step 6733, loss: 0.029934\n",
      "step 6734, loss: 0.029921\n",
      "step 6735, loss: 0.029909\n",
      "step 6736, loss: 0.029896\n",
      "step 6737, loss: 0.029884\n",
      "step 6738, loss: 0.029871\n",
      "step 6739, loss: 0.029859\n",
      "step 6740, loss: 0.029846\n",
      "step 6741, loss: 0.029833\n",
      "step 6742, loss: 0.029821\n",
      "step 6743, loss: 0.029808\n",
      "step 6744, loss: 0.029796\n",
      "step 6745, loss: 0.029783\n",
      "step 6746, loss: 0.029771\n",
      "step 6747, loss: 0.029758\n",
      "step 6748, loss: 0.029745\n",
      "step 6749, loss: 0.029733\n",
      "step 6750, loss: 0.029720\n",
      "step 6751, loss: 0.029708\n",
      "step 6752, loss: 0.029695\n",
      "step 6753, loss: 0.029683\n",
      "step 6754, loss: 0.029670\n",
      "step 6755, loss: 0.029658\n",
      "step 6756, loss: 0.029645\n",
      "step 6757, loss: 0.029633\n",
      "step 6758, loss: 0.029620\n",
      "step 6759, loss: 0.029608\n",
      "step 6760, loss: 0.029595\n",
      "step 6761, loss: 0.029582\n",
      "step 6762, loss: 0.029570\n",
      "step 6763, loss: 0.029557\n",
      "step 6764, loss: 0.029545\n",
      "step 6765, loss: 0.029532\n",
      "step 6766, loss: 0.029520\n",
      "step 6767, loss: 0.029507\n",
      "step 6768, loss: 0.029495\n",
      "step 6769, loss: 0.029482\n",
      "step 6770, loss: 0.029470\n",
      "step 6771, loss: 0.029457\n",
      "step 6772, loss: 0.029445\n",
      "step 6773, loss: 0.029432\n",
      "step 6774, loss: 0.029420\n",
      "step 6775, loss: 0.029407\n",
      "step 6776, loss: 0.029395\n",
      "step 6777, loss: 0.029382\n",
      "step 6778, loss: 0.029370\n",
      "step 6779, loss: 0.029358\n",
      "step 6780, loss: 0.029345\n",
      "step 6781, loss: 0.029333\n",
      "step 6782, loss: 0.029320\n",
      "step 6783, loss: 0.029308\n",
      "step 6784, loss: 0.029295\n",
      "step 6785, loss: 0.029283\n",
      "step 6786, loss: 0.029270\n",
      "step 6787, loss: 0.029258\n",
      "step 6788, loss: 0.029245\n",
      "step 6789, loss: 0.029233\n",
      "step 6790, loss: 0.029220\n",
      "step 6791, loss: 0.029208\n",
      "step 6792, loss: 0.029196\n",
      "step 6793, loss: 0.029183\n",
      "step 6794, loss: 0.029171\n",
      "step 6795, loss: 0.029158\n",
      "step 6796, loss: 0.029146\n",
      "step 6797, loss: 0.029133\n",
      "step 6798, loss: 0.029121\n",
      "step 6799, loss: 0.029109\n",
      "step 6800, loss: 0.029096\n",
      "step 6801, loss: 0.029084\n",
      "step 6802, loss: 0.029071\n",
      "step 6803, loss: 0.029059\n",
      "step 6804, loss: 0.029047\n",
      "step 6805, loss: 0.029034\n",
      "step 6806, loss: 0.029022\n",
      "step 6807, loss: 0.029009\n",
      "step 6808, loss: 0.028997\n",
      "step 6809, loss: 0.028984\n",
      "step 6810, loss: 0.028972\n",
      "step 6811, loss: 0.028960\n",
      "step 6812, loss: 0.028947\n",
      "step 6813, loss: 0.028935\n",
      "step 6814, loss: 0.028923\n",
      "step 6815, loss: 0.028910\n",
      "step 6816, loss: 0.028898\n",
      "step 6817, loss: 0.028885\n",
      "step 6818, loss: 0.028873\n",
      "step 6819, loss: 0.028861\n",
      "step 6820, loss: 0.028848\n",
      "step 6821, loss: 0.028836\n",
      "step 6822, loss: 0.028824\n",
      "step 6823, loss: 0.028811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6824, loss: 0.028799\n",
      "step 6825, loss: 0.028787\n",
      "step 6826, loss: 0.028774\n",
      "step 6827, loss: 0.028762\n",
      "step 6828, loss: 0.028750\n",
      "step 6829, loss: 0.028737\n",
      "step 6830, loss: 0.028725\n",
      "step 6831, loss: 0.028713\n",
      "step 6832, loss: 0.028700\n",
      "step 6833, loss: 0.028688\n",
      "step 6834, loss: 0.028676\n",
      "step 6835, loss: 0.028663\n",
      "step 6836, loss: 0.028651\n",
      "step 6837, loss: 0.028639\n",
      "step 6838, loss: 0.028626\n",
      "step 6839, loss: 0.028614\n",
      "step 6840, loss: 0.028602\n",
      "step 6841, loss: 0.028589\n",
      "step 6842, loss: 0.028577\n",
      "step 6843, loss: 0.028565\n",
      "step 6844, loss: 0.028552\n",
      "step 6845, loss: 0.028540\n",
      "step 6846, loss: 0.028528\n",
      "step 6847, loss: 0.028516\n",
      "step 6848, loss: 0.028503\n",
      "step 6849, loss: 0.028491\n",
      "step 6850, loss: 0.028479\n",
      "step 6851, loss: 0.028466\n",
      "step 6852, loss: 0.028454\n",
      "step 6853, loss: 0.028442\n",
      "step 6854, loss: 0.028430\n",
      "step 6855, loss: 0.028417\n",
      "step 6856, loss: 0.028405\n",
      "step 6857, loss: 0.028393\n",
      "step 6858, loss: 0.028381\n",
      "step 6859, loss: 0.028368\n",
      "step 6860, loss: 0.028356\n",
      "step 6861, loss: 0.028344\n",
      "step 6862, loss: 0.028332\n",
      "step 6863, loss: 0.028319\n",
      "step 6864, loss: 0.028307\n",
      "step 6865, loss: 0.028295\n",
      "step 6866, loss: 0.028283\n",
      "step 6867, loss: 0.028270\n",
      "step 6868, loss: 0.028258\n",
      "step 6869, loss: 0.028246\n",
      "step 6870, loss: 0.028234\n",
      "step 6871, loss: 0.028221\n",
      "step 6872, loss: 0.028209\n",
      "step 6873, loss: 0.028197\n",
      "step 6874, loss: 0.028185\n",
      "step 6875, loss: 0.028173\n",
      "step 6876, loss: 0.028160\n",
      "step 6877, loss: 0.028148\n",
      "step 6878, loss: 0.028136\n",
      "step 6879, loss: 0.028124\n",
      "step 6880, loss: 0.028112\n",
      "step 6881, loss: 0.028099\n",
      "step 6882, loss: 0.028087\n",
      "step 6883, loss: 0.028075\n",
      "step 6884, loss: 0.028063\n",
      "step 6885, loss: 0.028051\n",
      "step 6886, loss: 0.028039\n",
      "step 6887, loss: 0.028026\n",
      "step 6888, loss: 0.028014\n",
      "step 6889, loss: 0.028002\n",
      "step 6890, loss: 0.027990\n",
      "step 6891, loss: 0.027978\n",
      "step 6892, loss: 0.027966\n",
      "step 6893, loss: 0.027953\n",
      "step 6894, loss: 0.027941\n",
      "step 6895, loss: 0.027929\n",
      "step 6896, loss: 0.027917\n",
      "step 6897, loss: 0.027905\n",
      "step 6898, loss: 0.027893\n",
      "step 6899, loss: 0.027881\n",
      "step 6900, loss: 0.027868\n",
      "step 6901, loss: 0.027856\n",
      "step 6902, loss: 0.027844\n",
      "step 6903, loss: 0.027832\n",
      "step 6904, loss: 0.027820\n",
      "step 6905, loss: 0.027808\n",
      "step 6906, loss: 0.027796\n",
      "step 6907, loss: 0.027784\n",
      "step 6908, loss: 0.027771\n",
      "step 6909, loss: 0.027759\n",
      "step 6910, loss: 0.027747\n",
      "step 6911, loss: 0.027735\n",
      "step 6912, loss: 0.027723\n",
      "step 6913, loss: 0.027711\n",
      "step 6914, loss: 0.027699\n",
      "step 6915, loss: 0.027687\n",
      "step 6916, loss: 0.027675\n",
      "step 6917, loss: 0.027663\n",
      "step 6918, loss: 0.027651\n",
      "step 6919, loss: 0.027638\n",
      "step 6920, loss: 0.027626\n",
      "step 6921, loss: 0.027614\n",
      "step 6922, loss: 0.027602\n",
      "step 6923, loss: 0.027590\n",
      "step 6924, loss: 0.027578\n",
      "step 6925, loss: 0.027566\n",
      "step 6926, loss: 0.027554\n",
      "step 6927, loss: 0.027542\n",
      "step 6928, loss: 0.027530\n",
      "step 6929, loss: 0.027518\n",
      "step 6930, loss: 0.027506\n",
      "step 6931, loss: 0.027494\n",
      "step 6932, loss: 0.027482\n",
      "step 6933, loss: 0.027470\n",
      "step 6934, loss: 0.027458\n",
      "step 6935, loss: 0.027446\n",
      "step 6936, loss: 0.027434\n",
      "step 6937, loss: 0.027422\n",
      "step 6938, loss: 0.027410\n",
      "step 6939, loss: 0.027398\n",
      "step 6940, loss: 0.027386\n",
      "step 6941, loss: 0.027374\n",
      "step 6942, loss: 0.027362\n",
      "step 6943, loss: 0.027350\n",
      "step 6944, loss: 0.027338\n",
      "step 6945, loss: 0.027326\n",
      "step 6946, loss: 0.027314\n",
      "step 6947, loss: 0.027302\n",
      "step 6948, loss: 0.027290\n",
      "step 6949, loss: 0.027278\n",
      "step 6950, loss: 0.027266\n",
      "step 6951, loss: 0.027254\n",
      "step 6952, loss: 0.027242\n",
      "step 6953, loss: 0.027230\n",
      "step 6954, loss: 0.027218\n",
      "step 6955, loss: 0.027206\n",
      "step 6956, loss: 0.027194\n",
      "step 6957, loss: 0.027182\n",
      "step 6958, loss: 0.027170\n",
      "step 6959, loss: 0.027158\n",
      "step 6960, loss: 0.027146\n",
      "step 6961, loss: 0.027134\n",
      "step 6962, loss: 0.027122\n",
      "step 6963, loss: 0.027110\n",
      "step 6964, loss: 0.027098\n",
      "step 6965, loss: 0.027087\n",
      "step 6966, loss: 0.027075\n",
      "step 6967, loss: 0.027063\n",
      "step 6968, loss: 0.027051\n",
      "step 6969, loss: 0.027039\n",
      "step 6970, loss: 0.027027\n",
      "step 6971, loss: 0.027015\n",
      "step 6972, loss: 0.027003\n",
      "step 6973, loss: 0.026991\n",
      "step 6974, loss: 0.026979\n",
      "step 6975, loss: 0.026967\n",
      "step 6976, loss: 0.026956\n",
      "step 6977, loss: 0.026944\n",
      "step 6978, loss: 0.026932\n",
      "step 6979, loss: 0.026920\n",
      "step 6980, loss: 0.026908\n",
      "step 6981, loss: 0.026896\n",
      "step 6982, loss: 0.026884\n",
      "step 6983, loss: 0.026873\n",
      "step 6984, loss: 0.026861\n",
      "step 6985, loss: 0.026849\n",
      "step 6986, loss: 0.026837\n",
      "step 6987, loss: 0.026825\n",
      "step 6988, loss: 0.026813\n",
      "step 6989, loss: 0.026801\n",
      "step 6990, loss: 0.026789\n",
      "step 6991, loss: 0.026778\n",
      "step 6992, loss: 0.026766\n",
      "step 6993, loss: 0.026754\n",
      "step 6994, loss: 0.026742\n",
      "step 6995, loss: 0.026730\n",
      "step 6996, loss: 0.026718\n",
      "step 6997, loss: 0.026707\n",
      "step 6998, loss: 0.026695\n",
      "step 6999, loss: 0.026683\n",
      "step 7000, loss: 0.026671\n",
      "step 7001, loss: 0.026659\n",
      "step 7002, loss: 0.026648\n",
      "step 7003, loss: 0.026636\n",
      "step 7004, loss: 0.026624\n",
      "step 7005, loss: 0.026612\n",
      "step 7006, loss: 0.026600\n",
      "step 7007, loss: 0.026589\n",
      "step 7008, loss: 0.026577\n",
      "step 7009, loss: 0.026565\n",
      "step 7010, loss: 0.026553\n",
      "step 7011, loss: 0.026542\n",
      "step 7012, loss: 0.026530\n",
      "step 7013, loss: 0.026518\n",
      "step 7014, loss: 0.026506\n",
      "step 7015, loss: 0.026495\n",
      "step 7016, loss: 0.026483\n",
      "step 7017, loss: 0.026471\n",
      "step 7018, loss: 0.026459\n",
      "step 7019, loss: 0.026448\n",
      "step 7020, loss: 0.026436\n",
      "step 7021, loss: 0.026424\n",
      "step 7022, loss: 0.026412\n",
      "step 7023, loss: 0.026401\n",
      "step 7024, loss: 0.026389\n",
      "step 7025, loss: 0.026377\n",
      "step 7026, loss: 0.026365\n",
      "step 7027, loss: 0.026354\n",
      "step 7028, loss: 0.026342\n",
      "step 7029, loss: 0.026330\n",
      "step 7030, loss: 0.026319\n",
      "step 7031, loss: 0.026307\n",
      "step 7032, loss: 0.026295\n",
      "step 7033, loss: 0.026284\n",
      "step 7034, loss: 0.026272\n",
      "step 7035, loss: 0.026260\n",
      "step 7036, loss: 0.026248\n",
      "step 7037, loss: 0.026237\n",
      "step 7038, loss: 0.026225\n",
      "step 7039, loss: 0.026213\n",
      "step 7040, loss: 0.026202\n",
      "step 7041, loss: 0.026190\n",
      "step 7042, loss: 0.026178\n",
      "step 7043, loss: 0.026167\n",
      "step 7044, loss: 0.026155\n",
      "step 7045, loss: 0.026143\n",
      "step 7046, loss: 0.026132\n",
      "step 7047, loss: 0.026120\n",
      "step 7048, loss: 0.026109\n",
      "step 7049, loss: 0.026097\n",
      "step 7050, loss: 0.026085\n",
      "step 7051, loss: 0.026074\n",
      "step 7052, loss: 0.026062\n",
      "step 7053, loss: 0.026050\n",
      "step 7054, loss: 0.026039\n",
      "step 7055, loss: 0.026027\n",
      "step 7056, loss: 0.026016\n",
      "step 7057, loss: 0.026004\n",
      "step 7058, loss: 0.025992\n",
      "step 7059, loss: 0.025981\n",
      "step 7060, loss: 0.025969\n",
      "step 7061, loss: 0.025958\n",
      "step 7062, loss: 0.025946\n",
      "step 7063, loss: 0.025934\n",
      "step 7064, loss: 0.025923\n",
      "step 7065, loss: 0.025911\n",
      "step 7066, loss: 0.025900\n",
      "step 7067, loss: 0.025888\n",
      "step 7068, loss: 0.025877\n",
      "step 7069, loss: 0.025865\n",
      "step 7070, loss: 0.025853\n",
      "step 7071, loss: 0.025842\n",
      "step 7072, loss: 0.025830\n",
      "step 7073, loss: 0.025819\n",
      "step 7074, loss: 0.025807\n",
      "step 7075, loss: 0.025796\n",
      "step 7076, loss: 0.025784\n",
      "step 7077, loss: 0.025773\n",
      "step 7078, loss: 0.025761\n",
      "step 7079, loss: 0.025749\n",
      "step 7080, loss: 0.025738\n",
      "step 7081, loss: 0.025726\n",
      "step 7082, loss: 0.025715\n",
      "step 7083, loss: 0.025703\n",
      "step 7084, loss: 0.025692\n",
      "step 7085, loss: 0.025680\n",
      "step 7086, loss: 0.025669\n",
      "step 7087, loss: 0.025657\n",
      "step 7088, loss: 0.025646\n",
      "step 7089, loss: 0.025634\n",
      "step 7090, loss: 0.025623\n",
      "step 7091, loss: 0.025611\n",
      "step 7092, loss: 0.025600\n",
      "step 7093, loss: 0.025588\n",
      "step 7094, loss: 0.025577\n",
      "step 7095, loss: 0.025565\n",
      "step 7096, loss: 0.025554\n",
      "step 7097, loss: 0.025543\n",
      "step 7098, loss: 0.025531\n",
      "step 7099, loss: 0.025520\n",
      "step 7100, loss: 0.025508\n",
      "step 7101, loss: 0.025497\n",
      "step 7102, loss: 0.025485\n",
      "step 7103, loss: 0.025474\n",
      "step 7104, loss: 0.025463\n",
      "step 7105, loss: 0.025451\n",
      "step 7106, loss: 0.025440\n",
      "step 7107, loss: 0.025428\n",
      "step 7108, loss: 0.025417\n",
      "step 7109, loss: 0.025405\n",
      "step 7110, loss: 0.025394\n",
      "step 7111, loss: 0.025383\n",
      "step 7112, loss: 0.025371\n",
      "step 7113, loss: 0.025360\n",
      "step 7114, loss: 0.025348\n",
      "step 7115, loss: 0.025337\n",
      "step 7116, loss: 0.025326\n",
      "step 7117, loss: 0.025314\n",
      "step 7118, loss: 0.025303\n",
      "step 7119, loss: 0.025291\n",
      "step 7120, loss: 0.025280\n",
      "step 7121, loss: 0.025269\n",
      "step 7122, loss: 0.025257\n",
      "step 7123, loss: 0.025246\n",
      "step 7124, loss: 0.025235\n",
      "step 7125, loss: 0.025223\n",
      "step 7126, loss: 0.025212\n",
      "step 7127, loss: 0.025200\n",
      "step 7128, loss: 0.025189\n",
      "step 7129, loss: 0.025178\n",
      "step 7130, loss: 0.025166\n",
      "step 7131, loss: 0.025155\n",
      "step 7132, loss: 0.025144\n",
      "step 7133, loss: 0.025133\n",
      "step 7134, loss: 0.025121\n",
      "step 7135, loss: 0.025110\n",
      "step 7136, loss: 0.025099\n",
      "step 7137, loss: 0.025087\n",
      "step 7138, loss: 0.025076\n",
      "step 7139, loss: 0.025065\n",
      "step 7140, loss: 0.025053\n",
      "step 7141, loss: 0.025042\n",
      "step 7142, loss: 0.025031\n",
      "step 7143, loss: 0.025020\n",
      "step 7144, loss: 0.025008\n",
      "step 7145, loss: 0.024997\n",
      "step 7146, loss: 0.024986\n",
      "step 7147, loss: 0.024974\n",
      "step 7148, loss: 0.024963\n",
      "step 7149, loss: 0.024952\n",
      "step 7150, loss: 0.024941\n",
      "step 7151, loss: 0.024929\n",
      "step 7152, loss: 0.024918\n",
      "step 7153, loss: 0.024907\n",
      "step 7154, loss: 0.024896\n",
      "step 7155, loss: 0.024884\n",
      "step 7156, loss: 0.024873\n",
      "step 7157, loss: 0.024862\n",
      "step 7158, loss: 0.024851\n",
      "step 7159, loss: 0.024839\n",
      "step 7160, loss: 0.024828\n",
      "step 7161, loss: 0.024817\n",
      "step 7162, loss: 0.024806\n",
      "step 7163, loss: 0.024795\n",
      "step 7164, loss: 0.024783\n",
      "step 7165, loss: 0.024772\n",
      "step 7166, loss: 0.024761\n",
      "step 7167, loss: 0.024750\n",
      "step 7168, loss: 0.024739\n",
      "step 7169, loss: 0.024727\n",
      "step 7170, loss: 0.024716\n",
      "step 7171, loss: 0.024705\n",
      "step 7172, loss: 0.024694\n",
      "step 7173, loss: 0.024683\n",
      "step 7174, loss: 0.024672\n",
      "step 7175, loss: 0.024660\n",
      "step 7176, loss: 0.024649\n",
      "step 7177, loss: 0.024638\n",
      "step 7178, loss: 0.024627\n",
      "step 7179, loss: 0.024616\n",
      "step 7180, loss: 0.024605\n",
      "step 7181, loss: 0.024594\n",
      "step 7182, loss: 0.024582\n",
      "step 7183, loss: 0.024571\n",
      "step 7184, loss: 0.024560\n",
      "step 7185, loss: 0.024549\n",
      "step 7186, loss: 0.024538\n",
      "step 7187, loss: 0.024527\n",
      "step 7188, loss: 0.024516\n",
      "step 7189, loss: 0.024505\n",
      "step 7190, loss: 0.024494\n",
      "step 7191, loss: 0.024483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7192, loss: 0.024471\n",
      "step 7193, loss: 0.024460\n",
      "step 7194, loss: 0.024449\n",
      "step 7195, loss: 0.024438\n",
      "step 7196, loss: 0.024427\n",
      "step 7197, loss: 0.024416\n",
      "step 7198, loss: 0.024405\n",
      "step 7199, loss: 0.024394\n",
      "step 7200, loss: 0.024383\n",
      "step 7201, loss: 0.024372\n",
      "step 7202, loss: 0.024361\n",
      "step 7203, loss: 0.024350\n",
      "step 7204, loss: 0.024339\n",
      "step 7205, loss: 0.024328\n",
      "step 7206, loss: 0.024317\n",
      "step 7207, loss: 0.024306\n",
      "step 7208, loss: 0.024295\n",
      "step 7209, loss: 0.024284\n",
      "step 7210, loss: 0.024272\n",
      "step 7211, loss: 0.024261\n",
      "step 7212, loss: 0.024250\n",
      "step 7213, loss: 0.024239\n",
      "step 7214, loss: 0.024228\n",
      "step 7215, loss: 0.024217\n",
      "step 7216, loss: 0.024206\n",
      "step 7217, loss: 0.024195\n",
      "step 7218, loss: 0.024184\n",
      "step 7219, loss: 0.024174\n",
      "step 7220, loss: 0.024163\n",
      "step 7221, loss: 0.024152\n",
      "step 7222, loss: 0.024141\n",
      "step 7223, loss: 0.024130\n",
      "step 7224, loss: 0.024119\n",
      "step 7225, loss: 0.024108\n",
      "step 7226, loss: 0.024097\n",
      "step 7227, loss: 0.024086\n",
      "step 7228, loss: 0.024075\n",
      "step 7229, loss: 0.024064\n",
      "step 7230, loss: 0.024053\n",
      "step 7231, loss: 0.024042\n",
      "step 7232, loss: 0.024031\n",
      "step 7233, loss: 0.024020\n",
      "step 7234, loss: 0.024009\n",
      "step 7235, loss: 0.023998\n",
      "step 7236, loss: 0.023988\n",
      "step 7237, loss: 0.023977\n",
      "step 7238, loss: 0.023966\n",
      "step 7239, loss: 0.023955\n",
      "step 7240, loss: 0.023944\n",
      "step 7241, loss: 0.023933\n",
      "step 7242, loss: 0.023922\n",
      "step 7243, loss: 0.023911\n",
      "step 7244, loss: 0.023900\n",
      "step 7245, loss: 0.023890\n",
      "step 7246, loss: 0.023879\n",
      "step 7247, loss: 0.023868\n",
      "step 7248, loss: 0.023857\n",
      "step 7249, loss: 0.023846\n",
      "step 7250, loss: 0.023835\n",
      "step 7251, loss: 0.023824\n",
      "step 7252, loss: 0.023814\n",
      "step 7253, loss: 0.023803\n",
      "step 7254, loss: 0.023792\n",
      "step 7255, loss: 0.023781\n",
      "step 7256, loss: 0.023770\n",
      "step 7257, loss: 0.023759\n",
      "step 7258, loss: 0.023749\n",
      "step 7259, loss: 0.023738\n",
      "step 7260, loss: 0.023727\n",
      "step 7261, loss: 0.023716\n",
      "step 7262, loss: 0.023705\n",
      "step 7263, loss: 0.023695\n",
      "step 7264, loss: 0.023684\n",
      "step 7265, loss: 0.023673\n",
      "step 7266, loss: 0.023662\n",
      "step 7267, loss: 0.023651\n",
      "step 7268, loss: 0.023641\n",
      "step 7269, loss: 0.023630\n",
      "step 7270, loss: 0.023619\n",
      "step 7271, loss: 0.023608\n",
      "step 7272, loss: 0.023598\n",
      "step 7273, loss: 0.023587\n",
      "step 7274, loss: 0.023576\n",
      "step 7275, loss: 0.023565\n",
      "step 7276, loss: 0.023555\n",
      "step 7277, loss: 0.023544\n",
      "step 7278, loss: 0.023533\n",
      "step 7279, loss: 0.023523\n",
      "step 7280, loss: 0.023512\n",
      "step 7281, loss: 0.023501\n",
      "step 7282, loss: 0.023490\n",
      "step 7283, loss: 0.023480\n",
      "step 7284, loss: 0.023469\n",
      "step 7285, loss: 0.023458\n",
      "step 7286, loss: 0.023448\n",
      "step 7287, loss: 0.023437\n",
      "step 7288, loss: 0.023426\n",
      "step 7289, loss: 0.023416\n",
      "step 7290, loss: 0.023405\n",
      "step 7291, loss: 0.023394\n",
      "step 7292, loss: 0.023384\n",
      "step 7293, loss: 0.023373\n",
      "step 7294, loss: 0.023362\n",
      "step 7295, loss: 0.023352\n",
      "step 7296, loss: 0.023341\n",
      "step 7297, loss: 0.023330\n",
      "step 7298, loss: 0.023320\n",
      "step 7299, loss: 0.023309\n",
      "step 7300, loss: 0.023298\n",
      "step 7301, loss: 0.023288\n",
      "step 7302, loss: 0.023277\n",
      "step 7303, loss: 0.023267\n",
      "step 7304, loss: 0.023256\n",
      "step 7305, loss: 0.023245\n",
      "step 7306, loss: 0.023235\n",
      "step 7307, loss: 0.023224\n",
      "step 7308, loss: 0.023214\n",
      "step 7309, loss: 0.023203\n",
      "step 7310, loss: 0.023192\n",
      "step 7311, loss: 0.023182\n",
      "step 7312, loss: 0.023171\n",
      "step 7313, loss: 0.023161\n",
      "step 7314, loss: 0.023150\n",
      "step 7315, loss: 0.023139\n",
      "step 7316, loss: 0.023129\n",
      "step 7317, loss: 0.023118\n",
      "step 7318, loss: 0.023108\n",
      "step 7319, loss: 0.023097\n",
      "step 7320, loss: 0.023087\n",
      "step 7321, loss: 0.023076\n",
      "step 7322, loss: 0.023066\n",
      "step 7323, loss: 0.023055\n",
      "step 7324, loss: 0.023045\n",
      "step 7325, loss: 0.023034\n",
      "step 7326, loss: 0.023024\n",
      "step 7327, loss: 0.023013\n",
      "step 7328, loss: 0.023002\n",
      "step 7329, loss: 0.022992\n",
      "step 7330, loss: 0.022981\n",
      "step 7331, loss: 0.022971\n",
      "step 7332, loss: 0.022960\n",
      "step 7333, loss: 0.022950\n",
      "step 7334, loss: 0.022939\n",
      "step 7335, loss: 0.022929\n",
      "step 7336, loss: 0.022919\n",
      "step 7337, loss: 0.022908\n",
      "step 7338, loss: 0.022898\n",
      "step 7339, loss: 0.022887\n",
      "step 7340, loss: 0.022877\n",
      "step 7341, loss: 0.022866\n",
      "step 7342, loss: 0.022856\n",
      "step 7343, loss: 0.022845\n",
      "step 7344, loss: 0.022835\n",
      "step 7345, loss: 0.022824\n",
      "step 7346, loss: 0.022814\n",
      "step 7347, loss: 0.022804\n",
      "step 7348, loss: 0.022793\n",
      "step 7349, loss: 0.022783\n",
      "step 7350, loss: 0.022772\n",
      "step 7351, loss: 0.022762\n",
      "step 7352, loss: 0.022752\n",
      "step 7353, loss: 0.022741\n",
      "step 7354, loss: 0.022731\n",
      "step 7355, loss: 0.022720\n",
      "step 7356, loss: 0.022710\n",
      "step 7357, loss: 0.022700\n",
      "step 7358, loss: 0.022689\n",
      "step 7359, loss: 0.022679\n",
      "step 7360, loss: 0.022669\n",
      "step 7361, loss: 0.022658\n",
      "step 7362, loss: 0.022648\n",
      "step 7363, loss: 0.022637\n",
      "step 7364, loss: 0.022627\n",
      "step 7365, loss: 0.022617\n",
      "step 7366, loss: 0.022606\n",
      "step 7367, loss: 0.022596\n",
      "step 7368, loss: 0.022586\n",
      "step 7369, loss: 0.022576\n",
      "step 7370, loss: 0.022565\n",
      "step 7371, loss: 0.022555\n",
      "step 7372, loss: 0.022545\n",
      "step 7373, loss: 0.022534\n",
      "step 7374, loss: 0.022524\n",
      "step 7375, loss: 0.022514\n",
      "step 7376, loss: 0.022503\n",
      "step 7377, loss: 0.022493\n",
      "step 7378, loss: 0.022483\n",
      "step 7379, loss: 0.022473\n",
      "step 7380, loss: 0.022462\n",
      "step 7381, loss: 0.022452\n",
      "step 7382, loss: 0.022442\n",
      "step 7383, loss: 0.022432\n",
      "step 7384, loss: 0.022421\n",
      "step 7385, loss: 0.022411\n",
      "step 7386, loss: 0.022401\n",
      "step 7387, loss: 0.022391\n",
      "step 7388, loss: 0.022380\n",
      "step 7389, loss: 0.022370\n",
      "step 7390, loss: 0.022360\n",
      "step 7391, loss: 0.022350\n",
      "step 7392, loss: 0.022340\n",
      "step 7393, loss: 0.022329\n",
      "step 7394, loss: 0.022319\n",
      "step 7395, loss: 0.022309\n",
      "step 7396, loss: 0.022299\n",
      "step 7397, loss: 0.022289\n",
      "step 7398, loss: 0.022278\n",
      "step 7399, loss: 0.022268\n",
      "step 7400, loss: 0.022258\n",
      "step 7401, loss: 0.022248\n",
      "step 7402, loss: 0.022238\n",
      "step 7403, loss: 0.022228\n",
      "step 7404, loss: 0.022217\n",
      "step 7405, loss: 0.022207\n",
      "step 7406, loss: 0.022197\n",
      "step 7407, loss: 0.022187\n",
      "step 7408, loss: 0.022177\n",
      "step 7409, loss: 0.022167\n",
      "step 7410, loss: 0.022157\n",
      "step 7411, loss: 0.022147\n",
      "step 7412, loss: 0.022136\n",
      "step 7413, loss: 0.022126\n",
      "step 7414, loss: 0.022116\n",
      "step 7415, loss: 0.022106\n",
      "step 7416, loss: 0.022096\n",
      "step 7417, loss: 0.022086\n",
      "step 7418, loss: 0.022076\n",
      "step 7419, loss: 0.022066\n",
      "step 7420, loss: 0.022056\n",
      "step 7421, loss: 0.022046\n",
      "step 7422, loss: 0.022035\n",
      "step 7423, loss: 0.022025\n",
      "step 7424, loss: 0.022015\n",
      "step 7425, loss: 0.022005\n",
      "step 7426, loss: 0.021995\n",
      "step 7427, loss: 0.021985\n",
      "step 7428, loss: 0.021975\n",
      "step 7429, loss: 0.021965\n",
      "step 7430, loss: 0.021955\n",
      "step 7431, loss: 0.021945\n",
      "step 7432, loss: 0.021935\n",
      "step 7433, loss: 0.021925\n",
      "step 7434, loss: 0.021915\n",
      "step 7435, loss: 0.021905\n",
      "step 7436, loss: 0.021895\n",
      "step 7437, loss: 0.021885\n",
      "step 7438, loss: 0.021875\n",
      "step 7439, loss: 0.021865\n",
      "step 7440, loss: 0.021855\n",
      "step 7441, loss: 0.021845\n",
      "step 7442, loss: 0.021835\n",
      "step 7443, loss: 0.021825\n",
      "step 7444, loss: 0.021815\n",
      "step 7445, loss: 0.021805\n",
      "step 7446, loss: 0.021795\n",
      "step 7447, loss: 0.021785\n",
      "step 7448, loss: 0.021775\n",
      "step 7449, loss: 0.021765\n",
      "step 7450, loss: 0.021755\n",
      "step 7451, loss: 0.021745\n",
      "step 7452, loss: 0.021735\n",
      "step 7453, loss: 0.021725\n",
      "step 7454, loss: 0.021715\n",
      "step 7455, loss: 0.021705\n",
      "step 7456, loss: 0.021696\n",
      "step 7457, loss: 0.021686\n",
      "step 7458, loss: 0.021676\n",
      "step 7459, loss: 0.021666\n",
      "step 7460, loss: 0.021656\n",
      "step 7461, loss: 0.021646\n",
      "step 7462, loss: 0.021636\n",
      "step 7463, loss: 0.021626\n",
      "step 7464, loss: 0.021616\n",
      "step 7465, loss: 0.021606\n",
      "step 7466, loss: 0.021597\n",
      "step 7467, loss: 0.021587\n",
      "step 7468, loss: 0.021577\n",
      "step 7469, loss: 0.021567\n",
      "step 7470, loss: 0.021557\n",
      "step 7471, loss: 0.021547\n",
      "step 7472, loss: 0.021537\n",
      "step 7473, loss: 0.021528\n",
      "step 7474, loss: 0.021518\n",
      "step 7475, loss: 0.021508\n",
      "step 7476, loss: 0.021498\n",
      "step 7477, loss: 0.021488\n",
      "step 7478, loss: 0.021478\n",
      "step 7479, loss: 0.021469\n",
      "step 7480, loss: 0.021459\n",
      "step 7481, loss: 0.021449\n",
      "step 7482, loss: 0.021439\n",
      "step 7483, loss: 0.021429\n",
      "step 7484, loss: 0.021420\n",
      "step 7485, loss: 0.021410\n",
      "step 7486, loss: 0.021400\n",
      "step 7487, loss: 0.021390\n",
      "step 7488, loss: 0.021381\n",
      "step 7489, loss: 0.021371\n",
      "step 7490, loss: 0.021361\n",
      "step 7491, loss: 0.021351\n",
      "step 7492, loss: 0.021342\n",
      "step 7493, loss: 0.021332\n",
      "step 7494, loss: 0.021322\n",
      "step 7495, loss: 0.021312\n",
      "step 7496, loss: 0.021303\n",
      "step 7497, loss: 0.021293\n",
      "step 7498, loss: 0.021283\n",
      "step 7499, loss: 0.021274\n",
      "step 7500, loss: 0.021264\n",
      "step 7501, loss: 0.021254\n",
      "step 7502, loss: 0.021244\n",
      "step 7503, loss: 0.021235\n",
      "step 7504, loss: 0.021225\n",
      "step 7505, loss: 0.021215\n",
      "step 7506, loss: 0.021206\n",
      "step 7507, loss: 0.021196\n",
      "step 7508, loss: 0.021186\n",
      "step 7509, loss: 0.021177\n",
      "step 7510, loss: 0.021167\n",
      "step 7511, loss: 0.021158\n",
      "step 7512, loss: 0.021148\n",
      "step 7513, loss: 0.021138\n",
      "step 7514, loss: 0.021129\n",
      "step 7515, loss: 0.021119\n",
      "step 7516, loss: 0.021109\n",
      "step 7517, loss: 0.021100\n",
      "step 7518, loss: 0.021090\n",
      "step 7519, loss: 0.021081\n",
      "step 7520, loss: 0.021071\n",
      "step 7521, loss: 0.021062\n",
      "step 7522, loss: 0.021052\n",
      "step 7523, loss: 0.021042\n",
      "step 7524, loss: 0.021033\n",
      "step 7525, loss: 0.021023\n",
      "step 7526, loss: 0.021014\n",
      "step 7527, loss: 0.021004\n",
      "step 7528, loss: 0.020995\n",
      "step 7529, loss: 0.020985\n",
      "step 7530, loss: 0.020975\n",
      "step 7531, loss: 0.020966\n",
      "step 7532, loss: 0.020956\n",
      "step 7533, loss: 0.020947\n",
      "step 7534, loss: 0.020937\n",
      "step 7535, loss: 0.020928\n",
      "step 7536, loss: 0.020918\n",
      "step 7537, loss: 0.020909\n",
      "step 7538, loss: 0.020899\n",
      "step 7539, loss: 0.020890\n",
      "step 7540, loss: 0.020880\n",
      "step 7541, loss: 0.020871\n",
      "step 7542, loss: 0.020861\n",
      "step 7543, loss: 0.020852\n",
      "step 7544, loss: 0.020842\n",
      "step 7545, loss: 0.020833\n",
      "step 7546, loss: 0.020824\n",
      "step 7547, loss: 0.020814\n",
      "step 7548, loss: 0.020805\n",
      "step 7549, loss: 0.020795\n",
      "step 7550, loss: 0.020786\n",
      "step 7551, loss: 0.020776\n",
      "step 7552, loss: 0.020767\n",
      "step 7553, loss: 0.020757\n",
      "step 7554, loss: 0.020748\n",
      "step 7555, loss: 0.020739\n",
      "step 7556, loss: 0.020729\n",
      "step 7557, loss: 0.020720\n",
      "step 7558, loss: 0.020710\n",
      "step 7559, loss: 0.020701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7560, loss: 0.020692\n",
      "step 7561, loss: 0.020682\n",
      "step 7562, loss: 0.020673\n",
      "step 7563, loss: 0.020663\n",
      "step 7564, loss: 0.020654\n",
      "step 7565, loss: 0.020645\n",
      "step 7566, loss: 0.020635\n",
      "step 7567, loss: 0.020626\n",
      "step 7568, loss: 0.020617\n",
      "step 7569, loss: 0.020607\n",
      "step 7570, loss: 0.020598\n",
      "step 7571, loss: 0.020589\n",
      "step 7572, loss: 0.020579\n",
      "step 7573, loss: 0.020570\n",
      "step 7574, loss: 0.020561\n",
      "step 7575, loss: 0.020551\n",
      "step 7576, loss: 0.020542\n",
      "step 7577, loss: 0.020533\n",
      "step 7578, loss: 0.020523\n",
      "step 7579, loss: 0.020514\n",
      "step 7580, loss: 0.020505\n",
      "step 7581, loss: 0.020495\n",
      "step 7582, loss: 0.020486\n",
      "step 7583, loss: 0.020477\n",
      "step 7584, loss: 0.020468\n",
      "step 7585, loss: 0.020458\n",
      "step 7586, loss: 0.020449\n",
      "step 7587, loss: 0.020440\n",
      "step 7588, loss: 0.020431\n",
      "step 7589, loss: 0.020421\n",
      "step 7590, loss: 0.020412\n",
      "step 7591, loss: 0.020403\n",
      "step 7592, loss: 0.020394\n",
      "step 7593, loss: 0.020384\n",
      "step 7594, loss: 0.020375\n",
      "step 7595, loss: 0.020366\n",
      "step 7596, loss: 0.020357\n",
      "step 7597, loss: 0.020347\n",
      "step 7598, loss: 0.020338\n",
      "step 7599, loss: 0.020329\n",
      "step 7600, loss: 0.020320\n",
      "step 7601, loss: 0.020311\n",
      "step 7602, loss: 0.020301\n",
      "step 7603, loss: 0.020292\n",
      "step 7604, loss: 0.020283\n",
      "step 7605, loss: 0.020274\n",
      "step 7606, loss: 0.020265\n",
      "step 7607, loss: 0.020256\n",
      "step 7608, loss: 0.020246\n",
      "step 7609, loss: 0.020237\n",
      "step 7610, loss: 0.020228\n",
      "step 7611, loss: 0.020219\n",
      "step 7612, loss: 0.020210\n",
      "step 7613, loss: 0.020201\n",
      "step 7614, loss: 0.020192\n",
      "step 7615, loss: 0.020182\n",
      "step 7616, loss: 0.020173\n",
      "step 7617, loss: 0.020164\n",
      "step 7618, loss: 0.020155\n",
      "step 7619, loss: 0.020146\n",
      "step 7620, loss: 0.020137\n",
      "step 7621, loss: 0.020128\n",
      "step 7622, loss: 0.020119\n",
      "step 7623, loss: 0.020110\n",
      "step 7624, loss: 0.020101\n",
      "step 7625, loss: 0.020092\n",
      "step 7626, loss: 0.020082\n",
      "step 7627, loss: 0.020073\n",
      "step 7628, loss: 0.020064\n",
      "step 7629, loss: 0.020055\n",
      "step 7630, loss: 0.020046\n",
      "step 7631, loss: 0.020037\n",
      "step 7632, loss: 0.020028\n",
      "step 7633, loss: 0.020019\n",
      "step 7634, loss: 0.020010\n",
      "step 7635, loss: 0.020001\n",
      "step 7636, loss: 0.019992\n",
      "step 7637, loss: 0.019983\n",
      "step 7638, loss: 0.019974\n",
      "step 7639, loss: 0.019965\n",
      "step 7640, loss: 0.019956\n",
      "step 7641, loss: 0.019947\n",
      "step 7642, loss: 0.019938\n",
      "step 7643, loss: 0.019929\n",
      "step 7644, loss: 0.019920\n",
      "step 7645, loss: 0.019911\n",
      "step 7646, loss: 0.019902\n",
      "step 7647, loss: 0.019894\n",
      "step 7648, loss: 0.019885\n",
      "step 7649, loss: 0.019876\n",
      "step 7650, loss: 0.019867\n",
      "step 7651, loss: 0.019858\n",
      "step 7652, loss: 0.019849\n",
      "step 7653, loss: 0.019840\n",
      "step 7654, loss: 0.019831\n",
      "step 7655, loss: 0.019822\n",
      "step 7656, loss: 0.019813\n",
      "step 7657, loss: 0.019804\n",
      "step 7658, loss: 0.019795\n",
      "step 7659, loss: 0.019787\n",
      "step 7660, loss: 0.019778\n",
      "step 7661, loss: 0.019769\n",
      "step 7662, loss: 0.019760\n",
      "step 7663, loss: 0.019751\n",
      "step 7664, loss: 0.019742\n",
      "step 7665, loss: 0.019733\n",
      "step 7666, loss: 0.019724\n",
      "step 7667, loss: 0.019716\n",
      "step 7668, loss: 0.019707\n",
      "step 7669, loss: 0.019698\n",
      "step 7670, loss: 0.019689\n",
      "step 7671, loss: 0.019680\n",
      "step 7672, loss: 0.019671\n",
      "step 7673, loss: 0.019663\n",
      "step 7674, loss: 0.019654\n",
      "step 7675, loss: 0.019645\n",
      "step 7676, loss: 0.019636\n",
      "step 7677, loss: 0.019627\n",
      "step 7678, loss: 0.019619\n",
      "step 7679, loss: 0.019610\n",
      "step 7680, loss: 0.019601\n",
      "step 7681, loss: 0.019592\n",
      "step 7682, loss: 0.019583\n",
      "step 7683, loss: 0.019575\n",
      "step 7684, loss: 0.019566\n",
      "step 7685, loss: 0.019557\n",
      "step 7686, loss: 0.019548\n",
      "step 7687, loss: 0.019540\n",
      "step 7688, loss: 0.019531\n",
      "step 7689, loss: 0.019522\n",
      "step 7690, loss: 0.019514\n",
      "step 7691, loss: 0.019505\n",
      "step 7692, loss: 0.019496\n",
      "step 7693, loss: 0.019487\n",
      "step 7694, loss: 0.019479\n",
      "step 7695, loss: 0.019470\n",
      "step 7696, loss: 0.019461\n",
      "step 7697, loss: 0.019453\n",
      "step 7698, loss: 0.019444\n",
      "step 7699, loss: 0.019435\n",
      "step 7700, loss: 0.019427\n",
      "step 7701, loss: 0.019418\n",
      "step 7702, loss: 0.019409\n",
      "step 7703, loss: 0.019401\n",
      "step 7704, loss: 0.019392\n",
      "step 7705, loss: 0.019383\n",
      "step 7706, loss: 0.019375\n",
      "step 7707, loss: 0.019366\n",
      "step 7708, loss: 0.019357\n",
      "step 7709, loss: 0.019349\n",
      "step 7710, loss: 0.019340\n",
      "step 7711, loss: 0.019332\n",
      "step 7712, loss: 0.019323\n",
      "step 7713, loss: 0.019314\n",
      "step 7714, loss: 0.019306\n",
      "step 7715, loss: 0.019297\n",
      "step 7716, loss: 0.019289\n",
      "step 7717, loss: 0.019280\n",
      "step 7718, loss: 0.019271\n",
      "step 7719, loss: 0.019263\n",
      "step 7720, loss: 0.019254\n",
      "step 7721, loss: 0.019246\n",
      "step 7722, loss: 0.019237\n",
      "step 7723, loss: 0.019229\n",
      "step 7724, loss: 0.019220\n",
      "step 7725, loss: 0.019212\n",
      "step 7726, loss: 0.019203\n",
      "step 7727, loss: 0.019194\n",
      "step 7728, loss: 0.019186\n",
      "step 7729, loss: 0.019177\n",
      "step 7730, loss: 0.019169\n",
      "step 7731, loss: 0.019160\n",
      "step 7732, loss: 0.019152\n",
      "step 7733, loss: 0.019143\n",
      "step 7734, loss: 0.019135\n",
      "step 7735, loss: 0.019126\n",
      "step 7736, loss: 0.019118\n",
      "step 7737, loss: 0.019109\n",
      "step 7738, loss: 0.019101\n",
      "step 7739, loss: 0.019092\n",
      "step 7740, loss: 0.019084\n",
      "step 7741, loss: 0.019076\n",
      "step 7742, loss: 0.019067\n",
      "step 7743, loss: 0.019059\n",
      "step 7744, loss: 0.019050\n",
      "step 7745, loss: 0.019042\n",
      "step 7746, loss: 0.019033\n",
      "step 7747, loss: 0.019025\n",
      "step 7748, loss: 0.019017\n",
      "step 7749, loss: 0.019008\n",
      "step 7750, loss: 0.019000\n",
      "step 7751, loss: 0.018991\n",
      "step 7752, loss: 0.018983\n",
      "step 7753, loss: 0.018975\n",
      "step 7754, loss: 0.018966\n",
      "step 7755, loss: 0.018958\n",
      "step 7756, loss: 0.018949\n",
      "step 7757, loss: 0.018941\n",
      "step 7758, loss: 0.018933\n",
      "step 7759, loss: 0.018924\n",
      "step 7760, loss: 0.018916\n",
      "step 7761, loss: 0.018908\n",
      "step 7762, loss: 0.018899\n",
      "step 7763, loss: 0.018891\n",
      "step 7764, loss: 0.018883\n",
      "step 7765, loss: 0.018874\n",
      "step 7766, loss: 0.018866\n",
      "step 7767, loss: 0.018858\n",
      "step 7768, loss: 0.018849\n",
      "step 7769, loss: 0.018841\n",
      "step 7770, loss: 0.018833\n",
      "step 7771, loss: 0.018824\n",
      "step 7772, loss: 0.018816\n",
      "step 7773, loss: 0.018808\n",
      "step 7774, loss: 0.018800\n",
      "step 7775, loss: 0.018791\n",
      "step 7776, loss: 0.018783\n",
      "step 7777, loss: 0.018775\n",
      "step 7778, loss: 0.018767\n",
      "step 7779, loss: 0.018758\n",
      "step 7780, loss: 0.018750\n",
      "step 7781, loss: 0.018742\n",
      "step 7782, loss: 0.018734\n",
      "step 7783, loss: 0.018725\n",
      "step 7784, loss: 0.018717\n",
      "step 7785, loss: 0.018709\n",
      "step 7786, loss: 0.018701\n",
      "step 7787, loss: 0.018692\n",
      "step 7788, loss: 0.018684\n",
      "step 7789, loss: 0.018676\n",
      "step 7790, loss: 0.018668\n",
      "step 7791, loss: 0.018660\n",
      "step 7792, loss: 0.018652\n",
      "step 7793, loss: 0.018643\n",
      "step 7794, loss: 0.018635\n",
      "step 7795, loss: 0.018627\n",
      "step 7796, loss: 0.018619\n",
      "step 7797, loss: 0.018611\n",
      "step 7798, loss: 0.018603\n",
      "step 7799, loss: 0.018594\n",
      "step 7800, loss: 0.018586\n",
      "step 7801, loss: 0.018578\n",
      "step 7802, loss: 0.018570\n",
      "step 7803, loss: 0.018562\n",
      "step 7804, loss: 0.018554\n",
      "step 7805, loss: 0.018546\n",
      "step 7806, loss: 0.018538\n",
      "step 7807, loss: 0.018529\n",
      "step 7808, loss: 0.018521\n",
      "step 7809, loss: 0.018513\n",
      "step 7810, loss: 0.018505\n",
      "step 7811, loss: 0.018497\n",
      "step 7812, loss: 0.018489\n",
      "step 7813, loss: 0.018481\n",
      "step 7814, loss: 0.018473\n",
      "step 7815, loss: 0.018465\n",
      "step 7816, loss: 0.018457\n",
      "step 7817, loss: 0.018449\n",
      "step 7818, loss: 0.018441\n",
      "step 7819, loss: 0.018433\n",
      "step 7820, loss: 0.018425\n",
      "step 7821, loss: 0.018417\n",
      "step 7822, loss: 0.018409\n",
      "step 7823, loss: 0.018401\n",
      "step 7824, loss: 0.018393\n",
      "step 7825, loss: 0.018385\n",
      "step 7826, loss: 0.018377\n",
      "step 7827, loss: 0.018369\n",
      "step 7828, loss: 0.018361\n",
      "step 7829, loss: 0.018353\n",
      "step 7830, loss: 0.018345\n",
      "step 7831, loss: 0.018337\n",
      "step 7832, loss: 0.018329\n",
      "step 7833, loss: 0.018321\n",
      "step 7834, loss: 0.018313\n",
      "step 7835, loss: 0.018305\n",
      "step 7836, loss: 0.018297\n",
      "step 7837, loss: 0.018289\n",
      "step 7838, loss: 0.018281\n",
      "step 7839, loss: 0.018273\n",
      "step 7840, loss: 0.018265\n",
      "step 7841, loss: 0.018257\n",
      "step 7842, loss: 0.018249\n",
      "step 7843, loss: 0.018241\n",
      "step 7844, loss: 0.018234\n",
      "step 7845, loss: 0.018226\n",
      "step 7846, loss: 0.018218\n",
      "step 7847, loss: 0.018210\n",
      "step 7848, loss: 0.018202\n",
      "step 7849, loss: 0.018194\n",
      "step 7850, loss: 0.018186\n",
      "step 7851, loss: 0.018179\n",
      "step 7852, loss: 0.018171\n",
      "step 7853, loss: 0.018163\n",
      "step 7854, loss: 0.018155\n",
      "step 7855, loss: 0.018147\n",
      "step 7856, loss: 0.018139\n",
      "step 7857, loss: 0.018131\n",
      "step 7858, loss: 0.018124\n",
      "step 7859, loss: 0.018116\n",
      "step 7860, loss: 0.018108\n",
      "step 7861, loss: 0.018100\n",
      "step 7862, loss: 0.018092\n",
      "step 7863, loss: 0.018085\n",
      "step 7864, loss: 0.018077\n",
      "step 7865, loss: 0.018069\n",
      "step 7866, loss: 0.018061\n",
      "step 7867, loss: 0.018053\n",
      "step 7868, loss: 0.018046\n",
      "step 7869, loss: 0.018038\n",
      "step 7870, loss: 0.018030\n",
      "step 7871, loss: 0.018022\n",
      "step 7872, loss: 0.018015\n",
      "step 7873, loss: 0.018007\n",
      "step 7874, loss: 0.017999\n",
      "step 7875, loss: 0.017992\n",
      "step 7876, loss: 0.017984\n",
      "step 7877, loss: 0.017976\n",
      "step 7878, loss: 0.017968\n",
      "step 7879, loss: 0.017961\n",
      "step 7880, loss: 0.017953\n",
      "step 7881, loss: 0.017945\n",
      "step 7882, loss: 0.017938\n",
      "step 7883, loss: 0.017930\n",
      "step 7884, loss: 0.017922\n",
      "step 7885, loss: 0.017915\n",
      "step 7886, loss: 0.017907\n",
      "step 7887, loss: 0.017899\n",
      "step 7888, loss: 0.017892\n",
      "step 7889, loss: 0.017884\n",
      "step 7890, loss: 0.017876\n",
      "step 7891, loss: 0.017869\n",
      "step 7892, loss: 0.017861\n",
      "step 7893, loss: 0.017853\n",
      "step 7894, loss: 0.017846\n",
      "step 7895, loss: 0.017838\n",
      "step 7896, loss: 0.017830\n",
      "step 7897, loss: 0.017823\n",
      "step 7898, loss: 0.017815\n",
      "step 7899, loss: 0.017808\n",
      "step 7900, loss: 0.017800\n",
      "step 7901, loss: 0.017793\n",
      "step 7902, loss: 0.017785\n",
      "step 7903, loss: 0.017777\n",
      "step 7904, loss: 0.017770\n",
      "step 7905, loss: 0.017762\n",
      "step 7906, loss: 0.017755\n",
      "step 7907, loss: 0.017747\n",
      "step 7908, loss: 0.017740\n",
      "step 7909, loss: 0.017732\n",
      "step 7910, loss: 0.017725\n",
      "step 7911, loss: 0.017717\n",
      "step 7912, loss: 0.017709\n",
      "step 7913, loss: 0.017702\n",
      "step 7914, loss: 0.017694\n",
      "step 7915, loss: 0.017687\n",
      "step 7916, loss: 0.017679\n",
      "step 7917, loss: 0.017672\n",
      "step 7918, loss: 0.017664\n",
      "step 7919, loss: 0.017657\n",
      "step 7920, loss: 0.017649\n",
      "step 7921, loss: 0.017642\n",
      "step 7922, loss: 0.017634\n",
      "step 7923, loss: 0.017627\n",
      "step 7924, loss: 0.017620\n",
      "step 7925, loss: 0.017612\n",
      "step 7926, loss: 0.017605\n",
      "step 7927, loss: 0.017597\n",
      "step 7928, loss: 0.017590\n",
      "step 7929, loss: 0.017582\n",
      "step 7930, loss: 0.017575\n",
      "step 7931, loss: 0.017568\n",
      "step 7932, loss: 0.017560\n",
      "step 7933, loss: 0.017553\n",
      "step 7934, loss: 0.017545\n",
      "step 7935, loss: 0.017538\n",
      "step 7936, loss: 0.017530\n",
      "step 7937, loss: 0.017523\n",
      "step 7938, loss: 0.017516\n",
      "step 7939, loss: 0.017508\n",
      "step 7940, loss: 0.017501\n",
      "step 7941, loss: 0.017494\n",
      "step 7942, loss: 0.017486\n",
      "step 7943, loss: 0.017479\n",
      "step 7944, loss: 0.017472\n",
      "step 7945, loss: 0.017464\n",
      "step 7946, loss: 0.017457\n",
      "step 7947, loss: 0.017450\n",
      "step 7948, loss: 0.017442\n",
      "step 7949, loss: 0.017435\n",
      "step 7950, loss: 0.017428\n",
      "step 7951, loss: 0.017420\n",
      "step 7952, loss: 0.017413\n",
      "step 7953, loss: 0.017406\n",
      "step 7954, loss: 0.017398\n",
      "step 7955, loss: 0.017391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7956, loss: 0.017384\n",
      "step 7957, loss: 0.017377\n",
      "step 7958, loss: 0.017369\n",
      "step 7959, loss: 0.017362\n",
      "step 7960, loss: 0.017355\n",
      "step 7961, loss: 0.017347\n",
      "step 7962, loss: 0.017340\n",
      "step 7963, loss: 0.017333\n",
      "step 7964, loss: 0.017326\n",
      "step 7965, loss: 0.017318\n",
      "step 7966, loss: 0.017311\n",
      "step 7967, loss: 0.017304\n",
      "step 7968, loss: 0.017297\n",
      "step 7969, loss: 0.017290\n",
      "step 7970, loss: 0.017282\n",
      "step 7971, loss: 0.017275\n",
      "step 7972, loss: 0.017268\n",
      "step 7973, loss: 0.017261\n",
      "step 7974, loss: 0.017254\n",
      "step 7975, loss: 0.017246\n",
      "step 7976, loss: 0.017239\n",
      "step 7977, loss: 0.017232\n",
      "step 7978, loss: 0.017225\n",
      "step 7979, loss: 0.017218\n",
      "step 7980, loss: 0.017211\n",
      "step 7981, loss: 0.017204\n",
      "step 7982, loss: 0.017196\n",
      "step 7983, loss: 0.017189\n",
      "step 7984, loss: 0.017182\n",
      "step 7985, loss: 0.017175\n",
      "step 7986, loss: 0.017168\n",
      "step 7987, loss: 0.017161\n",
      "step 7988, loss: 0.017154\n",
      "step 7989, loss: 0.017147\n",
      "step 7990, loss: 0.017139\n",
      "step 7991, loss: 0.017132\n",
      "step 7992, loss: 0.017125\n",
      "step 7993, loss: 0.017118\n",
      "step 7994, loss: 0.017111\n",
      "step 7995, loss: 0.017104\n",
      "step 7996, loss: 0.017097\n",
      "step 7997, loss: 0.017090\n",
      "step 7998, loss: 0.017083\n",
      "step 7999, loss: 0.017076\n",
      "w_final: 4.692999, b_final: 2.163553\n",
      "y_pred_np.shape: (100, 1)\n",
      "y_pred_np.shape2: (100,)\n",
      "pearson R^2: 0.994371\n",
      "RMS: 0.130648\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "n_iter = 8000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(n_iter):\n",
    "        feed_dict = {x: x_np, y: y_np}\n",
    "        _, summary, los = sess.run([train_op, merged, loss], feed_dict=feed_dict)\n",
    "        print(\"step %d, loss: %f\" % (i, los))\n",
    "        train_writer.add_summary(summary, i)\n",
    "    w_final, b_final = sess.run([W, b])\n",
    "    y_pred_np = sess.run(y_pred, feed_dict={x: x_np})\n",
    "\n",
    "print(\"w_final: %f, b_final: %f\" % (w_final, b_final))\n",
    "print(\"y_pred_np.shape:\", y_pred_np.shape)\n",
    "y_pred_np = np.reshape(y_pred_np, -1)\n",
    "print(\"y_pred_np.shape2:\", y_pred_np.shape)\n",
    "r2 = pearson_r2_score(y_np, y_pred_np)\n",
    "print(\"pearson R^2: %f\" % r2)\n",
    "rms = rms_score(y_np, y_pred_np)\n",
    "print(\"RMS: %f\" % rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw figure\n",
    "plt.clf()\n",
    "plt.xlabel(\"Y-true\")\n",
    "plt.ylabel(\"Y-pred\")\n",
    "plt.title(\"Predicted V.S. True values\")\n",
    "plt.scatter(y_np, y_pred_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the learned regression line\n",
    "plt.clf()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"True Model V.S. Learned Model\")\n",
    "plt.xlim(0, 1)\n",
    "plt.scatter(x_np, y_np)\n",
    "x_left = 0\n",
    "y_left = w_final[0]*x_left + b_final\n",
    "x_right = 1\n",
    "y_right = w_final[0]*x_right + b_final\n",
    "plt.plot([x_left, x_right], [y_left, y_right], color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
